script model/nntool_script
GEN ... /home/marco-gwt/GWT/AutotilerV2/CNN_Generators/CNN_Generator_Util.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators/CNN_Copy_Generators.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators/SSD_Generators.c /home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes/ResizeGenerator.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators_SQ8/CNN_Generators_SQ8.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators_SQ8/RNN_Generators_SQ8.c
python3 utils/test_accuracy_emul.py --tflite_model model/KWS_ds_cnn_l_quant_power.tflite --dct_coefficient_count 10 --window_size_ms 40 --window_stride_ms 20 --test_with_wav 1 --use_power_spectrogram 1
WARNING:tensorflow:From utils/test_accuracy_emul.py:311: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

In file included from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/wavIO.h:13,
                 from main_emulation.c:32:
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:46:20: warning: useless storage class specifier in empty declaration
   46 |     typedef struct pi_device {};
      |                    ^~~~~~~~~
In file included from /home/marco-gwt/GWT/AutotilerV2/Emulation/at_api.h:21,
                 from /home/marco-gwt/GWT/AutotilerV2/Emulation/Gap.h:18,
                 from /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_BasicKernels_SQ8.h:3,
                 from BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_l_quantKernels.h:5,
                 from BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_l_quantKernels.c:1:
/home/marco-gwt/GWT/AutotilerV2/Emulation/at_api_emul.h: In function ‘__at_hyperflash_fs_copy’:
/home/marco-gwt/GWT/AutotilerV2/Emulation/at_api_emul.h:173:8: warning: ignoring return value of ‘fread’, declared with attribute warn_unused_result [-Wunused-result]
  173 |   else fread(loc, 1, size, file);
      |        ^~~~~~~~~~~~~~~~~~~~~~~~~
In file included from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/ImgIO.h:13,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c:9:
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:46:20: warning: useless storage class specifier in empty declaration
   46 |     typedef struct pi_device {};
      |                    ^~~~~~~~~
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c: In function ‘WritePPMHeader’:
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c:404:17: warning: passing argument 1 of ‘write’ makes integer from pointer without a cast [-Wint-conversion]
  404 |         __WRITE(FD,&(Buffer[a]), sizeof(unsigned char));
      |                 ^~
      |                 |
      |                 void *
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:41:63: note: in definition of macro ‘__WRITE’
   41 |     #define __WRITE(__FD, __BUF, __LEN)                 write(__FD, __BUF, __LEN)
      |                                                               ^~~~
In file included from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:29,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/ImgIO.h:13,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c:9:
/usr/include/unistd.h:366:27: note: expected ‘int’ but argument is of type ‘void *’
  366 | extern ssize_t write (int __fd, const void *__buf, size_t __n) __wur;
      |                       ~~~~^~~~
In file included from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/ImgIO.h:13,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c:9:
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c: In function ‘WriteImageToFile’:
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:37:57: warning: initialization of ‘void *’ from ‘int’ makes pointer from integer without a cast [-Wint-conversion]
   37 |     #define __OPEN_WRITE(__FS, __NAME)                  open(__NAME, O_RDWR | O_CREAT, S_IRWXU)
      |                                                         ^~~~
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c:437:18: note: in expansion of macro ‘__OPEN_WRITE’
  437 |     void *File = __OPEN_WRITE(fs, ImageName);
      |                  ^~~~~~~~~~~~
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c:454:26: warning: passing argument 1 of ‘write’ makes integer from pointer without a cast [-Wint-conversion]
  454 |             ret+=__WRITE(File, img_rgb888, rgb888_size);
      |                          ^~~~
      |                          |
      |                          void *
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:41:63: note: in definition of macro ‘__WRITE’
   41 |     #define __WRITE(__FD, __BUF, __LEN)                 write(__FD, __BUF, __LEN)
      |                                                               ^~~~
In file included from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:29,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/ImgIO.h:13,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c:9:
/usr/include/unistd.h:366:27: note: expected ‘int’ but argument is of type ‘void *’
  366 | extern ssize_t write (int __fd, const void *__buf, size_t __n) __wur;
      |                       ~~~~^~~~
In file included from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/ImgIO.h:13,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c:9:
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c:460:26: warning: passing argument 1 of ‘write’ makes integer from pointer without a cast [-Wint-conversion]
  460 |             ret+=__WRITE(File, img_rgb888, rgb888_size);
      |                          ^~~~
      |                          |
      |                          void *
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:41:63: note: in definition of macro ‘__WRITE’
   41 |     #define __WRITE(__FD, __BUF, __LEN)                 write(__FD, __BUF, __LEN)
      |                                                               ^~~~
In file included from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:29,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/ImgIO.h:13,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c:9:
/usr/include/unistd.h:366:27: note: expected ‘int’ but argument is of type ‘void *’
  366 | extern ssize_t write (int __fd, const void *__buf, size_t __n) __wur;
      |                       ~~~~^~~~
In file included from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/ImgIO.h:13,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c:9:
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c:473:26: warning: passing argument 1 of ‘write’ makes integer from pointer without a cast [-Wint-conversion]
  473 |             ret+=__WRITE(File,OutBuffer +(CHUNK_SIZE*i), CHUNK_SIZE);
      |                          ^~~~
      |                          |
      |                          void *
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:41:63: note: in definition of macro ‘__WRITE’
   41 |     #define __WRITE(__FD, __BUF, __LEN)                 write(__FD, __BUF, __LEN)
      |                                                               ^~~~
In file included from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:29,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/ImgIO.h:13,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c:9:
/usr/include/unistd.h:366:27: note: expected ‘int’ but argument is of type ‘void *’
  366 | extern ssize_t write (int __fd, const void *__buf, size_t __n) __wur;
      |                       ~~~~^~~~
In file included from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/ImgIO.h:13,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c:9:
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c:476:26: warning: passing argument 1 of ‘write’ makes integer from pointer without a cast [-Wint-conversion]
  476 |             ret+=__WRITE(File,OutBuffer+(CHUNK_SIZE*steps) , ((W*H*PixelSize) % CHUNK_SIZE)*sizeof(unsigned char));
      |                          ^~~~
      |                          |
      |                          void *
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:41:63: note: in definition of macro ‘__WRITE’
   41 |     #define __WRITE(__FD, __BUF, __LEN)                 write(__FD, __BUF, __LEN)
      |                                                               ^~~~
In file included from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:29,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/ImgIO.h:13,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c:9:
/usr/include/unistd.h:366:27: note: expected ‘int’ but argument is of type ‘void *’
  366 | extern ssize_t write (int __fd, const void *__buf, size_t __n) __wur;
      |                       ~~~~^~~~
In file included from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/ImgIO.h:13,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c:9:
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c:479:13: warning: passing argument 1 of ‘close’ makes integer from pointer without a cast [-Wint-conversion]
  479 |     __CLOSE(File);
      |             ^~~~
      |             |
      |             void *
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:38:63: note: in definition of macro ‘__CLOSE’
   38 |     #define __CLOSE(__FD)                               close(__FD)
      |                                                               ^~~~
In file included from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:29,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/ImgIO.h:13,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c:9:
/usr/include/unistd.h:353:23: note: expected ‘int’ but argument is of type ‘void *’
  353 | extern int close (int __fd);
      |                   ~~~~^~~~
In file included from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/ImgIO.h:13,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c:9:
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c: In function ‘WritePPMHeader’:
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:41:57: warning: ignoring return value of ‘write’, declared with attribute warn_unused_result [-Wunused-result]
   41 |     #define __WRITE(__FD, __BUF, __LEN)                 write(__FD, __BUF, __LEN)
      |                                                         ^~~~~~~~~~~~~~~~~~~~~~~~~
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c:404:9: note: in expansion of macro ‘__WRITE’
  404 |         __WRITE(FD,&(Buffer[a]), sizeof(unsigned char));
      |         ^~~~~~~
In file included from /home/marco-gwt/GWT/AutotilerV2/Emulation/at_api.h:21,
                 from /home/marco-gwt/GWT/AutotilerV2/Emulation/Gap.h:18,
                 from /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_AT_Misc.c:7:
/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_AT_Misc.c: In function ‘AT_TensorGetNextPage’:
/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_AT_Misc.c:79:63: warning: cast from pointer to integer of different size [-Wpointer-to-int-cast]
   79 |    AT_HYPERFLASH_FS_CL_COPY((AT_HYPERFLASH_FS_T *) L3_Device, (AT_HYPERFLASH_FS_EXT_ADDR_TYPE) (Addr+Offset), (AT_HYPERFLASH_FS_INT_ADDR_TYPE) L2_BufferAddr, Size, 0, L3_Event);
      |                                                               ^
/home/marco-gwt/GWT/AutotilerV2/Emulation/at_api_emul.h:225:36: note: in definition of macro ‘AT_HYPERFLASH_FS_CL_COPY’
  225 |   __at_hyperflash_fs_copy(*(file), ext, loc, size, dir)
      |                                    ^~~
/home/marco-gwt/GWT/AutotilerV2/Emulation/at_api_emul.h: In function ‘__at_hyperflash_fs_copy’:
/home/marco-gwt/GWT/AutotilerV2/Emulation/at_api_emul.h:173:8: warning: ignoring return value of ‘fread’, declared with attribute warn_unused_result [-Wunused-result]
  173 |   else fread(loc, 1, size, file);
      |        ^~~~~~~~~~~~~~~~~~~~~~~~~
In file included from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/wavIO.h:13,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/wav_io/wavIO.c:9:
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:46:20: warning: useless storage class specifier in empty declaration
   46 |     typedef struct pi_device {};
      |                    ^~~~~~~~~
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/wav_io/wavIO.c: In function ‘WriteWavToFileNew’:
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:37:57: warning: initialization of ‘void *’ from ‘int’ makes pointer from integer without a cast [-Wint-conversion]
   37 |     #define __OPEN_WRITE(__FS, __NAME)                  open(__NAME, O_RDWR | O_CREAT, S_IRWXU)
      |                                                         ^~~~
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/wav_io/wavIO.c:236:18: note: in expansion of macro ‘__OPEN_WRITE’
  236 |     void *File = __OPEN_WRITE(fs, FileName);
      |                  ^~~~~~~~~~~~
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/wav_io/wavIO.c:328:20: warning: passing argument 1 of ‘write’ makes integer from pointer without a cast [-Wint-conversion]
  328 |     ret += __WRITE(File, header_buffer, WAV_HEADER_SIZE);
      |                    ^~~~
      |                    |
      |                    void *
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:41:63: note: in definition of macro ‘__WRITE’
   41 |     #define __WRITE(__FD, __BUF, __LEN)                 write(__FD, __BUF, __LEN)
      |                                                               ^~~~
In file included from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:29,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/wavIO.h:13,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/wav_io/wavIO.c:9:
/usr/include/unistd.h:366:27: note: expected ‘int’ but argument is of type ‘void *’
  366 | extern ssize_t write (int __fd, const void *__buf, size_t __n) __wur;
      |                       ~~~~^~~~
In file included from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/wavIO.h:13,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/wav_io/wavIO.c:9:
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/wav_io/wavIO.c:335:21: warning: passing argument 1 of ‘write’ makes integer from pointer without a cast [-Wint-conversion]
  335 |      ret += __WRITE(File, data, Size);
      |                     ^~~~
      |                     |
      |                     void *
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:41:63: note: in definition of macro ‘__WRITE’
   41 |     #define __WRITE(__FD, __BUF, __LEN)                 write(__FD, __BUF, __LEN)
      |                                                               ^~~~
In file included from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:29,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/wavIO.h:13,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/wav_io/wavIO.c:9:
/usr/include/unistd.h:366:27: note: expected ‘int’ but argument is of type ‘void *’
  366 | extern ssize_t write (int __fd, const void *__buf, size_t __n) __wur;
      |                       ~~~~^~~~
In file included from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/wavIO.h:13,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/wav_io/wavIO.c:9:
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/wav_io/wavIO.c:338:13: warning: passing argument 1 of ‘close’ makes integer from pointer without a cast [-Wint-conversion]
  338 |     __CLOSE(File);
      |             ^~~~
      |             |
      |             void *
/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:38:63: note: in definition of macro ‘__CLOSE’
   38 |     #define __CLOSE(__FD)                               close(__FD)
      |                                                               ^~~~
In file included from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/fs_switch.h:29,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include/gaplib/wavIO.h:13,
                 from /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/wav_io/wavIO.c:9:
/usr/include/unistd.h:353:23: note: expected ‘int’ but argument is of type ‘void *’
  353 | extern int close (int __fd);
      |                   ~~~~^~~~
make -f emul.mk clean_model clean all DUMP_TENSORS=0 SMALL=0 MEDIUM=0 LARGE=1 WITH_MFCC=1 USE_POWER=1
make[1]: Entering directory '/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting'
script model/nntool_script
GEN ... /home/marco-gwt/GWT/AutotilerV2/CNN_Generators/CNN_Generator_Util.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators/CNN_Copy_Generators.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators/SSD_Generators.c /home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes/ResizeGenerator.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators_SQ8/CNN_Generators_SQ8.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators_SQ8/RNN_Generators_SQ8.c
rm -f -rf BUILD_MODEL_SQ8_EMUL
rm -f -r BUILD_EMUL
rm -f kws_ds_cnn_emul
mkdir BUILD_MODEL_SQ8_EMUL
cp model/KWS_ds_cnn_l_quant_power.tflite BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_l_quant.tflite
echo "GENERATING NNTOOL STATE FILE"
GENERATING NNTOOL STATE FILE
nntool -s model/nntool_script BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_l_quant.tflite -q
settings - set log level to INFO
log_level - was: 'INFO'
now: 'INFO'
open - opening graph file BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_l_quant.tflite load_quantization = True
tflite - Importing TFLITE model version 3
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
forwards SOFTMAX_0_13 in: -33.88<(i8-0.00)*0.26467398<33.61 out: None stop [] fusion False
handler SoftmaxTanHMult selected for SoftMaxParameters(SOFTMAX_0_13)
forwards handler SOFTMAX_0_13 returned in: -64.00<(i8-0.00)*0.50000000<63.50 forced out: -1.00<(i16-0.00)*0.00003052<1.00 fusion False
forwards in edge 0 does not match was -33.88<(i8-0.00)*0.26467398<33.61 need -64.00<(i8-0.00)*0.50000000<63.50 forced
go backwackwards to F 12x1x1x276 B 1 
backwards FULLY_CONNECTED_0_12 in: -8.63<(i8-0.00)*0.06741016<8.56,chan<(i8-0.00)*chan<chan,chan<(i32-0.00)*chan<chan out: -64.00<(i8-0.00)*0.50000000<63.50 forced stop SOFTMAX_0_13 fusion False
handler FilterMult selected for FcParameters(FULLY_CONNECTED_0_12)
filter_mult - selecting SQ8 software kernel filter quantizer
filter_mult - node FULLY_CONNECTED_0_12 output forced to range [-64.]/[63.5] - actual range [-33.878273]/[33.61358] symmetric
unified_quantization_handler - indicating change of FULLY_CONNECTED_0_12 input from c, out_cin_c, out_c to chw, out_cin_chw, out_c order - rerun adjust command
unified_quantization_handler - indicating change of FULLY_CONNECTED_0_12 output from c to chw order - rerun adjust command
backwards handler FULLY_CONNECTED_0_12 returned in: -8.63<(i8-0.00)*0.06741016<8.56,chan<(i8-0.00)*chan<chan,chan<(i32-0.00)*chan<chan out: -64.00<(i8-0.00)*0.50000000<63.50 forced force_removed False fusion False
backwards finished in_edges FULLY_CONNECTED_0_12
backwards out edge 0 does not match was -33.88<(i8-0.00)*0.26467398<33.61 need -64.00<(i8-0.00)*0.50000000<63.50 forced
---- STOPPED AT SOFTMAX_0_13
backwards finished out_edges FULLY_CONNECTED_0_12
forwards finished in edges SOFTMAX_0_13
forwards at SOFTMAX_0_13 on out edge 0
forwards output_1 in: -1.00<(i16-0.00)*0.00003052<1.00 out: None stop [] fusion False
handler OutputMult selected for OutputParameters(output_1)
forwards handler output_1 returned in: -1.00<(i16-0.00)*0.00003052<1.00 out: -1.00<(i16-0.00)*0.00003052<1.00 fusion False
forwards finished in edges output_1
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
debug - was: False
now: True
adjust_order - adding transposes to correct tensor order for AT kernels
set_aliases - looking for aliased edges
eliminate_transposes - eliminating unnecessary transposes
eliminate_transposes - search for transposes
eliminate_transposes - ++ Starting up from CONV_2D_0_0[1]
eliminate_transposes - looking up at DSCNNconv_1weights_quantFakeQu[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_1weights_quantFakeQu - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for CONV_2D_0_0[1]
eliminate_transposes - ++ Starting down from CONV_2D_0_0[0]
eliminate_transposes - looking down at CONV_2D_0_0_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_1[0] transpose [1, 2, 0]
eliminate_transposes - accepted DEPTHWISE_CONV_2D_0_1 - transpose in (1)
eliminate_transposes - ++ Found results for CONV_2D_0_0[0]
eliminate_transposes - ++ Starting up from DEPTHWISE_CONV_2D_0_1[1]
eliminate_transposes - looking up at DSCNNconv_ds_1dw_convweights_q[0] transpose [3, 0, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_1dw_convweights_q - constant input - transpose constant [3, 0, 1, 2]
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_1[1]
eliminate_transposes - ++ Starting down from DEPTHWISE_CONV_2D_0_1[0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_1_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at CONV_2D_0_2[0] transpose [1, 2, 0]
eliminate_transposes - accepted CONV_2D_0_2 - transpose in (1)
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_1[0]
eliminate_transposes - ++ Starting up from CONV_2D_0_2[1]
eliminate_transposes - looking up at DSCNNconv_ds_1pw_convweights_q[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_1pw_convweights_q - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for CONV_2D_0_2[1]
eliminate_transposes - ++ Starting down from CONV_2D_0_2[0]
eliminate_transposes - looking down at CONV_2D_0_2_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_3[0] transpose [1, 2, 0]
eliminate_transposes - accepted DEPTHWISE_CONV_2D_0_3 - transpose in (1)
eliminate_transposes - ++ Found results for CONV_2D_0_2[0]
eliminate_transposes - ++ Starting up from DEPTHWISE_CONV_2D_0_3[1]
eliminate_transposes - looking up at DSCNNconv_ds_2dw_convweights_q[0] transpose [3, 0, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_2dw_convweights_q - constant input - transpose constant [3, 0, 1, 2]
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_3[1]
eliminate_transposes - ++ Starting down from DEPTHWISE_CONV_2D_0_3[0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_3_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at CONV_2D_0_4[0] transpose [1, 2, 0]
eliminate_transposes - accepted CONV_2D_0_4 - transpose in (1)
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_3[0]
eliminate_transposes - ++ Starting up from CONV_2D_0_4[1]
eliminate_transposes - looking up at DSCNNconv_ds_2pw_convweights_q[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_2pw_convweights_q - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for CONV_2D_0_4[1]
eliminate_transposes - ++ Starting down from CONV_2D_0_4[0]
eliminate_transposes - looking down at CONV_2D_0_4_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_5[0] transpose [1, 2, 0]
eliminate_transposes - accepted DEPTHWISE_CONV_2D_0_5 - transpose in (1)
eliminate_transposes - ++ Found results for CONV_2D_0_4[0]
eliminate_transposes - ++ Starting up from DEPTHWISE_CONV_2D_0_5[1]
eliminate_transposes - looking up at DSCNNconv_ds_3dw_convweights_q[0] transpose [3, 0, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_3dw_convweights_q - constant input - transpose constant [3, 0, 1, 2]
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_5[1]
eliminate_transposes - ++ Starting down from DEPTHWISE_CONV_2D_0_5[0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_5_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at CONV_2D_0_6[0] transpose [1, 2, 0]
eliminate_transposes - accepted CONV_2D_0_6 - transpose in (1)
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_5[0]
eliminate_transposes - ++ Starting up from CONV_2D_0_6[1]
eliminate_transposes - looking up at DSCNNconv_ds_3pw_convweights_q[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_3pw_convweights_q - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for CONV_2D_0_6[1]
eliminate_transposes - ++ Starting down from CONV_2D_0_6[0]
eliminate_transposes - looking down at CONV_2D_0_6_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_7[0] transpose [1, 2, 0]
eliminate_transposes - accepted DEPTHWISE_CONV_2D_0_7 - transpose in (1)
eliminate_transposes - ++ Found results for CONV_2D_0_6[0]
eliminate_transposes - ++ Starting up from DEPTHWISE_CONV_2D_0_7[1]
eliminate_transposes - looking up at DSCNNconv_ds_4dw_convweights_q[0] transpose [3, 0, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_4dw_convweights_q - constant input - transpose constant [3, 0, 1, 2]
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_7[1]
eliminate_transposes - ++ Starting down from DEPTHWISE_CONV_2D_0_7[0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_7_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at CONV_2D_0_8[0] transpose [1, 2, 0]
eliminate_transposes - accepted CONV_2D_0_8 - transpose in (1)
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_7[0]
eliminate_transposes - ++ Starting up from CONV_2D_0_8[1]
eliminate_transposes - looking up at DSCNNconv_ds_4pw_convweights_q[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_4pw_convweights_q - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for CONV_2D_0_8[1]
eliminate_transposes - ++ Starting down from CONV_2D_0_8[0]
eliminate_transposes - looking down at CONV_2D_0_8_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_9[0] transpose [1, 2, 0]
eliminate_transposes - accepted DEPTHWISE_CONV_2D_0_9 - transpose in (1)
eliminate_transposes - ++ Found results for CONV_2D_0_8[0]
eliminate_transposes - ++ Starting up from DEPTHWISE_CONV_2D_0_9[1]
eliminate_transposes - looking up at DSCNNconv_ds_5dw_convweights_q[0] transpose [3, 0, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_5dw_convweights_q - constant input - transpose constant [3, 0, 1, 2]
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_9[1]
eliminate_transposes - ++ Starting down from DEPTHWISE_CONV_2D_0_9[0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_9_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at CONV_2D_0_10[0] transpose [1, 2, 0]
eliminate_transposes - accepted CONV_2D_0_10 - transpose in (1)
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_9[0]
eliminate_transposes - ++ Starting up from CONV_2D_0_10[1]
eliminate_transposes - looking up at DSCNNconv_ds_5pw_convweights_q[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_5pw_convweights_q - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for CONV_2D_0_10[1]
eliminate_transposes - ++ Starting down from CONV_2D_0_10[0]
eliminate_transposes - looking down at CONV_2D_0_10_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at AVERAGE_POOL_2D_0_11[0] transpose [1, 2, 0]
eliminate_transposes - accepted AVERAGE_POOL_2D_0_11 - transpose in (1)
eliminate_transposes - ++ Found results for CONV_2D_0_10[0]
eliminate_transposes - ++ Starting down from AVERAGE_POOL_2D_0_11[0]
eliminate_transposes - looking down at FULLY_CONNECTED_0_12[0] transpose [1, 2, 0]
eliminate_transposes - accepted FULLY_CONNECTED_0_12 - linear layer reorder input
eliminate_transposes - ++ Found results for AVERAGE_POOL_2D_0_11[0]
eliminate_transposes - eliminate transposes
eliminate_transposes_actions - Start Action (up): CONV_2D_0_0
eliminate_transposes_actions - CONV_2D_0_0 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_0 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_1weights_quantFakeQu reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_1weights_quantFakeQu
eliminate_transposes_actions - Start Action (down): CONV_2D_0_0
eliminate_transposes_actions - CONV_2D_0_0 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_0 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [3, 0, 1, 2], None]
eliminate_transposes_actions - End Action (down): DEPTHWISE_CONV_2D_0_1
eliminate_transposes_actions - Start Action (up): DEPTHWISE_CONV_2D_0_1
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_1dw_convweights_q reorder constant input to (3, 0, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_1dw_convweights_q
eliminate_transposes_actions - Start Action (down): DEPTHWISE_CONV_2D_0_1
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_2 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_2 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [0, 3, 1, 2], None]
eliminate_transposes_actions - End Action (down): CONV_2D_0_2
eliminate_transposes_actions - Start Action (up): CONV_2D_0_2
eliminate_transposes_actions - CONV_2D_0_2 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_2 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_1pw_convweights_q reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_1pw_convweights_q
eliminate_transposes_actions - Start Action (down): CONV_2D_0_2
eliminate_transposes_actions - CONV_2D_0_2 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_2 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [3, 0, 1, 2], None]
eliminate_transposes_actions - End Action (down): DEPTHWISE_CONV_2D_0_3
eliminate_transposes_actions - Start Action (up): DEPTHWISE_CONV_2D_0_3
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_2dw_convweights_q reorder constant input to (3, 0, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_2dw_convweights_q
eliminate_transposes_actions - Start Action (down): DEPTHWISE_CONV_2D_0_3
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_4 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_4 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [0, 3, 1, 2], None]
eliminate_transposes_actions - End Action (down): CONV_2D_0_4
eliminate_transposes_actions - Start Action (up): CONV_2D_0_4
eliminate_transposes_actions - CONV_2D_0_4 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_4 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_2pw_convweights_q reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_2pw_convweights_q
eliminate_transposes_actions - Start Action (down): CONV_2D_0_4
eliminate_transposes_actions - CONV_2D_0_4 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_4 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [3, 0, 1, 2], None]
eliminate_transposes_actions - End Action (down): DEPTHWISE_CONV_2D_0_5
eliminate_transposes_actions - Start Action (up): DEPTHWISE_CONV_2D_0_5
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_3dw_convweights_q reorder constant input to (3, 0, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_3dw_convweights_q
eliminate_transposes_actions - Start Action (down): DEPTHWISE_CONV_2D_0_5
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_6 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_6 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [0, 3, 1, 2], None]
eliminate_transposes_actions - End Action (down): CONV_2D_0_6
eliminate_transposes_actions - Start Action (up): CONV_2D_0_6
eliminate_transposes_actions - CONV_2D_0_6 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_6 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_3pw_convweights_q reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_3pw_convweights_q
eliminate_transposes_actions - Start Action (down): CONV_2D_0_6
eliminate_transposes_actions - CONV_2D_0_6 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_6 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [3, 0, 1, 2], None]
eliminate_transposes_actions - End Action (down): DEPTHWISE_CONV_2D_0_7
eliminate_transposes_actions - Start Action (up): DEPTHWISE_CONV_2D_0_7
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_4dw_convweights_q reorder constant input to (3, 0, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_4dw_convweights_q
eliminate_transposes_actions - Start Action (down): DEPTHWISE_CONV_2D_0_7
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_8 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_8 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [0, 3, 1, 2], None]
eliminate_transposes_actions - End Action (down): CONV_2D_0_8
eliminate_transposes_actions - Start Action (up): CONV_2D_0_8
eliminate_transposes_actions - CONV_2D_0_8 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_8 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_4pw_convweights_q reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_4pw_convweights_q
eliminate_transposes_actions - Start Action (down): CONV_2D_0_8
eliminate_transposes_actions - CONV_2D_0_8 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_8 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_9 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_9 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [3, 0, 1, 2], None]
eliminate_transposes_actions - End Action (down): DEPTHWISE_CONV_2D_0_9
eliminate_transposes_actions - Start Action (up): DEPTHWISE_CONV_2D_0_9
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_9 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_9 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_5dw_convweights_q reorder constant input to (3, 0, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_5dw_convweights_q
eliminate_transposes_actions - Start Action (down): DEPTHWISE_CONV_2D_0_9
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_9 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_9 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_10 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_10 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [0, 3, 1, 2], None]
eliminate_transposes_actions - End Action (down): CONV_2D_0_10
eliminate_transposes_actions - Start Action (up): CONV_2D_0_10
eliminate_transposes_actions - CONV_2D_0_10 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_10 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_5pw_convweights_q reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_5pw_convweights_q
eliminate_transposes_actions - Start Action (down): CONV_2D_0_10
eliminate_transposes_actions - CONV_2D_0_10 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_10 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - AVERAGE_POOL_2D_0_11 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - AVERAGE_POOL_2D_0_11 delete transpose in[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - End Action (down): AVERAGE_POOL_2D_0_11
eliminate_transposes_actions - Start Action (down): AVERAGE_POOL_2D_0_11
eliminate_transposes_actions - AVERAGE_POOL_2D_0_11 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - AVERAGE_POOL_2D_0_11 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - reorder linear layer FULLY_CONNECTED_0_12 in with shape 1x1x276 transposed (2, 0, 1)
eliminate_transposes_actions - End Action (down): FULLY_CONNECTED_0_12
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
eliminate_transposes - search for transposes
eliminate_transposes - no transposes to eliminate found
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
eliminate_transposes - no further transpose sequences found
set_aliases - looking for aliased edges
nngraph - adjusted order
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
duplicate_operations - match_duplicate_operations does not handle quantized graphs
match_gap_conv - fusing nodes CONV_2D_0_0,CONV_2D_0_0_activation
match_gap_conv - fusing nodes DEPTHWISE_CONV_2D_0_1,DEPTHWISE_CONV_2D_0_1_activation
match_gap_conv - fusing nodes CONV_2D_0_2,CONV_2D_0_2_activation
match_gap_conv - fusing nodes DEPTHWISE_CONV_2D_0_3,DEPTHWISE_CONV_2D_0_3_activation
match_gap_conv - fusing nodes CONV_2D_0_4,CONV_2D_0_4_activation
match_gap_conv - fusing nodes DEPTHWISE_CONV_2D_0_5,DEPTHWISE_CONV_2D_0_5_activation
match_gap_conv - fusing nodes CONV_2D_0_6,CONV_2D_0_6_activation
match_gap_conv - fusing nodes DEPTHWISE_CONV_2D_0_7,DEPTHWISE_CONV_2D_0_7_activation
match_gap_conv - fusing nodes CONV_2D_0_8,CONV_2D_0_8_activation
match_gap_conv - fusing nodes DEPTHWISE_CONV_2D_0_9,DEPTHWISE_CONV_2D_0_9_activation
match_gap_conv - fusing nodes CONV_2D_0_10,CONV_2D_0_10_activation
matcher - ++ fusion fuse_gap_convs modified graph
set_aliases - looking for aliased edges
set_aliases - looking for aliased edges
duplicate_operations - match_duplicate_operations does not handle quantized graphs
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
graph_produce_node_names - was: False
now: True
graph_produce_operinfos - was: False
now: True
graph_monitor_cycles - was: False
now: True
graph_const_exec_from_flash - was: False
now: False
save_state - saved state to /home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_l_quant.json
echo "GENERATING AUTOTILER MODEL"
GENERATING AUTOTILER MODEL
nntool -g -M BUILD_MODEL_SQ8_EMUL -m KWS_ds_cnn_l_quantModel.c -T BUILD_MODEL_SQ8_EMUL/tensors -H KWS_ds_cnn_l_quantInfo.h  BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_l_quant.json
settings - set log level to INFO
log_level - was: 'INFO'
now: 'INFO'
open - opening graph file BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_l_quant.tflite load_quantization = True
tflite - Importing TFLITE model version 3
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
forwards SOFTMAX_0_13 in: -33.88<(i8-0.00)*0.26467398<33.61 out: None stop [] fusion False
handler SoftmaxTanHMult selected for SoftMaxParameters(SOFTMAX_0_13)
forwards handler SOFTMAX_0_13 returned in: -64.00<(i8-0.00)*0.50000000<63.50 forced out: -1.00<(i16-0.00)*0.00003052<1.00 fusion False
forwards in edge 0 does not match was -33.88<(i8-0.00)*0.26467398<33.61 need -64.00<(i8-0.00)*0.50000000<63.50 forced
go backwackwards to F 12x1x1x276 B 1 
backwards FULLY_CONNECTED_0_12 in: -8.63<(i8-0.00)*0.06741016<8.56,chan<(i8-0.00)*chan<chan,chan<(i32-0.00)*chan<chan out: -64.00<(i8-0.00)*0.50000000<63.50 forced stop SOFTMAX_0_13 fusion False
handler FilterMult selected for FcParameters(FULLY_CONNECTED_0_12)
filter_mult - selecting SQ8 software kernel filter quantizer
filter_mult - node FULLY_CONNECTED_0_12 output forced to range [-64.]/[63.5] - actual range [-33.878273]/[33.61358] symmetric
unified_quantization_handler - indicating change of FULLY_CONNECTED_0_12 input from c, out_cin_c, out_c to chw, out_cin_chw, out_c order - rerun adjust command
unified_quantization_handler - indicating change of FULLY_CONNECTED_0_12 output from c to chw order - rerun adjust command
backwards handler FULLY_CONNECTED_0_12 returned in: -8.63<(i8-0.00)*0.06741016<8.56,chan<(i8-0.00)*chan<chan,chan<(i32-0.00)*chan<chan out: -64.00<(i8-0.00)*0.50000000<63.50 forced force_removed False fusion False
backwards finished in_edges FULLY_CONNECTED_0_12
backwards out edge 0 does not match was -33.88<(i8-0.00)*0.26467398<33.61 need -64.00<(i8-0.00)*0.50000000<63.50 forced
---- STOPPED AT SOFTMAX_0_13
backwards finished out_edges FULLY_CONNECTED_0_12
forwards finished in edges SOFTMAX_0_13
forwards at SOFTMAX_0_13 on out edge 0
forwards output_1 in: -1.00<(i16-0.00)*0.00003052<1.00 out: None stop [] fusion False
handler OutputMult selected for OutputParameters(output_1)
forwards handler output_1 returned in: -1.00<(i16-0.00)*0.00003052<1.00 out: -1.00<(i16-0.00)*0.00003052<1.00 fusion False
forwards finished in edges output_1
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
debug - was: False
now: True
adjust_order - adding transposes to correct tensor order for AT kernels
set_aliases - looking for aliased edges
eliminate_transposes - eliminating unnecessary transposes
eliminate_transposes - search for transposes
eliminate_transposes - ++ Starting up from CONV_2D_0_0[1]
eliminate_transposes - looking up at DSCNNconv_1weights_quantFakeQu[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_1weights_quantFakeQu - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for CONV_2D_0_0[1]
eliminate_transposes - ++ Starting down from CONV_2D_0_0[0]
eliminate_transposes - looking down at CONV_2D_0_0_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_1[0] transpose [1, 2, 0]
eliminate_transposes - accepted DEPTHWISE_CONV_2D_0_1 - transpose in (1)
eliminate_transposes - ++ Found results for CONV_2D_0_0[0]
eliminate_transposes - ++ Starting up from DEPTHWISE_CONV_2D_0_1[1]
eliminate_transposes - looking up at DSCNNconv_ds_1dw_convweights_q[0] transpose [3, 0, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_1dw_convweights_q - constant input - transpose constant [3, 0, 1, 2]
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_1[1]
eliminate_transposes - ++ Starting down from DEPTHWISE_CONV_2D_0_1[0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_1_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at CONV_2D_0_2[0] transpose [1, 2, 0]
eliminate_transposes - accepted CONV_2D_0_2 - transpose in (1)
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_1[0]
eliminate_transposes - ++ Starting up from CONV_2D_0_2[1]
eliminate_transposes - looking up at DSCNNconv_ds_1pw_convweights_q[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_1pw_convweights_q - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for CONV_2D_0_2[1]
eliminate_transposes - ++ Starting down from CONV_2D_0_2[0]
eliminate_transposes - looking down at CONV_2D_0_2_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_3[0] transpose [1, 2, 0]
eliminate_transposes - accepted DEPTHWISE_CONV_2D_0_3 - transpose in (1)
eliminate_transposes - ++ Found results for CONV_2D_0_2[0]
eliminate_transposes - ++ Starting up from DEPTHWISE_CONV_2D_0_3[1]
eliminate_transposes - looking up at DSCNNconv_ds_2dw_convweights_q[0] transpose [3, 0, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_2dw_convweights_q - constant input - transpose constant [3, 0, 1, 2]
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_3[1]
eliminate_transposes - ++ Starting down from DEPTHWISE_CONV_2D_0_3[0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_3_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at CONV_2D_0_4[0] transpose [1, 2, 0]
eliminate_transposes - accepted CONV_2D_0_4 - transpose in (1)
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_3[0]
eliminate_transposes - ++ Starting up from CONV_2D_0_4[1]
eliminate_transposes - looking up at DSCNNconv_ds_2pw_convweights_q[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_2pw_convweights_q - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for CONV_2D_0_4[1]
eliminate_transposes - ++ Starting down from CONV_2D_0_4[0]
eliminate_transposes - looking down at CONV_2D_0_4_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_5[0] transpose [1, 2, 0]
eliminate_transposes - accepted DEPTHWISE_CONV_2D_0_5 - transpose in (1)
eliminate_transposes - ++ Found results for CONV_2D_0_4[0]
eliminate_transposes - ++ Starting up from DEPTHWISE_CONV_2D_0_5[1]
eliminate_transposes - looking up at DSCNNconv_ds_3dw_convweights_q[0] transpose [3, 0, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_3dw_convweights_q - constant input - transpose constant [3, 0, 1, 2]
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_5[1]
eliminate_transposes - ++ Starting down from DEPTHWISE_CONV_2D_0_5[0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_5_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at CONV_2D_0_6[0] transpose [1, 2, 0]
eliminate_transposes - accepted CONV_2D_0_6 - transpose in (1)
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_5[0]
eliminate_transposes - ++ Starting up from CONV_2D_0_6[1]
eliminate_transposes - looking up at DSCNNconv_ds_3pw_convweights_q[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_3pw_convweights_q - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for CONV_2D_0_6[1]
eliminate_transposes - ++ Starting down from CONV_2D_0_6[0]
eliminate_transposes - looking down at CONV_2D_0_6_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_7[0] transpose [1, 2, 0]
eliminate_transposes - accepted DEPTHWISE_CONV_2D_0_7 - transpose in (1)
eliminate_transposes - ++ Found results for CONV_2D_0_6[0]
eliminate_transposes - ++ Starting up from DEPTHWISE_CONV_2D_0_7[1]
eliminate_transposes - looking up at DSCNNconv_ds_4dw_convweights_q[0] transpose [3, 0, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_4dw_convweights_q - constant input - transpose constant [3, 0, 1, 2]
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_7[1]
eliminate_transposes - ++ Starting down from DEPTHWISE_CONV_2D_0_7[0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_7_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at CONV_2D_0_8[0] transpose [1, 2, 0]
eliminate_transposes - accepted CONV_2D_0_8 - transpose in (1)
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_7[0]
eliminate_transposes - ++ Starting up from CONV_2D_0_8[1]
eliminate_transposes - looking up at DSCNNconv_ds_4pw_convweights_q[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_4pw_convweights_q - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for CONV_2D_0_8[1]
eliminate_transposes - ++ Starting down from CONV_2D_0_8[0]
eliminate_transposes - looking down at CONV_2D_0_8_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_9[0] transpose [1, 2, 0]
eliminate_transposes - accepted DEPTHWISE_CONV_2D_0_9 - transpose in (1)
eliminate_transposes - ++ Found results for CONV_2D_0_8[0]
eliminate_transposes - ++ Starting up from DEPTHWISE_CONV_2D_0_9[1]
eliminate_transposes - looking up at DSCNNconv_ds_5dw_convweights_q[0] transpose [3, 0, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_5dw_convweights_q - constant input - transpose constant [3, 0, 1, 2]
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_9[1]
eliminate_transposes - ++ Starting down from DEPTHWISE_CONV_2D_0_9[0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_9_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at CONV_2D_0_10[0] transpose [1, 2, 0]
eliminate_transposes - accepted CONV_2D_0_10 - transpose in (1)
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_9[0]
eliminate_transposes - ++ Starting up from CONV_2D_0_10[1]
eliminate_transposes - looking up at DSCNNconv_ds_5pw_convweights_q[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_5pw_convweights_q - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for CONV_2D_0_10[1]
eliminate_transposes - ++ Starting down from CONV_2D_0_10[0]
eliminate_transposes - looking down at CONV_2D_0_10_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at AVERAGE_POOL_2D_0_11[0] transpose [1, 2, 0]
eliminate_transposes - accepted AVERAGE_POOL_2D_0_11 - transpose in (1)
eliminate_transposes - ++ Found results for CONV_2D_0_10[0]
eliminate_transposes - ++ Starting down from AVERAGE_POOL_2D_0_11[0]
eliminate_transposes - looking down at FULLY_CONNECTED_0_12[0] transpose [1, 2, 0]
eliminate_transposes - accepted FULLY_CONNECTED_0_12 - linear layer reorder input
eliminate_transposes - ++ Found results for AVERAGE_POOL_2D_0_11[0]
eliminate_transposes - eliminate transposes
eliminate_transposes_actions - Start Action (up): CONV_2D_0_0
eliminate_transposes_actions - CONV_2D_0_0 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_0 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_1weights_quantFakeQu reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_1weights_quantFakeQu
eliminate_transposes_actions - Start Action (down): CONV_2D_0_0
eliminate_transposes_actions - CONV_2D_0_0 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_0 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [3, 0, 1, 2], None]
eliminate_transposes_actions - End Action (down): DEPTHWISE_CONV_2D_0_1
eliminate_transposes_actions - Start Action (up): DEPTHWISE_CONV_2D_0_1
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_1dw_convweights_q reorder constant input to (3, 0, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_1dw_convweights_q
eliminate_transposes_actions - Start Action (down): DEPTHWISE_CONV_2D_0_1
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_2 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_2 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [0, 3, 1, 2], None]
eliminate_transposes_actions - End Action (down): CONV_2D_0_2
eliminate_transposes_actions - Start Action (up): CONV_2D_0_2
eliminate_transposes_actions - CONV_2D_0_2 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_2 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_1pw_convweights_q reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_1pw_convweights_q
eliminate_transposes_actions - Start Action (down): CONV_2D_0_2
eliminate_transposes_actions - CONV_2D_0_2 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_2 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [3, 0, 1, 2], None]
eliminate_transposes_actions - End Action (down): DEPTHWISE_CONV_2D_0_3
eliminate_transposes_actions - Start Action (up): DEPTHWISE_CONV_2D_0_3
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_2dw_convweights_q reorder constant input to (3, 0, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_2dw_convweights_q
eliminate_transposes_actions - Start Action (down): DEPTHWISE_CONV_2D_0_3
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_4 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_4 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [0, 3, 1, 2], None]
eliminate_transposes_actions - End Action (down): CONV_2D_0_4
eliminate_transposes_actions - Start Action (up): CONV_2D_0_4
eliminate_transposes_actions - CONV_2D_0_4 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_4 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_2pw_convweights_q reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_2pw_convweights_q
eliminate_transposes_actions - Start Action (down): CONV_2D_0_4
eliminate_transposes_actions - CONV_2D_0_4 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_4 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [3, 0, 1, 2], None]
eliminate_transposes_actions - End Action (down): DEPTHWISE_CONV_2D_0_5
eliminate_transposes_actions - Start Action (up): DEPTHWISE_CONV_2D_0_5
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_3dw_convweights_q reorder constant input to (3, 0, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_3dw_convweights_q
eliminate_transposes_actions - Start Action (down): DEPTHWISE_CONV_2D_0_5
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_6 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_6 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [0, 3, 1, 2], None]
eliminate_transposes_actions - End Action (down): CONV_2D_0_6
eliminate_transposes_actions - Start Action (up): CONV_2D_0_6
eliminate_transposes_actions - CONV_2D_0_6 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_6 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_3pw_convweights_q reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_3pw_convweights_q
eliminate_transposes_actions - Start Action (down): CONV_2D_0_6
eliminate_transposes_actions - CONV_2D_0_6 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_6 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [3, 0, 1, 2], None]
eliminate_transposes_actions - End Action (down): DEPTHWISE_CONV_2D_0_7
eliminate_transposes_actions - Start Action (up): DEPTHWISE_CONV_2D_0_7
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_4dw_convweights_q reorder constant input to (3, 0, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_4dw_convweights_q
eliminate_transposes_actions - Start Action (down): DEPTHWISE_CONV_2D_0_7
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_8 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_8 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [0, 3, 1, 2], None]
eliminate_transposes_actions - End Action (down): CONV_2D_0_8
eliminate_transposes_actions - Start Action (up): CONV_2D_0_8
eliminate_transposes_actions - CONV_2D_0_8 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_8 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_4pw_convweights_q reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_4pw_convweights_q
eliminate_transposes_actions - Start Action (down): CONV_2D_0_8
eliminate_transposes_actions - CONV_2D_0_8 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_8 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_9 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_9 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [3, 0, 1, 2], None]
eliminate_transposes_actions - End Action (down): DEPTHWISE_CONV_2D_0_9
eliminate_transposes_actions - Start Action (up): DEPTHWISE_CONV_2D_0_9
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_9 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_9 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_5dw_convweights_q reorder constant input to (3, 0, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_5dw_convweights_q
eliminate_transposes_actions - Start Action (down): DEPTHWISE_CONV_2D_0_9
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_9 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_9 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_10 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_10 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [0, 3, 1, 2], None]
eliminate_transposes_actions - End Action (down): CONV_2D_0_10
eliminate_transposes_actions - Start Action (up): CONV_2D_0_10
eliminate_transposes_actions - CONV_2D_0_10 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_10 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_5pw_convweights_q reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_5pw_convweights_q
eliminate_transposes_actions - Start Action (down): CONV_2D_0_10
eliminate_transposes_actions - CONV_2D_0_10 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_10 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - AVERAGE_POOL_2D_0_11 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - AVERAGE_POOL_2D_0_11 delete transpose in[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - End Action (down): AVERAGE_POOL_2D_0_11
eliminate_transposes_actions - Start Action (down): AVERAGE_POOL_2D_0_11
eliminate_transposes_actions - AVERAGE_POOL_2D_0_11 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - AVERAGE_POOL_2D_0_11 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - reorder linear layer FULLY_CONNECTED_0_12 in with shape 1x1x276 transposed (2, 0, 1)
eliminate_transposes_actions - End Action (down): FULLY_CONNECTED_0_12
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
eliminate_transposes - search for transposes
eliminate_transposes - no transposes to eliminate found
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
eliminate_transposes - no further transpose sequences found
set_aliases - looking for aliased edges
nngraph - adjusted order
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
duplicate_operations - match_duplicate_operations does not handle quantized graphs
match_gap_conv - fusing nodes CONV_2D_0_0,CONV_2D_0_0_activation
match_gap_conv - fusing nodes DEPTHWISE_CONV_2D_0_1,DEPTHWISE_CONV_2D_0_1_activation
match_gap_conv - fusing nodes CONV_2D_0_2,CONV_2D_0_2_activation
match_gap_conv - fusing nodes DEPTHWISE_CONV_2D_0_3,DEPTHWISE_CONV_2D_0_3_activation
match_gap_conv - fusing nodes CONV_2D_0_4,CONV_2D_0_4_activation
match_gap_conv - fusing nodes DEPTHWISE_CONV_2D_0_5,DEPTHWISE_CONV_2D_0_5_activation
match_gap_conv - fusing nodes CONV_2D_0_6,CONV_2D_0_6_activation
match_gap_conv - fusing nodes DEPTHWISE_CONV_2D_0_7,DEPTHWISE_CONV_2D_0_7_activation
match_gap_conv - fusing nodes CONV_2D_0_8,CONV_2D_0_8_activation
match_gap_conv - fusing nodes DEPTHWISE_CONV_2D_0_9,DEPTHWISE_CONV_2D_0_9_activation
match_gap_conv - fusing nodes CONV_2D_0_10,CONV_2D_0_10_activation
matcher - ++ fusion fuse_gap_convs modified graph
set_aliases - looking for aliased edges
set_aliases - looking for aliased edges
duplicate_operations - match_duplicate_operations does not handle quantized graphs
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
graph_produce_node_names - was: False
now: True
graph_produce_operinfos - was: False
now: True
graph_monitor_cycles - was: False
now: True
graph_const_exec_from_flash - was: False
now: False
generator - Saving model to BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_l_quantModel.c
code_generator - edge from step 1 CONV_2D_0_0_r_hwc_chw is not used and is replaced with edge from step input_1:0 0 cname: Input_1
generator - Writing constants to BUILD_MODEL_SQ8_EMUL
echo "COMPILING AUTOTILER MODEL"
COMPILING AUTOTILER MODEL
gcc -g -o BUILD_MODEL_SQ8_EMUL/GenTile -I. -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/CNN_Generators -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Generators_SQ8 -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 /home/marco-gwt/GWT/AutotilerV2/CNN_Generators/CNN_Generator_Util.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators/CNN_Copy_Generators.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators/SSD_Generators.c /home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes/ResizeGenerator.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators_SQ8/CNN_Generators_SQ8.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators_SQ8/RNN_Generators_SQ8.c BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_l_quantModel.c /home/marco-gwt/GWT/AutotilerV2/install/lib/libtile.a -lSDL2 -lSDL2_ttf 
echo "RUNNING AUTOTILER MODEL"
RUNNING AUTOTILER MODEL
BUILD_MODEL_SQ8_EMUL/GenTile -o BUILD_MODEL_SQ8_EMUL -c BUILD_MODEL_SQ8_EMUL -f BUILD_MODEL_SQ8_EMUL --L1 48736 --L2 350000 --L3 6388608
InFeat: 1, OutFeat: 276
Conv => W:  10, Pad:[1,2] PadT:[1,2] => Wc: 10, Filter:[4,10]
     => H:  49, Pad:[4,5] PadT:[4,5] => Hc: 25
Pool => Wc: 10, Pad:[0,0] => Wo: 10, Filter:[1,1]
     => Hc: 25, Pad:[0,0] => Ho: 25
OverlapC: 8
OverlapP: 0
TileCons: 2
UsedIn  : [10 x 49]
UsedC   : [10 x 25]
      SetBiasKerName: KerParSetBiasB32_SQ8
         ConvKerName: KerParConvNxMStrideSxSy_SQ8
  DPReductionKerName: KerParReduct_CC_ReLU_SQ8
Nb Oper : 2829000

==== Process Tiling For User Kernel:            S4_Conv2d_276x1x10x4_Relu =======================
S4_Conv2d_276x1x10x4_Relu Partition[0] Size = 312657 (Min:    200, Max: 440689), Fraction:       1.00, Giving:  48736 Bytes out of  48736 Bytes

Reference object:                  Out, Dim=25
	                  In Dim:  58, TileOverlap:  8, Ratio: 2.000000
	                 Out Dim:  25, TileOverlap:  0, Ratio: 1.000000
	             ConvOut Dim:  25, TileOverlap:  0, Ratio: 1.000000

S4_Conv2d_276x1x10x4_Relu Full buffering on Arg: Scale, was using 288 Bytes will require 276 Bytes buffer
S4_Conv2d_276x1x10x4_Relu Full buffering on Arg: ScaleN, was using 288 Bytes will require 276 Bytes buffer
S4_Conv2d_276x1x10x4_Relu, TiledSpace: Tile0 Iteration Count: 7 Parametric Space: [D1, M0=144] Parametric Space: [D0, M1=1]
              In : Ratio: 2.000000, FixDim:     10, VarDim:     16 [    52], Size:    320, Total:     320, Move:       1940 (Decl x 3.959184) L2
            Bias : Ratio: 0.000000,                                          Size:   1152, Total:    1472, Move:       1104 (Decl x 1.000000) L2
*          Scale : Ratio: 0.000000,                                          Size:    276, Total:    1748, Move:        276 (Decl x 1.000000) L2
*         ScaleN : Ratio: 0.000000,                                          Size:    276, Total:    2024, Move:        276 (Decl x 1.000000) L2
@         Filter : Ratio: 0.000000,                                          Size:  11520, Total:   13544, Move:      11040 (Decl x 1.000000) L2
             Out : Ratio: 1.000000, FixDim:     10, VarDim:      4 [    25], Size:  11520, Total:   25064, Move:      69000 (Decl x 1.000000) L2
*        ConvOut : Ratio: 1.000000, FixDim:     10, VarDim:      4 [    25], Size:  23040, Total:   48104, Move:          0 (Decl x 0.000000) L2
*          Infos : Ratio: 0.000000,                                          Size:     12, Total:   48116, Move:          9 (Decl x 1.000000) L2
S4_Conv2d_276x1x10x4_Relu - IterSpace: Tile0 - L1 Memory:  48116, L2Move: 83645, L3Move: 0, Tiling Overhead: 1.017641
S4_Conv2d_276x1x10x4_Relu Partial buffering on Arg: Filter, From: D0 To: D1. Current is (Par) 1 x [W:1, H:1] x 40 => Partial buffer size is 11520 Bytes
S4_Conv2d_276x1x10x4_Relu Found Parametric value for space D1 (Initial: 276, Div: 8) = 144 [144*1 + 132] and space D0 (Initial: 1, Div: 4) = 1 [1*1 + 0], Iteration for Tiled Space: 7
Kernel: S4_Conv2d_276x1x10x4_Relu, Arg: In, Last Tile: 2+8, Pad: 5 => Requires Padding of tile N-1
Ker: S4_Conv2d_276x1x10x4_Relu, Arg:         In, Size:    160, Base1:      0, Base2:    160
Ker: S4_Conv2d_276x1x10x4_Relu, Arg:       Bias, Size:    576, Base1:    320, Base2:    896
Ker: S4_Conv2d_276x1x10x4_Relu, Arg:      Scale, Size:    276, Base1:   1472, Base2:      0
Ker: S4_Conv2d_276x1x10x4_Relu, Arg:     ScaleN, Size:    276, Base1:   1748, Base2:      0
Ker: S4_Conv2d_276x1x10x4_Relu, Arg:     Filter, Size:   5760, Base1:   2024, Base2:   7784
Ker: S4_Conv2d_276x1x10x4_Relu, Arg:        Out, Size:   5760, Base1:  13544, Base2:  19304
Ker: S4_Conv2d_276x1x10x4_Relu, Arg:    ConvOut, Size:  23040, Base1:  25064, Base2:      0
Ker: S4_Conv2d_276x1x10x4_Relu, Arg:      Infos, Size:     12, Base1:  48104, Base2:      0
S4_Conv2d_276x1x10x4_Relu For Iter Space: 0 Iteration count:   7 (Last one is truncated), Given L1 Memory:  48736, Used L1 Memory:  48116, Reusable Memory: 620, Used L2 Memory: 0
=================================================================================================

InFeat: 276, OutFeat: 276
Conv => W:  10, Pad:[0,1] PadT:[0,1] => Wc: 5, Filter:[3,3]
     => H:  25, Pad:[1,1] PadT:[1,1] => Hc: 13
Pool => Wc: 5, Pad:[0,0] => Wo: 5, Filter:[1,1]
     => Hc: 13, Pad:[0,0] => Ho: 13
OverlapC: 1
OverlapP: 0
TileCons: 2
UsedIn  : [10 x 25]
UsedC   : [5 x 13]
         ConvKerName: KerParConvDW3x3Stride2B32_SQ8
  DPReductionKerName: KerParReduct_CC_ReLU_SQ8
Nb Oper : 179400

==== Process Tiling For User Kernel:             S7_Conv2d_276x1x3x3_Relu =======================
S7_Conv2d_276x1x3x3_Relu Partition[0] Size = 102145 (Min:     60, Max: 265097), Fraction:       1.00, Giving:  48736 Bytes out of  48736 Bytes

Reference object:                  Out, Dim=13
	                  In Dim:  27, TileOverlap:  1, Ratio: 2.000000
	                 Out Dim:  13, TileOverlap:  0, Ratio: 1.000000
	             ConvOut Dim:  13, TileOverlap:  0, Ratio: 1.000000

S7_Conv2d_276x1x3x3_Relu Full buffering on Arg: Bias, was using 384 Bytes will require 1104 Bytes buffer
S7_Conv2d_276x1x3x3_Relu Full buffering on Arg: Scale, was using 96 Bytes will require 276 Bytes buffer
S7_Conv2d_276x1x3x3_Relu Full buffering on Arg: ScaleN, was using 96 Bytes will require 276 Bytes buffer
S7_Conv2d_276x1x3x3_Relu Full buffering on Arg: Filter, was using 864 Bytes will require 2484 Bytes buffer
S7_Conv2d_276x1x3x3_Relu, TiledSpace: Tile0 Iteration Count: 1 Parametric Space: [D0, M0=48]
              In : Ratio: 2.000000, FixDim:     10, VarDim:     25 [    25], Size:  24000, Total:   24000, Move:      69000 (Decl x 1.000000) L2
*           Bias : Ratio: 0.000000,                                          Size:   1104, Total:   25104, Move:       1104 (Decl x 1.000000) L2
*          Scale : Ratio: 0.000000,                                          Size:    276, Total:   25380, Move:        276 (Decl x 1.000000) L2
*         ScaleN : Ratio: 0.000000,                                          Size:    276, Total:   25656, Move:        276 (Decl x 1.000000) L2
*         Filter : Ratio: 0.000000,                                          Size:   2484, Total:   28140, Move:       2484 (Decl x 1.000000) L2
             Out : Ratio: 1.000000, FixDim:      5, VarDim:     13 [    13], Size:   6240, Total:   34380, Move:      17940 (Decl x 1.000000) L2
*        ConvOut : Ratio: 1.000000, FixDim:      5, VarDim:     13 [    13], Size:  12480, Total:   46860, Move:          0 (Decl x 0.000000) L2
*          Infos : Ratio: 0.000000,                                          Size:     12, Total:   46872, Move:          9 (Decl x 1.000000) L2
S7_Conv2d_276x1x3x3_Relu - IterSpace: Tile0 - L1 Memory:  46872, L2Move: 91089, L3Move: 0, Tiling Overhead: 1.000000
S7_Conv2d_276x1x3x3_Relu Found Parametric value for space D0 (Initial: 276, Div: 8) = 48 [48*5 + 36], Iteration for Tiled Space: 1
Ker: S7_Conv2d_276x1x3x3_Relu, Arg:         In, Size:  12000, Base1:      0, Base2:  12000
Ker: S7_Conv2d_276x1x3x3_Relu, Arg:       Bias, Size:   1104, Base1:  24000, Base2:      0
Ker: S7_Conv2d_276x1x3x3_Relu, Arg:      Scale, Size:    276, Base1:  25104, Base2:      0
Ker: S7_Conv2d_276x1x3x3_Relu, Arg:     ScaleN, Size:    276, Base1:  25380, Base2:      0
Ker: S7_Conv2d_276x1x3x3_Relu, Arg:     Filter, Size:   2484, Base1:  25656, Base2:      0
Ker: S7_Conv2d_276x1x3x3_Relu, Arg:        Out, Size:   3120, Base1:  28140, Base2:  31260
Ker: S7_Conv2d_276x1x3x3_Relu, Arg:    ConvOut, Size:  12480, Base1:  34380, Base2:      0
Ker: S7_Conv2d_276x1x3x3_Relu, Arg:      Infos, Size:     12, Base1:  46860, Base2:      0
S7_Conv2d_276x1x3x3_Relu For Iter Space: 0 Iteration count:   1, Given L1 Memory:  48736, Used L1 Memory:  46872, Reusable Memory: 1864, Used L2 Memory: 0
=================================================================================================

InFeat: 276, OutFeat: 276
Conv => W:  5, Pad:[0,0] PadT:[0,0] => Wc: 5, Filter:[1,1]
     => H:  13, Pad:[0,0] PadT:[0,0] => Hc: 13
Pool => Wc: 5, Pad:[0,0] => Wo: 5, Filter:[1,1]
     => Hc: 13, Pad:[0,0] => Ho: 13
OverlapC: 0
OverlapP: 0
TileCons: 1
UsedIn  : [5 x 13]
UsedC   : [5 x 13]
      SetBiasKerName: KerParSetBiasB32_SQ8
         ConvKerName: KerParConv1x1Stride1_SQ8
  DPReductionKerName: KerParReduct_CC_ReLU_SQ8
Nb Oper : 4969380
Mapping this convolution to matrix multiplication
CNN_MatMul_SQ8: S10_Conv2d_276x276x1x1_Relu
In1  => W:  276, H:  276 
In2  => W:   65, H:  276, w:    5, h:   13, Sx: 1, Sy: 1
Out  => W:   65, H:  276 => Line First
       MatMulKerName: KerParMatMulB32_2x4_ReLU_SQ8

==== Process Tiling For User Kernel:          S10_Conv2d_276x276x1x1_Relu =======================
S10_Conv2d_276x276x1x1_Relu Partition[0] Size =   2233 (Min:   2208, Max:  35921), Fraction:       0.21, Giving:  10399 Bytes out of  48736 Bytes
S10_Conv2d_276x276x1x1_Relu Partition[1] Size =   8232 (Min:   4416, Max: 191088), Fraction:       0.79, Giving:  38336 Bytes out of  48736 Bytes

Reference object:                  In1, Dim=276
	                 In1 Dim: 276, TileOverlap:  0, Ratio: 1.000000
	                Bias Dim: 276, TileOverlap:  0, Ratio: 1.000000
	                 Out Dim: 276, TileOverlap:  0, Ratio: 1.000000
	               Scale Dim: 276, TileOverlap:  0, Ratio: 1.000000
	              ScaleN Dim: 276, TileOverlap:  0, Ratio: 1.000000

S10_Conv2d_276x276x1x1_Relu, TiledSpace: Tile1 Iteration Count: 6
*        KerBuff : Ratio: 0.000000,                                          Size:   1104, Total:    1104, Move:          0 (Decl x 0.000000) L2
             In1 : Ratio: 1.000000, FixDim:    276, VarDim:     48 [   276], Size:  26496, Total:   27600, Move:      76176 (Decl x 1.000000) L2
*           Bias : Ratio: 1.000000,                                          Size:   1104, Total:   28704, Move:       1104 (Decl x 1.000000) L2
             Out : Ratio: 1.000000, FixDim:     65, VarDim:     48 [   276], Size:   6240, Total:   34944, Move:      17940 (Decl x 1.000000) L2
*          Scale : Ratio: 1.000000,                                          Size:    276, Total:   35220, Move:        276 (Decl x 1.000000) L2
*         ScaleN : Ratio: 1.000000,                                          Size:    276, Total:   35496, Move:        276 (Decl x 1.000000) L2
S10_Conv2d_276x276x1x1_Relu - IterSpace: Tile1 - L1 Memory:  35496, L2Move: 95772, L3Move: 0, Tiling Overhead: 1.000000
S10_Conv2d_276x276x1x1_Relu Iteration for Tiled Space: 6
Ker: S10_Conv2d_276x276x1x1_Relu, Arg:    KerBuff, Size:   1104, Base1:      0, Base2:      0
Ker: S10_Conv2d_276x276x1x1_Relu, Arg:        In1, Size:  13248, Base1:   1104, Base2:  14352
Ker: S10_Conv2d_276x276x1x1_Relu, Arg:       Bias, Size:   1104, Base1:  27600, Base2:      0
Ker: S10_Conv2d_276x276x1x1_Relu, Arg:        Out, Size:   3120, Base1:  28704, Base2:  31824
Ker: S10_Conv2d_276x276x1x1_Relu, Arg:      Scale, Size:    276, Base1:  34944, Base2:      0
Ker: S10_Conv2d_276x276x1x1_Relu, Arg:     ScaleN, Size:    276, Base1:  35220, Base2:      0
S10_Conv2d_276x276x1x1_Relu For Iter Space: 1 Iteration count:   6 (Last one is truncated), Given L1 Memory:  38336, Used L1 Memory:  35496, Reusable Memory: 2840, Used L2 Memory: 0

Reference object:                  In2, Dim=65
	                 In2 Dim:  65, TileOverlap:  0, Ratio: 1.000000

S10_Conv2d_276x276x1x1_Relu, TiledSpace: Tile0 Iteration Count: 5
             In2 : Ratio: 1.000000, FixDim:    276, VarDim:     16 [    65], Size:   8832, Total:    8832, Move:     107640 (Decl x 6.000000) L2
*          Infos : Ratio: 0.000000,                                          Size:     12, Total:    8844, Move:          9 (Decl x 1.000000) L2
S10_Conv2d_276x276x1x1_Relu - IterSpace: Tile0 - L1 Memory:   8844, L2Move: 107649, L3Move: 0, Tiling Overhead: 5.997493
S10_Conv2d_276x276x1x1_Relu Iteration for Tiled Space: 5
Ker: S10_Conv2d_276x276x1x1_Relu, Arg:        In2, Size:   4416, Base1:  35496, Base2:  39912
Ker: S10_Conv2d_276x276x1x1_Relu, Arg:      Infos, Size:     12, Base1:  44328, Base2:      0
S10_Conv2d_276x276x1x1_Relu For Iter Space: 0 Iteration count:   5 (Last one is truncated), Given L1 Memory:  10399, Used L1 Memory:   8844, Reusable Memory: 1552, Used L2 Memory: 0
=================================================================================================

InFeat: 276, OutFeat: 276
Conv => W:  5, Pad:[1,1] PadT:[1,1] => Wc: 5, Filter:[3,3]
     => H:  13, Pad:[1,1] PadT:[1,1] => Hc: 13
Pool => Wc: 5, Pad:[0,0] => Wo: 5, Filter:[1,1]
     => Hc: 13, Pad:[0,0] => Ho: 13
OverlapC: 2
OverlapP: 0
TileCons: 1
UsedIn  : [5 x 13]
UsedC   : [5 x 13]
         ConvKerName: KerParConvDW3x3Stride1B32_SQ8
  DPReductionKerName: KerParReduct_CC_ReLU_SQ8
Nb Oper : 179400

==== Process Tiling For User Kernel:            S13_Conv2d_276x1x3x3_Relu =======================
S13_Conv2d_276x1x3x3_Relu Partition[0] Size =  91105 (Min:     30, Max: 157457), Fraction:       1.00, Giving:  48736 Bytes out of  48736 Bytes

Reference object:                   In, Dim=13
	                  In Dim:  15, TileOverlap:  2, Ratio: 1.000000
	                 Out Dim:  13, TileOverlap:  0, Ratio: 1.000000
	             ConvOut Dim:  13, TileOverlap:  0, Ratio: 1.000000

S13_Conv2d_276x1x3x3_Relu Full buffering on Arg: Bias, was using 640 Bytes will require 1104 Bytes buffer
S13_Conv2d_276x1x3x3_Relu Full buffering on Arg: Scale, was using 160 Bytes will require 276 Bytes buffer
S13_Conv2d_276x1x3x3_Relu Full buffering on Arg: ScaleN, was using 160 Bytes will require 276 Bytes buffer
S13_Conv2d_276x1x3x3_Relu Full buffering on Arg: Filter, was using 1440 Bytes will require 2484 Bytes buffer
S13_Conv2d_276x1x3x3_Relu, TiledSpace: Tile0 Iteration Count: 1 Parametric Space: [D0, M0=80]
              In : Ratio: 1.000000, FixDim:      5, VarDim:     13 [    13], Size:  10400, Total:   10400, Move:      17940 (Decl x 1.000000) L2
*           Bias : Ratio: 0.000000,                                          Size:   1104, Total:   11504, Move:       1104 (Decl x 1.000000) L2
*          Scale : Ratio: 0.000000,                                          Size:    276, Total:   11780, Move:        276 (Decl x 1.000000) L2
*         ScaleN : Ratio: 0.000000,                                          Size:    276, Total:   12056, Move:        276 (Decl x 1.000000) L2
*         Filter : Ratio: 0.000000,                                          Size:   2484, Total:   14540, Move:       2484 (Decl x 1.000000) L2
             Out : Ratio: 1.000000, FixDim:      5, VarDim:     13 [    13], Size:  10400, Total:   24940, Move:      17940 (Decl x 1.000000) L2
*        ConvOut : Ratio: 1.000000, FixDim:      5, VarDim:     13 [    13], Size:  20800, Total:   45740, Move:          0 (Decl x 0.000000) L2
*          Infos : Ratio: 0.000000,                                          Size:     12, Total:   45752, Move:          9 (Decl x 1.000000) L2
S13_Conv2d_276x1x3x3_Relu - IterSpace: Tile0 - L1 Memory:  45752, L2Move: 40029, L3Move: 0, Tiling Overhead: 1.000000
S13_Conv2d_276x1x3x3_Relu Found Parametric value for space D0 (Initial: 276, Div: 8) = 80 [80*3 + 36], Iteration for Tiled Space: 1
Ker: S13_Conv2d_276x1x3x3_Relu, Arg:         In, Size:   5200, Base1:      0, Base2:   5200
Ker: S13_Conv2d_276x1x3x3_Relu, Arg:       Bias, Size:   1104, Base1:  10400, Base2:      0
Ker: S13_Conv2d_276x1x3x3_Relu, Arg:      Scale, Size:    276, Base1:  11504, Base2:      0
Ker: S13_Conv2d_276x1x3x3_Relu, Arg:     ScaleN, Size:    276, Base1:  11780, Base2:      0
Ker: S13_Conv2d_276x1x3x3_Relu, Arg:     Filter, Size:   2484, Base1:  12056, Base2:      0
Ker: S13_Conv2d_276x1x3x3_Relu, Arg:        Out, Size:   5200, Base1:  14540, Base2:  19740
Ker: S13_Conv2d_276x1x3x3_Relu, Arg:    ConvOut, Size:  20800, Base1:  24940, Base2:      0
Ker: S13_Conv2d_276x1x3x3_Relu, Arg:      Infos, Size:     12, Base1:  45740, Base2:      0
S13_Conv2d_276x1x3x3_Relu For Iter Space: 0 Iteration count:   1, Given L1 Memory:  48736, Used L1 Memory:  45752, Reusable Memory: 2984, Used L2 Memory: 0
=================================================================================================

InFeat: 276, OutFeat: 276
Conv => W:  5, Pad:[0,0] PadT:[0,0] => Wc: 5, Filter:[1,1]
     => H:  13, Pad:[0,0] PadT:[0,0] => Hc: 13
Pool => Wc: 5, Pad:[0,0] => Wo: 5, Filter:[1,1]
     => Hc: 13, Pad:[0,0] => Ho: 13
OverlapC: 0
OverlapP: 0
TileCons: 1
UsedIn  : [5 x 13]
UsedC   : [5 x 13]
      SetBiasKerName: KerParSetBiasB32_SQ8
         ConvKerName: KerParConv1x1Stride1_SQ8
  DPReductionKerName: KerParReduct_CC_ReLU_SQ8
Nb Oper : 4969380
Mapping this convolution to matrix multiplication
CNN_MatMul_SQ8: S16_Conv2d_276x276x1x1_Relu
In1  => W:  276, H:  276 
In2  => W:   65, H:  276, w:    5, h:   13, Sx: 1, Sy: 1
Out  => W:   65, H:  276 => Line First
       MatMulKerName: KerParMatMulB32_2x4_ReLU_SQ8

==== Process Tiling For User Kernel:          S16_Conv2d_276x276x1x1_Relu =======================
S16_Conv2d_276x276x1x1_Relu Partition[0] Size =   2233 (Min:   2208, Max:  35921), Fraction:       0.21, Giving:  10399 Bytes out of  48736 Bytes
S16_Conv2d_276x276x1x1_Relu Partition[1] Size =   8232 (Min:   4416, Max: 191088), Fraction:       0.79, Giving:  38336 Bytes out of  48736 Bytes

Reference object:                  In1, Dim=276
	                 In1 Dim: 276, TileOverlap:  0, Ratio: 1.000000
	                Bias Dim: 276, TileOverlap:  0, Ratio: 1.000000
	                 Out Dim: 276, TileOverlap:  0, Ratio: 1.000000
	               Scale Dim: 276, TileOverlap:  0, Ratio: 1.000000
	              ScaleN Dim: 276, TileOverlap:  0, Ratio: 1.000000

S16_Conv2d_276x276x1x1_Relu, TiledSpace: Tile1 Iteration Count: 6
*        KerBuff : Ratio: 0.000000,                                          Size:   1104, Total:    1104, Move:          0 (Decl x 0.000000) L2
             In1 : Ratio: 1.000000, FixDim:    276, VarDim:     48 [   276], Size:  26496, Total:   27600, Move:      76176 (Decl x 1.000000) L2
*           Bias : Ratio: 1.000000,                                          Size:   1104, Total:   28704, Move:       1104 (Decl x 1.000000) L2
             Out : Ratio: 1.000000, FixDim:     65, VarDim:     48 [   276], Size:   6240, Total:   34944, Move:      17940 (Decl x 1.000000) L2
*          Scale : Ratio: 1.000000,                                          Size:    276, Total:   35220, Move:        276 (Decl x 1.000000) L2
*         ScaleN : Ratio: 1.000000,                                          Size:    276, Total:   35496, Move:        276 (Decl x 1.000000) L2
S16_Conv2d_276x276x1x1_Relu - IterSpace: Tile1 - L1 Memory:  35496, L2Move: 95772, L3Move: 0, Tiling Overhead: 1.000000
S16_Conv2d_276x276x1x1_Relu Iteration for Tiled Space: 6
Ker: S16_Conv2d_276x276x1x1_Relu, Arg:    KerBuff, Size:   1104, Base1:      0, Base2:      0
Ker: S16_Conv2d_276x276x1x1_Relu, Arg:        In1, Size:  13248, Base1:   1104, Base2:  14352
Ker: S16_Conv2d_276x276x1x1_Relu, Arg:       Bias, Size:   1104, Base1:  27600, Base2:      0
Ker: S16_Conv2d_276x276x1x1_Relu, Arg:        Out, Size:   3120, Base1:  28704, Base2:  31824
Ker: S16_Conv2d_276x276x1x1_Relu, Arg:      Scale, Size:    276, Base1:  34944, Base2:      0
Ker: S16_Conv2d_276x276x1x1_Relu, Arg:     ScaleN, Size:    276, Base1:  35220, Base2:      0
S16_Conv2d_276x276x1x1_Relu For Iter Space: 1 Iteration count:   6 (Last one is truncated), Given L1 Memory:  38336, Used L1 Memory:  35496, Reusable Memory: 2840, Used L2 Memory: 0

Reference object:                  In2, Dim=65
	                 In2 Dim:  65, TileOverlap:  0, Ratio: 1.000000

S16_Conv2d_276x276x1x1_Relu, TiledSpace: Tile0 Iteration Count: 5
             In2 : Ratio: 1.000000, FixDim:    276, VarDim:     16 [    65], Size:   8832, Total:    8832, Move:     107640 (Decl x 6.000000) L2
*          Infos : Ratio: 0.000000,                                          Size:     12, Total:    8844, Move:          9 (Decl x 1.000000) L2
S16_Conv2d_276x276x1x1_Relu - IterSpace: Tile0 - L1 Memory:   8844, L2Move: 107649, L3Move: 0, Tiling Overhead: 5.997493
S16_Conv2d_276x276x1x1_Relu Iteration for Tiled Space: 5
Ker: S16_Conv2d_276x276x1x1_Relu, Arg:        In2, Size:   4416, Base1:  35496, Base2:  39912
Ker: S16_Conv2d_276x276x1x1_Relu, Arg:      Infos, Size:     12, Base1:  44328, Base2:      0
S16_Conv2d_276x276x1x1_Relu For Iter Space: 0 Iteration count:   5 (Last one is truncated), Given L1 Memory:  10399, Used L1 Memory:   8844, Reusable Memory: 1552, Used L2 Memory: 0
=================================================================================================

InFeat: 276, OutFeat: 276
Conv => W:  5, Pad:[1,1] PadT:[1,1] => Wc: 5, Filter:[3,3]
     => H:  13, Pad:[1,1] PadT:[1,1] => Hc: 13
Pool => Wc: 5, Pad:[0,0] => Wo: 5, Filter:[1,1]
     => Hc: 13, Pad:[0,0] => Ho: 13
OverlapC: 2
OverlapP: 0
TileCons: 1
UsedIn  : [5 x 13]
UsedC   : [5 x 13]
         ConvKerName: KerParConvDW3x3Stride1B32_SQ8
  DPReductionKerName: KerParReduct_CC_ReLU_SQ8
Nb Oper : 179400

==== Process Tiling For User Kernel:            S19_Conv2d_276x1x3x3_Relu =======================
S19_Conv2d_276x1x3x3_Relu Partition[0] Size =  91105 (Min:     30, Max: 157457), Fraction:       1.00, Giving:  48736 Bytes out of  48736 Bytes

Reference object:                   In, Dim=13
	                  In Dim:  15, TileOverlap:  2, Ratio: 1.000000
	                 Out Dim:  13, TileOverlap:  0, Ratio: 1.000000
	             ConvOut Dim:  13, TileOverlap:  0, Ratio: 1.000000

S19_Conv2d_276x1x3x3_Relu Full buffering on Arg: Bias, was using 640 Bytes will require 1104 Bytes buffer
S19_Conv2d_276x1x3x3_Relu Full buffering on Arg: Scale, was using 160 Bytes will require 276 Bytes buffer
S19_Conv2d_276x1x3x3_Relu Full buffering on Arg: ScaleN, was using 160 Bytes will require 276 Bytes buffer
S19_Conv2d_276x1x3x3_Relu Full buffering on Arg: Filter, was using 1440 Bytes will require 2484 Bytes buffer
S19_Conv2d_276x1x3x3_Relu, TiledSpace: Tile0 Iteration Count: 1 Parametric Space: [D0, M0=80]
              In : Ratio: 1.000000, FixDim:      5, VarDim:     13 [    13], Size:  10400, Total:   10400, Move:      17940 (Decl x 1.000000) L2
*           Bias : Ratio: 0.000000,                                          Size:   1104, Total:   11504, Move:       1104 (Decl x 1.000000) L2
*          Scale : Ratio: 0.000000,                                          Size:    276, Total:   11780, Move:        276 (Decl x 1.000000) L2
*         ScaleN : Ratio: 0.000000,                                          Size:    276, Total:   12056, Move:        276 (Decl x 1.000000) L2
*         Filter : Ratio: 0.000000,                                          Size:   2484, Total:   14540, Move:       2484 (Decl x 1.000000) L2
             Out : Ratio: 1.000000, FixDim:      5, VarDim:     13 [    13], Size:  10400, Total:   24940, Move:      17940 (Decl x 1.000000) L2
*        ConvOut : Ratio: 1.000000, FixDim:      5, VarDim:     13 [    13], Size:  20800, Total:   45740, Move:          0 (Decl x 0.000000) L2
*          Infos : Ratio: 0.000000,                                          Size:     12, Total:   45752, Move:          9 (Decl x 1.000000) L2
S19_Conv2d_276x1x3x3_Relu - IterSpace: Tile0 - L1 Memory:  45752, L2Move: 40029, L3Move: 0, Tiling Overhead: 1.000000
S19_Conv2d_276x1x3x3_Relu Found Parametric value for space D0 (Initial: 276, Div: 8) = 80 [80*3 + 36], Iteration for Tiled Space: 1
Ker: S19_Conv2d_276x1x3x3_Relu, Arg:         In, Size:   5200, Base1:      0, Base2:   5200
Ker: S19_Conv2d_276x1x3x3_Relu, Arg:       Bias, Size:   1104, Base1:  10400, Base2:      0
Ker: S19_Conv2d_276x1x3x3_Relu, Arg:      Scale, Size:    276, Base1:  11504, Base2:      0
Ker: S19_Conv2d_276x1x3x3_Relu, Arg:     ScaleN, Size:    276, Base1:  11780, Base2:      0
Ker: S19_Conv2d_276x1x3x3_Relu, Arg:     Filter, Size:   2484, Base1:  12056, Base2:      0
Ker: S19_Conv2d_276x1x3x3_Relu, Arg:        Out, Size:   5200, Base1:  14540, Base2:  19740
Ker: S19_Conv2d_276x1x3x3_Relu, Arg:    ConvOut, Size:  20800, Base1:  24940, Base2:      0
Ker: S19_Conv2d_276x1x3x3_Relu, Arg:      Infos, Size:     12, Base1:  45740, Base2:      0
S19_Conv2d_276x1x3x3_Relu For Iter Space: 0 Iteration count:   1, Given L1 Memory:  48736, Used L1 Memory:  45752, Reusable Memory: 2984, Used L2 Memory: 0
=================================================================================================

InFeat: 276, OutFeat: 276
Conv => W:  5, Pad:[0,0] PadT:[0,0] => Wc: 5, Filter:[1,1]
     => H:  13, Pad:[0,0] PadT:[0,0] => Hc: 13
Pool => Wc: 5, Pad:[0,0] => Wo: 5, Filter:[1,1]
     => Hc: 13, Pad:[0,0] => Ho: 13
OverlapC: 0
OverlapP: 0
TileCons: 1
UsedIn  : [5 x 13]
UsedC   : [5 x 13]
      SetBiasKerName: KerParSetBiasB32_SQ8
         ConvKerName: KerParConv1x1Stride1_SQ8
  DPReductionKerName: KerParReduct_CC_ReLU_SQ8
Nb Oper : 4969380
Mapping this convolution to matrix multiplication
CNN_MatMul_SQ8: S22_Conv2d_276x276x1x1_Relu
In1  => W:  276, H:  276 
In2  => W:   65, H:  276, w:    5, h:   13, Sx: 1, Sy: 1
Out  => W:   65, H:  276 => Line First
       MatMulKerName: KerParMatMulB32_2x4_ReLU_SQ8

==== Process Tiling For User Kernel:          S22_Conv2d_276x276x1x1_Relu =======================
S22_Conv2d_276x276x1x1_Relu Partition[0] Size =   2233 (Min:   2208, Max:  35921), Fraction:       0.21, Giving:  10399 Bytes out of  48736 Bytes
S22_Conv2d_276x276x1x1_Relu Partition[1] Size =   8232 (Min:   4416, Max: 191088), Fraction:       0.79, Giving:  38336 Bytes out of  48736 Bytes

Reference object:                  In1, Dim=276
	                 In1 Dim: 276, TileOverlap:  0, Ratio: 1.000000
	                Bias Dim: 276, TileOverlap:  0, Ratio: 1.000000
	                 Out Dim: 276, TileOverlap:  0, Ratio: 1.000000
	               Scale Dim: 276, TileOverlap:  0, Ratio: 1.000000
	              ScaleN Dim: 276, TileOverlap:  0, Ratio: 1.000000

S22_Conv2d_276x276x1x1_Relu, TiledSpace: Tile1 Iteration Count: 6
*        KerBuff : Ratio: 0.000000,                                          Size:   1104, Total:    1104, Move:          0 (Decl x 0.000000) L2
             In1 : Ratio: 1.000000, FixDim:    276, VarDim:     48 [   276], Size:  26496, Total:   27600, Move:      76176 (Decl x 1.000000) L2
*           Bias : Ratio: 1.000000,                                          Size:   1104, Total:   28704, Move:       1104 (Decl x 1.000000) L2
             Out : Ratio: 1.000000, FixDim:     65, VarDim:     48 [   276], Size:   6240, Total:   34944, Move:      17940 (Decl x 1.000000) L2
*          Scale : Ratio: 1.000000,                                          Size:    276, Total:   35220, Move:        276 (Decl x 1.000000) L2
*         ScaleN : Ratio: 1.000000,                                          Size:    276, Total:   35496, Move:        276 (Decl x 1.000000) L2
S22_Conv2d_276x276x1x1_Relu - IterSpace: Tile1 - L1 Memory:  35496, L2Move: 95772, L3Move: 0, Tiling Overhead: 1.000000
S22_Conv2d_276x276x1x1_Relu Iteration for Tiled Space: 6
Ker: S22_Conv2d_276x276x1x1_Relu, Arg:    KerBuff, Size:   1104, Base1:      0, Base2:      0
Ker: S22_Conv2d_276x276x1x1_Relu, Arg:        In1, Size:  13248, Base1:   1104, Base2:  14352
Ker: S22_Conv2d_276x276x1x1_Relu, Arg:       Bias, Size:   1104, Base1:  27600, Base2:      0
Ker: S22_Conv2d_276x276x1x1_Relu, Arg:        Out, Size:   3120, Base1:  28704, Base2:  31824
Ker: S22_Conv2d_276x276x1x1_Relu, Arg:      Scale, Size:    276, Base1:  34944, Base2:      0
Ker: S22_Conv2d_276x276x1x1_Relu, Arg:     ScaleN, Size:    276, Base1:  35220, Base2:      0
S22_Conv2d_276x276x1x1_Relu For Iter Space: 1 Iteration count:   6 (Last one is truncated), Given L1 Memory:  38336, Used L1 Memory:  35496, Reusable Memory: 2840, Used L2 Memory: 0

Reference object:                  In2, Dim=65
	                 In2 Dim:  65, TileOverlap:  0, Ratio: 1.000000

S22_Conv2d_276x276x1x1_Relu, TiledSpace: Tile0 Iteration Count: 5
             In2 : Ratio: 1.000000, FixDim:    276, VarDim:     16 [    65], Size:   8832, Total:    8832, Move:     107640 (Decl x 6.000000) L2
*          Infos : Ratio: 0.000000,                                          Size:     12, Total:    8844, Move:          9 (Decl x 1.000000) L2
S22_Conv2d_276x276x1x1_Relu - IterSpace: Tile0 - L1 Memory:   8844, L2Move: 107649, L3Move: 0, Tiling Overhead: 5.997493
S22_Conv2d_276x276x1x1_Relu Iteration for Tiled Space: 5
Ker: S22_Conv2d_276x276x1x1_Relu, Arg:        In2, Size:   4416, Base1:  35496, Base2:  39912
Ker: S22_Conv2d_276x276x1x1_Relu, Arg:      Infos, Size:     12, Base1:  44328, Base2:      0
S22_Conv2d_276x276x1x1_Relu For Iter Space: 0 Iteration count:   5 (Last one is truncated), Given L1 Memory:  10399, Used L1 Memory:   8844, Reusable Memory: 1552, Used L2 Memory: 0
=================================================================================================

InFeat: 276, OutFeat: 276
Conv => W:  5, Pad:[1,1] PadT:[1,1] => Wc: 5, Filter:[3,3]
     => H:  13, Pad:[1,1] PadT:[1,1] => Hc: 13
Pool => Wc: 5, Pad:[0,0] => Wo: 5, Filter:[1,1]
     => Hc: 13, Pad:[0,0] => Ho: 13
OverlapC: 2
OverlapP: 0
TileCons: 1
UsedIn  : [5 x 13]
UsedC   : [5 x 13]
         ConvKerName: KerParConvDW3x3Stride1B32_SQ8
  DPReductionKerName: KerParReduct_CC_ReLU_SQ8
Nb Oper : 179400

==== Process Tiling For User Kernel:            S25_Conv2d_276x1x3x3_Relu =======================
S25_Conv2d_276x1x3x3_Relu Partition[0] Size =  91105 (Min:     30, Max: 157457), Fraction:       1.00, Giving:  48736 Bytes out of  48736 Bytes

Reference object:                   In, Dim=13
	                  In Dim:  15, TileOverlap:  2, Ratio: 1.000000
	                 Out Dim:  13, TileOverlap:  0, Ratio: 1.000000
	             ConvOut Dim:  13, TileOverlap:  0, Ratio: 1.000000

S25_Conv2d_276x1x3x3_Relu Full buffering on Arg: Bias, was using 640 Bytes will require 1104 Bytes buffer
S25_Conv2d_276x1x3x3_Relu Full buffering on Arg: Scale, was using 160 Bytes will require 276 Bytes buffer
S25_Conv2d_276x1x3x3_Relu Full buffering on Arg: ScaleN, was using 160 Bytes will require 276 Bytes buffer
S25_Conv2d_276x1x3x3_Relu Full buffering on Arg: Filter, was using 1440 Bytes will require 2484 Bytes buffer
S25_Conv2d_276x1x3x3_Relu, TiledSpace: Tile0 Iteration Count: 1 Parametric Space: [D0, M0=80]
              In : Ratio: 1.000000, FixDim:      5, VarDim:     13 [    13], Size:  10400, Total:   10400, Move:      17940 (Decl x 1.000000) L2
*           Bias : Ratio: 0.000000,                                          Size:   1104, Total:   11504, Move:       1104 (Decl x 1.000000) L2
*          Scale : Ratio: 0.000000,                                          Size:    276, Total:   11780, Move:        276 (Decl x 1.000000) L2
*         ScaleN : Ratio: 0.000000,                                          Size:    276, Total:   12056, Move:        276 (Decl x 1.000000) L2
*         Filter : Ratio: 0.000000,                                          Size:   2484, Total:   14540, Move:       2484 (Decl x 1.000000) L2
             Out : Ratio: 1.000000, FixDim:      5, VarDim:     13 [    13], Size:  10400, Total:   24940, Move:      17940 (Decl x 1.000000) L2
*        ConvOut : Ratio: 1.000000, FixDim:      5, VarDim:     13 [    13], Size:  20800, Total:   45740, Move:          0 (Decl x 0.000000) L2
*          Infos : Ratio: 0.000000,                                          Size:     12, Total:   45752, Move:          9 (Decl x 1.000000) L2
S25_Conv2d_276x1x3x3_Relu - IterSpace: Tile0 - L1 Memory:  45752, L2Move: 40029, L3Move: 0, Tiling Overhead: 1.000000
S25_Conv2d_276x1x3x3_Relu Found Parametric value for space D0 (Initial: 276, Div: 8) = 80 [80*3 + 36], Iteration for Tiled Space: 1
Ker: S25_Conv2d_276x1x3x3_Relu, Arg:         In, Size:   5200, Base1:      0, Base2:   5200
Ker: S25_Conv2d_276x1x3x3_Relu, Arg:       Bias, Size:   1104, Base1:  10400, Base2:      0
Ker: S25_Conv2d_276x1x3x3_Relu, Arg:      Scale, Size:    276, Base1:  11504, Base2:      0
Ker: S25_Conv2d_276x1x3x3_Relu, Arg:     ScaleN, Size:    276, Base1:  11780, Base2:      0
Ker: S25_Conv2d_276x1x3x3_Relu, Arg:     Filter, Size:   2484, Base1:  12056, Base2:      0
Ker: S25_Conv2d_276x1x3x3_Relu, Arg:        Out, Size:   5200, Base1:  14540, Base2:  19740
Ker: S25_Conv2d_276x1x3x3_Relu, Arg:    ConvOut, Size:  20800, Base1:  24940, Base2:      0
Ker: S25_Conv2d_276x1x3x3_Relu, Arg:      Infos, Size:     12, Base1:  45740, Base2:      0
S25_Conv2d_276x1x3x3_Relu For Iter Space: 0 Iteration count:   1, Given L1 Memory:  48736, Used L1 Memory:  45752, Reusable Memory: 2984, Used L2 Memory: 0
=================================================================================================

InFeat: 276, OutFeat: 276
Conv => W:  5, Pad:[0,0] PadT:[0,0] => Wc: 5, Filter:[1,1]
     => H:  13, Pad:[0,0] PadT:[0,0] => Hc: 13
Pool => Wc: 5, Pad:[0,0] => Wo: 5, Filter:[1,1]
     => Hc: 13, Pad:[0,0] => Ho: 13
OverlapC: 0
OverlapP: 0
TileCons: 1
UsedIn  : [5 x 13]
UsedC   : [5 x 13]
      SetBiasKerName: KerParSetBiasB32_SQ8
         ConvKerName: KerParConv1x1Stride1_SQ8
  DPReductionKerName: KerParReduct_CC_ReLU_SQ8
Nb Oper : 4969380
Mapping this convolution to matrix multiplication
CNN_MatMul_SQ8: S28_Conv2d_276x276x1x1_Relu
In1  => W:  276, H:  276 
In2  => W:   65, H:  276, w:    5, h:   13, Sx: 1, Sy: 1
Out  => W:   65, H:  276 => Line First
       MatMulKerName: KerParMatMulB32_2x4_ReLU_SQ8

==== Process Tiling For User Kernel:          S28_Conv2d_276x276x1x1_Relu =======================
S28_Conv2d_276x276x1x1_Relu Partition[0] Size =   2233 (Min:   2208, Max:  35921), Fraction:       0.21, Giving:  10399 Bytes out of  48736 Bytes
S28_Conv2d_276x276x1x1_Relu Partition[1] Size =   8232 (Min:   4416, Max: 191088), Fraction:       0.79, Giving:  38336 Bytes out of  48736 Bytes

Reference object:                  In1, Dim=276
	                 In1 Dim: 276, TileOverlap:  0, Ratio: 1.000000
	                Bias Dim: 276, TileOverlap:  0, Ratio: 1.000000
	                 Out Dim: 276, TileOverlap:  0, Ratio: 1.000000
	               Scale Dim: 276, TileOverlap:  0, Ratio: 1.000000
	              ScaleN Dim: 276, TileOverlap:  0, Ratio: 1.000000

S28_Conv2d_276x276x1x1_Relu, TiledSpace: Tile1 Iteration Count: 6
*        KerBuff : Ratio: 0.000000,                                          Size:   1104, Total:    1104, Move:          0 (Decl x 0.000000) L2
             In1 : Ratio: 1.000000, FixDim:    276, VarDim:     48 [   276], Size:  26496, Total:   27600, Move:      76176 (Decl x 1.000000) L2
*           Bias : Ratio: 1.000000,                                          Size:   1104, Total:   28704, Move:       1104 (Decl x 1.000000) L2
             Out : Ratio: 1.000000, FixDim:     65, VarDim:     48 [   276], Size:   6240, Total:   34944, Move:      17940 (Decl x 1.000000) L2
*          Scale : Ratio: 1.000000,                                          Size:    276, Total:   35220, Move:        276 (Decl x 1.000000) L2
*         ScaleN : Ratio: 1.000000,                                          Size:    276, Total:   35496, Move:        276 (Decl x 1.000000) L2
S28_Conv2d_276x276x1x1_Relu - IterSpace: Tile1 - L1 Memory:  35496, L2Move: 95772, L3Move: 0, Tiling Overhead: 1.000000
S28_Conv2d_276x276x1x1_Relu Iteration for Tiled Space: 6
Ker: S28_Conv2d_276x276x1x1_Relu, Arg:    KerBuff, Size:   1104, Base1:      0, Base2:      0
Ker: S28_Conv2d_276x276x1x1_Relu, Arg:        In1, Size:  13248, Base1:   1104, Base2:  14352
Ker: S28_Conv2d_276x276x1x1_Relu, Arg:       Bias, Size:   1104, Base1:  27600, Base2:      0
Ker: S28_Conv2d_276x276x1x1_Relu, Arg:        Out, Size:   3120, Base1:  28704, Base2:  31824
Ker: S28_Conv2d_276x276x1x1_Relu, Arg:      Scale, Size:    276, Base1:  34944, Base2:      0
Ker: S28_Conv2d_276x276x1x1_Relu, Arg:     ScaleN, Size:    276, Base1:  35220, Base2:      0
S28_Conv2d_276x276x1x1_Relu For Iter Space: 1 Iteration count:   6 (Last one is truncated), Given L1 Memory:  38336, Used L1 Memory:  35496, Reusable Memory: 2840, Used L2 Memory: 0

Reference object:                  In2, Dim=65
	                 In2 Dim:  65, TileOverlap:  0, Ratio: 1.000000

S28_Conv2d_276x276x1x1_Relu, TiledSpace: Tile0 Iteration Count: 5
             In2 : Ratio: 1.000000, FixDim:    276, VarDim:     16 [    65], Size:   8832, Total:    8832, Move:     107640 (Decl x 6.000000) L2
*          Infos : Ratio: 0.000000,                                          Size:     12, Total:    8844, Move:          9 (Decl x 1.000000) L2
S28_Conv2d_276x276x1x1_Relu - IterSpace: Tile0 - L1 Memory:   8844, L2Move: 107649, L3Move: 0, Tiling Overhead: 5.997493
S28_Conv2d_276x276x1x1_Relu Iteration for Tiled Space: 5
Ker: S28_Conv2d_276x276x1x1_Relu, Arg:        In2, Size:   4416, Base1:  35496, Base2:  39912
Ker: S28_Conv2d_276x276x1x1_Relu, Arg:      Infos, Size:     12, Base1:  44328, Base2:      0
S28_Conv2d_276x276x1x1_Relu For Iter Space: 0 Iteration count:   5 (Last one is truncated), Given L1 Memory:  10399, Used L1 Memory:   8844, Reusable Memory: 1552, Used L2 Memory: 0
=================================================================================================

InFeat: 276, OutFeat: 276
Conv => W:  5, Pad:[1,1] PadT:[1,1] => Wc: 5, Filter:[3,3]
     => H:  13, Pad:[1,1] PadT:[1,1] => Hc: 13
Pool => Wc: 5, Pad:[0,0] => Wo: 5, Filter:[1,1]
     => Hc: 13, Pad:[0,0] => Ho: 13
OverlapC: 2
OverlapP: 0
TileCons: 1
UsedIn  : [5 x 13]
UsedC   : [5 x 13]
         ConvKerName: KerParConvDW3x3Stride1B32_SQ8
  DPReductionKerName: KerParReduct_CC_ReLU_SQ8
Nb Oper : 179400

==== Process Tiling For User Kernel:            S31_Conv2d_276x1x3x3_Relu =======================
S31_Conv2d_276x1x3x3_Relu Partition[0] Size =  91105 (Min:     30, Max: 157457), Fraction:       1.00, Giving:  48736 Bytes out of  48736 Bytes

Reference object:                   In, Dim=13
	                  In Dim:  15, TileOverlap:  2, Ratio: 1.000000
	                 Out Dim:  13, TileOverlap:  0, Ratio: 1.000000
	             ConvOut Dim:  13, TileOverlap:  0, Ratio: 1.000000

S31_Conv2d_276x1x3x3_Relu Full buffering on Arg: Bias, was using 640 Bytes will require 1104 Bytes buffer
S31_Conv2d_276x1x3x3_Relu Full buffering on Arg: Scale, was using 160 Bytes will require 276 Bytes buffer
S31_Conv2d_276x1x3x3_Relu Full buffering on Arg: ScaleN, was using 160 Bytes will require 276 Bytes buffer
S31_Conv2d_276x1x3x3_Relu Full buffering on Arg: Filter, was using 1440 Bytes will require 2484 Bytes buffer
S31_Conv2d_276x1x3x3_Relu, TiledSpace: Tile0 Iteration Count: 1 Parametric Space: [D0, M0=80]
              In : Ratio: 1.000000, FixDim:      5, VarDim:     13 [    13], Size:  10400, Total:   10400, Move:      17940 (Decl x 1.000000) L2
*           Bias : Ratio: 0.000000,                                          Size:   1104, Total:   11504, Move:       1104 (Decl x 1.000000) L2
*          Scale : Ratio: 0.000000,                                          Size:    276, Total:   11780, Move:        276 (Decl x 1.000000) L2
*         ScaleN : Ratio: 0.000000,                                          Size:    276, Total:   12056, Move:        276 (Decl x 1.000000) L2
*         Filter : Ratio: 0.000000,                                          Size:   2484, Total:   14540, Move:       2484 (Decl x 1.000000) L2
             Out : Ratio: 1.000000, FixDim:      5, VarDim:     13 [    13], Size:  10400, Total:   24940, Move:      17940 (Decl x 1.000000) L2
*        ConvOut : Ratio: 1.000000, FixDim:      5, VarDim:     13 [    13], Size:  20800, Total:   45740, Move:          0 (Decl x 0.000000) L2
*          Infos : Ratio: 0.000000,                                          Size:     12, Total:   45752, Move:          9 (Decl x 1.000000) L2
S31_Conv2d_276x1x3x3_Relu - IterSpace: Tile0 - L1 Memory:  45752, L2Move: 40029, L3Move: 0, Tiling Overhead: 1.000000
S31_Conv2d_276x1x3x3_Relu Found Parametric value for space D0 (Initial: 276, Div: 8) = 80 [80*3 + 36], Iteration for Tiled Space: 1
Ker: S31_Conv2d_276x1x3x3_Relu, Arg:         In, Size:   5200, Base1:      0, Base2:   5200
Ker: S31_Conv2d_276x1x3x3_Relu, Arg:       Bias, Size:   1104, Base1:  10400, Base2:      0
Ker: S31_Conv2d_276x1x3x3_Relu, Arg:      Scale, Size:    276, Base1:  11504, Base2:      0
Ker: S31_Conv2d_276x1x3x3_Relu, Arg:     ScaleN, Size:    276, Base1:  11780, Base2:      0
Ker: S31_Conv2d_276x1x3x3_Relu, Arg:     Filter, Size:   2484, Base1:  12056, Base2:      0
Ker: S31_Conv2d_276x1x3x3_Relu, Arg:        Out, Size:   5200, Base1:  14540, Base2:  19740
Ker: S31_Conv2d_276x1x3x3_Relu, Arg:    ConvOut, Size:  20800, Base1:  24940, Base2:      0
Ker: S31_Conv2d_276x1x3x3_Relu, Arg:      Infos, Size:     12, Base1:  45740, Base2:      0
S31_Conv2d_276x1x3x3_Relu For Iter Space: 0 Iteration count:   1, Given L1 Memory:  48736, Used L1 Memory:  45752, Reusable Memory: 2984, Used L2 Memory: 0
=================================================================================================

InFeat: 276, OutFeat: 276
Conv => W:  5, Pad:[0,0] PadT:[0,0] => Wc: 5, Filter:[1,1]
     => H:  13, Pad:[0,0] PadT:[0,0] => Hc: 13
Pool => Wc: 5, Pad:[0,0] => Wo: 5, Filter:[1,1]
     => Hc: 13, Pad:[0,0] => Ho: 13
OverlapC: 0
OverlapP: 0
TileCons: 1
UsedIn  : [5 x 13]
UsedC   : [5 x 13]
      SetBiasKerName: KerParSetBiasB32_SQ8
         ConvKerName: KerParConv1x1Stride1_SQ8
  DPReductionKerName: KerParReduct_CC_ReLU_SQ8
Nb Oper : 4969380
Mapping this convolution to matrix multiplication
CNN_MatMul_SQ8: S34_Conv2d_276x276x1x1_Relu
In1  => W:  276, H:  276 
In2  => W:   65, H:  276, w:    5, h:   13, Sx: 1, Sy: 1
Out  => W:   65, H:  276 => Line First
       MatMulKerName: KerParMatMulB32_2x4_ReLU_SQ8

==== Process Tiling For User Kernel:          S34_Conv2d_276x276x1x1_Relu =======================
S34_Conv2d_276x276x1x1_Relu Partition[0] Size =   2233 (Min:   2208, Max:  35921), Fraction:       0.21, Giving:  10399 Bytes out of  48736 Bytes
S34_Conv2d_276x276x1x1_Relu Partition[1] Size =   8232 (Min:   4416, Max: 191088), Fraction:       0.79, Giving:  38336 Bytes out of  48736 Bytes

Reference object:                  In1, Dim=276
	                 In1 Dim: 276, TileOverlap:  0, Ratio: 1.000000
	                Bias Dim: 276, TileOverlap:  0, Ratio: 1.000000
	                 Out Dim: 276, TileOverlap:  0, Ratio: 1.000000
	               Scale Dim: 276, TileOverlap:  0, Ratio: 1.000000
	              ScaleN Dim: 276, TileOverlap:  0, Ratio: 1.000000

S34_Conv2d_276x276x1x1_Relu, TiledSpace: Tile1 Iteration Count: 6
*        KerBuff : Ratio: 0.000000,                                          Size:   1104, Total:    1104, Move:          0 (Decl x 0.000000) L2
             In1 : Ratio: 1.000000, FixDim:    276, VarDim:     48 [   276], Size:  26496, Total:   27600, Move:      76176 (Decl x 1.000000) L2
*           Bias : Ratio: 1.000000,                                          Size:   1104, Total:   28704, Move:       1104 (Decl x 1.000000) L2
             Out : Ratio: 1.000000, FixDim:     65, VarDim:     48 [   276], Size:   6240, Total:   34944, Move:      17940 (Decl x 1.000000) L2
*          Scale : Ratio: 1.000000,                                          Size:    276, Total:   35220, Move:        276 (Decl x 1.000000) L2
*         ScaleN : Ratio: 1.000000,                                          Size:    276, Total:   35496, Move:        276 (Decl x 1.000000) L2
S34_Conv2d_276x276x1x1_Relu - IterSpace: Tile1 - L1 Memory:  35496, L2Move: 95772, L3Move: 0, Tiling Overhead: 1.000000
S34_Conv2d_276x276x1x1_Relu Iteration for Tiled Space: 6
Ker: S34_Conv2d_276x276x1x1_Relu, Arg:    KerBuff, Size:   1104, Base1:      0, Base2:      0
Ker: S34_Conv2d_276x276x1x1_Relu, Arg:        In1, Size:  13248, Base1:   1104, Base2:  14352
Ker: S34_Conv2d_276x276x1x1_Relu, Arg:       Bias, Size:   1104, Base1:  27600, Base2:      0
Ker: S34_Conv2d_276x276x1x1_Relu, Arg:        Out, Size:   3120, Base1:  28704, Base2:  31824
Ker: S34_Conv2d_276x276x1x1_Relu, Arg:      Scale, Size:    276, Base1:  34944, Base2:      0
Ker: S34_Conv2d_276x276x1x1_Relu, Arg:     ScaleN, Size:    276, Base1:  35220, Base2:      0
S34_Conv2d_276x276x1x1_Relu For Iter Space: 1 Iteration count:   6 (Last one is truncated), Given L1 Memory:  38336, Used L1 Memory:  35496, Reusable Memory: 2840, Used L2 Memory: 0

Reference object:                  In2, Dim=65
	                 In2 Dim:  65, TileOverlap:  0, Ratio: 1.000000

S34_Conv2d_276x276x1x1_Relu, TiledSpace: Tile0 Iteration Count: 5
             In2 : Ratio: 1.000000, FixDim:    276, VarDim:     16 [    65], Size:   8832, Total:    8832, Move:     107640 (Decl x 6.000000) L2
*          Infos : Ratio: 0.000000,                                          Size:     12, Total:    8844, Move:          9 (Decl x 1.000000) L2
S34_Conv2d_276x276x1x1_Relu - IterSpace: Tile0 - L1 Memory:   8844, L2Move: 107649, L3Move: 0, Tiling Overhead: 5.997493
S34_Conv2d_276x276x1x1_Relu Iteration for Tiled Space: 5
Ker: S34_Conv2d_276x276x1x1_Relu, Arg:        In2, Size:   4416, Base1:  35496, Base2:  39912
Ker: S34_Conv2d_276x276x1x1_Relu, Arg:      Infos, Size:     12, Base1:  44328, Base2:      0
S34_Conv2d_276x276x1x1_Relu For Iter Space: 0 Iteration count:   5 (Last one is truncated), Given L1 Memory:  10399, Used L1 Memory:   8844, Reusable Memory: 1552, Used L2 Memory: 0
=================================================================================================

Pool => W: 5, Pad:[0,0] => Wo: 1
     => H: 13, Pad:[0,0] => Ho: 1
OverlapP: 11
TileCons: 2
UsedIn  : [5 x 13]
         PoolKerName: KerParPoolNxMStrideSxSy_SQ8
Nb Oper : 17940

==== Process Tiling For User Kernel:                 S35_AveragePool_13x5 =======================
S35_AveragePool_13x5 Partition[0] Size =  36999 (Min:    130, Max:  36459), Fraction:       1.00, Giving:  48736 Bytes out of  48736 Bytes

Reference object:                  Out, Dim=1
	                  In Dim:  13, TileOverlap: 11, Ratio: 2.000000
	                 Out Dim:   1, TileOverlap:  0, Ratio: 1.000000

Kernel: S35_AveragePool_13x5, Total Raw Memory: 18228 fits into L1 memory 48736. Promoting all kernel arguments to initialized buffers.
Ker: S35_AveragePool_13x5, Arg:         In, Size:  17940, Base1:      0, Base2:      0
Ker: S35_AveragePool_13x5, Arg:        Out, Size:    276, Base1:  17940, Base2:      0
Ker: S35_AveragePool_13x5, Arg:      Infos, Size:     12, Base1:  18216, Base2:      0
S35_AveragePool_13x5 For Iter Space: 0 Iteration count:   1, Given L1 Memory:  48736, Used L1 Memory:  18228, Reusable Memory: 30508, Used L2 Memory: 0
=================================================================================================

Linear Layer S38_Linear_12x276x1x1, Linear: InDim: 276, OutDim: 12, Activation: None
Linear Kernel: KerParLinearLayerFullFeatB32_SQ8

==== Process Tiling For User Kernel:                S38_Linear_12x276x1x1 =======================
S38_Linear_12x276x1x1 Partition[0] Size =   7091 (Min:      0, Max:   7175), Fraction:       1.00, Giving:  48736 Bytes out of  48736 Bytes

Reference object:                   In, Dim=1

Kernel: S38_Linear_12x276x1x1, Total Raw Memory: 3684 fits into L1 memory 48736. Promoting all kernel arguments to initialized buffers.
Ker: S38_Linear_12x276x1x1, Arg:         In, Size:    276, Base1:      0, Base2:      0
Ker: S38_Linear_12x276x1x1, Arg:     Filter, Size:   3312, Base1:    276, Base2:      0
Ker: S38_Linear_12x276x1x1, Arg:       Bias, Size:     48, Base1:   3588, Base2:      0
Ker: S38_Linear_12x276x1x1, Arg:        Out, Size:     12, Base1:   3636, Base2:      0
Ker: S38_Linear_12x276x1x1, Arg:      Scale, Size:     12, Base1:   3648, Base2:      0
Ker: S38_Linear_12x276x1x1, Arg:     ScaleN, Size:     12, Base1:   3660, Base2:      0
Ker: S38_Linear_12x276x1x1, Arg:      Infos, Size:     12, Base1:   3672, Base2:      0
S38_Linear_12x276x1x1 For Iter Space: 0 Iteration count:   1, Given L1 Memory:  48736, Used L1 Memory:   3684, Reusable Memory: 45052, Used L2 Memory: 0
=================================================================================================


==== Process Tiling For User Kernel:                          S39_SoftMax =======================
         S39_SoftMax Partition[0] Size =     51 (Min:      8, Max:     63), Fraction:       1.00, Giving:  48736 Bytes out of  48736 Bytes

Reference object:                   In, Dim=12
	                  In Dim:  12, TileOverlap:  0, Ratio: 1.000000
	                 Out Dim:  12, TileOverlap:  0, Ratio: 1.000000

Kernel:          S39_SoftMax, Total Raw Memory: 48 fits into L1 memory 48736. Promoting all kernel arguments to initialized buffers.
Ker: S39_SoftMax, Arg:         In, Size:     12, Base1:      0, Base2:      0
Ker: S39_SoftMax, Arg:        Out, Size:     24, Base1:     12, Base2:      0
Ker: S39_SoftMax, Arg:      Infos, Size:     12, Base1:     36, Base2:      0
         S39_SoftMax For Iter Space: 0 Iteration count:   1, Given L1 Memory:  48736, Used L1 Memory:     48, Reusable Memory: 48688, Used L2 Memory: 0
=================================================================================================

   Symbol:           S38_Output[   In] Adding   Edge From               S38_Linear_12x276x1x1 To                         S39_SoftMax New
   Symbol:           S35_Output[   In] Adding   Edge From                S35_AveragePool_13x5 To               S38_Linear_12x276x1x1 New
   Symbol:           S34_Output[   In] Adding   Edge From         S34_Conv2d_276x276x1x1_Relu To                S35_AveragePool_13x5 New
   Symbol:           S31_Output[   In] Adding   Edge From           S31_Conv2d_276x1x3x3_Relu To         S34_Conv2d_276x276x1x1_Relu New
   Symbol:           S28_Output[   In] Adding   Edge From         S28_Conv2d_276x276x1x1_Relu To           S31_Conv2d_276x1x3x3_Relu New
   Symbol:           S25_Output[   In] Adding   Edge From           S25_Conv2d_276x1x3x3_Relu To         S28_Conv2d_276x276x1x1_Relu New
   Symbol:           S22_Output[   In] Adding   Edge From         S22_Conv2d_276x276x1x1_Relu To           S25_Conv2d_276x1x3x3_Relu New
   Symbol:           S19_Output[   In] Adding   Edge From           S19_Conv2d_276x1x3x3_Relu To         S22_Conv2d_276x276x1x1_Relu New
   Symbol:           S16_Output[   In] Adding   Edge From         S16_Conv2d_276x276x1x1_Relu To           S19_Conv2d_276x1x3x3_Relu New
   Symbol:           S13_Output[   In] Adding   Edge From           S13_Conv2d_276x1x3x3_Relu To         S16_Conv2d_276x276x1x1_Relu New
   Symbol:           S10_Output[   In] Adding   Edge From         S10_Conv2d_276x276x1x1_Relu To           S13_Conv2d_276x1x3x3_Relu New
   Symbol:            S7_Output[   In] Adding   Edge From            S7_Conv2d_276x1x3x3_Relu To         S10_Conv2d_276x276x1x1_Relu New
   Symbol:            S4_Output[   In] Adding   Edge From           S4_Conv2d_276x1x10x4_Relu To            S7_Conv2d_276x1x3x3_Relu New
   Symbol:             Output_1[  Out] Adding   Edge From                         S39_SoftMax To                       __GraphExit__ New
   Symbol:            S39_Infos[   In] Adding   Edge From                      __GraphEntry__ To                         S39_SoftMax New
   Symbol:            S38_Infos[   In] Adding   Edge From                      __GraphEntry__ To               S38_Linear_12x276x1x1 New
   Symbol:        S38_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To               S38_Linear_12x276x1x1 Exists
   Symbol:        S38_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To               S38_Linear_12x276x1x1 Exists
   Symbol:  Dscnnfc1matmul_bias[   In] Adding   Edge From                      __GraphEntry__ To               S38_Linear_12x276x1x1 Exists
   Symbol: Dscnnfc1weights_quantfakequant[   In] Adding   Edge From                      __GraphEntry__ To               S38_Linear_12x276x1x1 Exists
   Symbol:            S35_Infos[   In] Adding   Edge From                      __GraphEntry__ To                S35_AveragePool_13x5 New
   Symbol:            S34_Infos[   In] Adding   Edge From                      __GraphEntry__ To         S34_Conv2d_276x276x1x1_Relu New
   Symbol:        S34_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To         S34_Conv2d_276x276x1x1_Relu Exists
   Symbol:        S34_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To         S34_Conv2d_276x276x1x1_Relu Exists
   Symbol: Dscnnconv_ds_5pw_convconv2d_fo[   In] Adding   Edge From                      __GraphEntry__ To         S34_Conv2d_276x276x1x1_Relu Exists
   Symbol: Dscnnconv_ds_5pw_convweights_q[   In] Adding   Edge From                      __GraphEntry__ To         S34_Conv2d_276x276x1x1_Relu Exists
   Symbol:            S31_Infos[   In] Adding   Edge From                      __GraphEntry__ To           S31_Conv2d_276x1x3x3_Relu New
   Symbol:        S31_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To           S31_Conv2d_276x1x3x3_Relu Exists
   Symbol:        S31_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To           S31_Conv2d_276x1x3x3_Relu Exists
   Symbol: Dscnnconv_ds_5dw_convdepthwise[   In] Adding   Edge From                      __GraphEntry__ To           S31_Conv2d_276x1x3x3_Relu Exists
   Symbol: Dscnnconv_ds_5dw_convweights_q[   In] Adding   Edge From                      __GraphEntry__ To           S31_Conv2d_276x1x3x3_Relu Exists
   Symbol:            S28_Infos[   In] Adding   Edge From                      __GraphEntry__ To         S28_Conv2d_276x276x1x1_Relu New
   Symbol:        S28_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To         S28_Conv2d_276x276x1x1_Relu Exists
   Symbol:        S28_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To         S28_Conv2d_276x276x1x1_Relu Exists
   Symbol: Dscnnconv_ds_4pw_convconv2d_fo[   In] Adding   Edge From                      __GraphEntry__ To         S28_Conv2d_276x276x1x1_Relu Exists
   Symbol: Dscnnconv_ds_4pw_convweights_q[   In] Adding   Edge From                      __GraphEntry__ To         S28_Conv2d_276x276x1x1_Relu Exists
   Symbol:            S25_Infos[   In] Adding   Edge From                      __GraphEntry__ To           S25_Conv2d_276x1x3x3_Relu New
   Symbol:        S25_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To           S25_Conv2d_276x1x3x3_Relu Exists
   Symbol:        S25_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To           S25_Conv2d_276x1x3x3_Relu Exists
   Symbol: Dscnnconv_ds_4dw_convdepthwise[   In] Adding   Edge From                      __GraphEntry__ To           S25_Conv2d_276x1x3x3_Relu Exists
   Symbol: Dscnnconv_ds_4dw_convweights_q[   In] Adding   Edge From                      __GraphEntry__ To           S25_Conv2d_276x1x3x3_Relu Exists
   Symbol:            S22_Infos[   In] Adding   Edge From                      __GraphEntry__ To         S22_Conv2d_276x276x1x1_Relu New
   Symbol:        S22_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To         S22_Conv2d_276x276x1x1_Relu Exists
   Symbol:        S22_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To         S22_Conv2d_276x276x1x1_Relu Exists
   Symbol: Dscnnconv_ds_3pw_convconv2d_fo[   In] Adding   Edge From                      __GraphEntry__ To         S22_Conv2d_276x276x1x1_Relu Exists
   Symbol: Dscnnconv_ds_3pw_convweights_q[   In] Adding   Edge From                      __GraphEntry__ To         S22_Conv2d_276x276x1x1_Relu Exists
   Symbol:            S19_Infos[   In] Adding   Edge From                      __GraphEntry__ To           S19_Conv2d_276x1x3x3_Relu New
   Symbol:        S19_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To           S19_Conv2d_276x1x3x3_Relu Exists
   Symbol:        S19_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To           S19_Conv2d_276x1x3x3_Relu Exists
   Symbol: Dscnnconv_ds_3dw_convdepthwise[   In] Adding   Edge From                      __GraphEntry__ To           S19_Conv2d_276x1x3x3_Relu Exists
   Symbol: Dscnnconv_ds_3dw_convweights_q[   In] Adding   Edge From                      __GraphEntry__ To           S19_Conv2d_276x1x3x3_Relu Exists
   Symbol:            S16_Infos[   In] Adding   Edge From                      __GraphEntry__ To         S16_Conv2d_276x276x1x1_Relu New
   Symbol:        S16_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To         S16_Conv2d_276x276x1x1_Relu Exists
   Symbol:        S16_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To         S16_Conv2d_276x276x1x1_Relu Exists
   Symbol: Dscnnconv_ds_2pw_convconv2d_fo[   In] Adding   Edge From                      __GraphEntry__ To         S16_Conv2d_276x276x1x1_Relu Exists
   Symbol: Dscnnconv_ds_2pw_convweights_q[   In] Adding   Edge From                      __GraphEntry__ To         S16_Conv2d_276x276x1x1_Relu Exists
   Symbol:            S13_Infos[   In] Adding   Edge From                      __GraphEntry__ To           S13_Conv2d_276x1x3x3_Relu New
   Symbol:        S13_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To           S13_Conv2d_276x1x3x3_Relu Exists
   Symbol:        S13_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To           S13_Conv2d_276x1x3x3_Relu Exists
   Symbol: Dscnnconv_ds_2dw_convdepthwise[   In] Adding   Edge From                      __GraphEntry__ To           S13_Conv2d_276x1x3x3_Relu Exists
   Symbol: Dscnnconv_ds_2dw_convweights_q[   In] Adding   Edge From                      __GraphEntry__ To           S13_Conv2d_276x1x3x3_Relu Exists
   Symbol:            S10_Infos[   In] Adding   Edge From                      __GraphEntry__ To         S10_Conv2d_276x276x1x1_Relu New
   Symbol:        S10_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To         S10_Conv2d_276x276x1x1_Relu Exists
   Symbol:        S10_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To         S10_Conv2d_276x276x1x1_Relu Exists
   Symbol: Dscnnconv_ds_1pw_convconv2d_fo[   In] Adding   Edge From                      __GraphEntry__ To         S10_Conv2d_276x276x1x1_Relu Exists
   Symbol: Dscnnconv_ds_1pw_convweights_q[   In] Adding   Edge From                      __GraphEntry__ To         S10_Conv2d_276x276x1x1_Relu Exists
   Symbol:             S7_Infos[   In] Adding   Edge From                      __GraphEntry__ To            S7_Conv2d_276x1x3x3_Relu New
   Symbol:         S7_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To            S7_Conv2d_276x1x3x3_Relu Exists
   Symbol:         S7_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To            S7_Conv2d_276x1x3x3_Relu Exists
   Symbol: Dscnnconv_ds_1dw_convdepthwise[   In] Adding   Edge From                      __GraphEntry__ To            S7_Conv2d_276x1x3x3_Relu Exists
   Symbol: Dscnnconv_ds_1dw_convweights_q[   In] Adding   Edge From                      __GraphEntry__ To            S7_Conv2d_276x1x3x3_Relu Exists
   Symbol:             S4_Infos[   In] Adding   Edge From                      __GraphEntry__ To           S4_Conv2d_276x1x10x4_Relu New
   Symbol:         S4_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To           S4_Conv2d_276x1x10x4_Relu Exists
   Symbol:         S4_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To           S4_Conv2d_276x1x10x4_Relu Exists
   Symbol: Dscnnconv_1conv2d_fold_bias[   In] Adding   Edge From                      __GraphEntry__ To           S4_Conv2d_276x1x10x4_Relu Exists
   Symbol: Dscnnconv_1weights_quantfakequ[   In] Adding   Edge From                      __GraphEntry__ To           S4_Conv2d_276x1x10x4_Relu Exists
   Symbol:              Input_1[   In] Adding   Edge From                      __GraphEntry__ To           S4_Conv2d_276x1x10x4_Relu Exists
After Dynamic Allocation, TopL3: 0, TopL2: 86940 => Alloc: OK

After Const Allocation, TopL3: 304704, TopL2: 284517 => Alloc: OK

[FULL] Remapping [163116 .. 284516] to [0 .. 121400] Align compensation: 3
[PART] Remapping [0 .. 163115] to [121404 .. 284519] Align compensation: 0
[PART] Remapping [284517 .. 349999] to [284520 .. 350002] Align compensation: 1
[FULL] Remapping [0 .. 304703] to [0 .. 304703] Align compensation: 0
[PART] Remapping [304704 .. 6388607] to [304704 .. 6388607] Align compensation: 0
Symbol allocation for graph KWS_ds_cnn_l_quantCNN is sucessfull, L2: 284517 out of 350000, L3: 304704 out of 6388608
------------------------------------------------------------------------------------------------------------------------------------------------
Graph structure:

Node   0, Channel   0  0: GraphEntry __GraphEntry__, Operations: 0
                                   (null) =>                        Input_1
                                   (null) => Dscnnconv_1weights_quantfakequ
                                   (null) =>    Dscnnconv_1conv2d_fold_bias
                                   (null) =>                   S4_Mul_scale
                                   (null) =>                   S4_Mul_shift
                                   (null) =>                       S4_Infos
                                   (null) => Dscnnconv_ds_1dw_convweights_q
                                   (null) => Dscnnconv_ds_1dw_convdepthwise
                                   (null) =>                   S7_Mul_scale
                                   (null) =>                   S7_Mul_shift
                                   (null) =>                       S7_Infos
                                   (null) => Dscnnconv_ds_1pw_convweights_q
                                   (null) => Dscnnconv_ds_1pw_convconv2d_fo
                                   (null) =>                  S10_Mul_scale
                                   (null) =>                  S10_Mul_shift
                                   (null) =>                      S10_Infos
                                   (null) => Dscnnconv_ds_2dw_convweights_q
                                   (null) => Dscnnconv_ds_2dw_convdepthwise
                                   (null) =>                  S13_Mul_scale
                                   (null) =>                  S13_Mul_shift
                                   (null) =>                      S13_Infos
                                   (null) => Dscnnconv_ds_2pw_convweights_q
                                   (null) => Dscnnconv_ds_2pw_convconv2d_fo
                                   (null) =>                  S16_Mul_scale
                                   (null) =>                  S16_Mul_shift
                                   (null) =>                      S16_Infos
                                   (null) => Dscnnconv_ds_3dw_convweights_q
                                   (null) => Dscnnconv_ds_3dw_convdepthwise
                                   (null) =>                  S19_Mul_scale
                                   (null) =>                  S19_Mul_shift
                                   (null) =>                      S19_Infos
                                   (null) => Dscnnconv_ds_3pw_convweights_q
                                   (null) => Dscnnconv_ds_3pw_convconv2d_fo
                                   (null) =>                  S22_Mul_scale
                                   (null) =>                  S22_Mul_shift
                                   (null) =>                      S22_Infos
                                   (null) => Dscnnconv_ds_4dw_convweights_q
                                   (null) => Dscnnconv_ds_4dw_convdepthwise
                                   (null) =>                  S25_Mul_scale
                                   (null) =>                  S25_Mul_shift
                                   (null) =>                      S25_Infos
                                   (null) => Dscnnconv_ds_4pw_convweights_q
                                   (null) => Dscnnconv_ds_4pw_convconv2d_fo
                                   (null) =>                  S28_Mul_scale
                                   (null) =>                  S28_Mul_shift
                                   (null) =>                      S28_Infos
                                   (null) => Dscnnconv_ds_5dw_convweights_q
                                   (null) => Dscnnconv_ds_5dw_convdepthwise
                                   (null) =>                  S31_Mul_scale
                                   (null) =>                  S31_Mul_shift
                                   (null) =>                      S31_Infos
                                   (null) => Dscnnconv_ds_5pw_convweights_q
                                   (null) => Dscnnconv_ds_5pw_convconv2d_fo
                                   (null) =>                  S34_Mul_scale
                                   (null) =>                  S34_Mul_shift
                                   (null) =>                      S34_Infos
                                   (null) =>                      S35_Infos
                                   (null) => Dscnnfc1weights_quantfakequant
                                   (null) =>            Dscnnfc1matmul_bias
                                   (null) =>                  S38_Mul_scale
                                   (null) =>                  S38_Mul_shift
                                   (null) =>                      S38_Infos
                                   (null) =>                      S39_Infos
	Kernel Memory      : L3:       0, L2:       0
	Kernel Total Memory:       0, L3 moves:       0, L2 moves:       0, Move overhead: 1.000000
	Kernel Operations  :       0 [KernelOper/GraphOper: 0.000000%], Move/Operation ratio: [L3: 0.000000, L2: 0.000000]
	Successors:  2 1 3 4 5 6 7 8 9 10 11 12 13 14

    Living Dynamic Symbols: [Input_1] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node   1, Channel   1  1:       UKer S4_Conv2d_276x1x10x4_Relu, Operations: 2829000
 I                                     In =>                        Input_1    --L2--    Size:     490, L3_Move:         0, L2_Move:      1940, TileOverhead: 3.959184, L2Buff:     0, Addr: 0
CI PartBuff                        Filter => Dscnnconv_1weights_quantfakequ    --L2--    Size:   11040, L3_Move:         0, L2_Move:     11040, TileOverhead: 1.000000, L2Buff:     0, Addr: 2024
CI                                   Bias =>    Dscnnconv_1conv2d_fold_bias    --L2--    Size:    1104, L3_Move:         0, L2_Move:      1104, TileOverhead: 1.000000, L2Buff:     0, Addr: 320
 O                                    Out =>                      S4_Output    --L2--    Size:   69000, L3_Move:         0, L2_Move:     69000, TileOverhead: 1.000000, L2Buff:     0, Addr: 13544
CI Buff                             Scale =>                   S4_Mul_scale    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 1472
CI Buff                            ScaleN =>                   S4_Mul_shift    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 1748
CI Buff                             Infos =>                       S4_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 48104
	Kernel Memory      : L3:       0, L2:   82195
	Kernel Total Memory:   82195, L3 moves:       0, L2 moves:   83645, Move overhead: 1.017641
	Kernel Operations  : 2829000 [KernelOper/GraphOper: 9.924761%], Move/Operation ratio: [L3: 0.000000, L2: 0.029567]
	Successors:  2

    Living Dynamic Symbols: [Input_1] [S4_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node   2, Channel   0  0:       UKer S7_Conv2d_276x1x3x3_Relu, Operations: 179400
 I                                     In =>                      S4_Output    --L2--    Size:   69000, L3_Move:         0, L2_Move:     69000, TileOverhead: 1.000000, L2Buff:     0, Addr: 0
CI Buff                            Filter => Dscnnconv_ds_1dw_convweights_q    --L2--    Size:    2484, L3_Move:         0, L2_Move:      2484, TileOverhead: 1.000000, L2Buff:     0, Addr: 25656
CI Buff                              Bias => Dscnnconv_ds_1dw_convdepthwise    --L2--    Size:    1104, L3_Move:         0, L2_Move:      1104, TileOverhead: 1.000000, L2Buff:     0, Addr: 24000
 O                                    Out =>                      S7_Output    --L2--    Size:   17940, L3_Move:         0, L2_Move:     17940, TileOverhead: 1.000000, L2Buff:     0, Addr: 28140
CI Buff                             Scale =>                   S7_Mul_scale    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 25104
CI Buff                            ScaleN =>                   S7_Mul_shift    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 25380
CI Buff                             Infos =>                       S7_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 46860
	Kernel Memory      : L3:       0, L2:   91089
	Kernel Total Memory:   91089, L3 moves:       0, L2 moves:   91089, Move overhead: 1.000000
	Kernel Operations  :  179400 [KernelOper/GraphOper: 0.629375%], Move/Operation ratio: [L3: 0.000000, L2: 0.507742]
	Successors:  3

    Living Dynamic Symbols: [S4_Output] [S7_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node   3, Channel   0  0:       UKer S10_Conv2d_276x276x1x1_Relu, Operations: 4951440
 I                                    In2 =>                      S7_Output    --L2--    Size:   17940, L3_Move:         0, L2_Move:    107640, TileOverhead: 6.000000, L2Buff:     0, Addr: 35496
CI                                    In1 => Dscnnconv_ds_1pw_convweights_q    --L2--    Size:   76176, L3_Move:     76176, L2_Move:     76176, TileOverhead: 1.000000, L2Buff:     0, Addr: 1104
CI Buff                              Bias => Dscnnconv_ds_1pw_convconv2d_fo    --L2--    Size:    1104, L3_Move:         0, L2_Move:      1104, TileOverhead: 1.000000, L2Buff:     0, Addr: 27600
 O                                    Out =>                     S10_Output    --L2--    Size:   17940, L3_Move:         0, L2_Move:     17940, TileOverhead: 1.000000, L2Buff:     0, Addr: 28704
CI Buff                             Scale =>                  S10_Mul_scale    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 34944
CI Buff                            ScaleN =>                  S10_Mul_shift    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 35220
CI Buff                             Infos =>                      S10_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 44328
	Kernel Memory      : L3:       0, L2:  113721
	Kernel Total Memory:  113721, L3 moves:   76176, L2 moves:  203421, Move overhead: 1.788772
	Kernel Operations  : 4951440 [KernelOper/GraphOper: 17.370753%], Move/Operation ratio: [L3: 0.015385, L2: 0.041083]
	Successors:  4

    Living Dynamic Symbols: [S7_Output] [S10_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node   4, Channel   0  0:       UKer S13_Conv2d_276x1x3x3_Relu, Operations: 179400
 I                                     In =>                     S10_Output    --L2--    Size:   17940, L3_Move:         0, L2_Move:     17940, TileOverhead: 1.000000, L2Buff:     0, Addr: 0
CI Buff                            Filter => Dscnnconv_ds_2dw_convweights_q    --L2--    Size:    2484, L3_Move:         0, L2_Move:      2484, TileOverhead: 1.000000, L2Buff:     0, Addr: 12056
CI Buff                              Bias => Dscnnconv_ds_2dw_convdepthwise    --L2--    Size:    1104, L3_Move:         0, L2_Move:      1104, TileOverhead: 1.000000, L2Buff:     0, Addr: 10400
 O                                    Out =>                     S13_Output    --L2--    Size:   17940, L3_Move:         0, L2_Move:     17940, TileOverhead: 1.000000, L2Buff:     0, Addr: 14540
CI Buff                             Scale =>                  S13_Mul_scale    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 11504
CI Buff                            ScaleN =>                  S13_Mul_shift    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 11780
CI Buff                             Infos =>                      S13_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 45740
	Kernel Memory      : L3:       0, L2:   40029
	Kernel Total Memory:   40029, L3 moves:       0, L2 moves:   40029, Move overhead: 1.000000
	Kernel Operations  :  179400 [KernelOper/GraphOper: 0.629375%], Move/Operation ratio: [L3: 0.000000, L2: 0.223127]
	Successors:  5

    Living Dynamic Symbols: [S10_Output] [S13_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node   5, Channel   0  0:       UKer S16_Conv2d_276x276x1x1_Relu, Operations: 4951440
 I                                    In2 =>                     S13_Output    --L2--    Size:   17940, L3_Move:         0, L2_Move:    107640, TileOverhead: 6.000000, L2Buff:     0, Addr: 35496
CI                                    In1 => Dscnnconv_ds_2pw_convweights_q    --L2--    Size:   76176, L3_Move:     76176, L2_Move:     76176, TileOverhead: 1.000000, L2Buff:     0, Addr: 1104
CI Buff                              Bias => Dscnnconv_ds_2pw_convconv2d_fo    --L2--    Size:    1104, L3_Move:         0, L2_Move:      1104, TileOverhead: 1.000000, L2Buff:     0, Addr: 27600
 O                                    Out =>                     S16_Output    --L2--    Size:   17940, L3_Move:         0, L2_Move:     17940, TileOverhead: 1.000000, L2Buff:     0, Addr: 28704
CI Buff                             Scale =>                  S16_Mul_scale    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 34944
CI Buff                            ScaleN =>                  S16_Mul_shift    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 35220
CI Buff                             Infos =>                      S16_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 44328
	Kernel Memory      : L3:       0, L2:  113721
	Kernel Total Memory:  113721, L3 moves:   76176, L2 moves:  203421, Move overhead: 1.788772
	Kernel Operations  : 4951440 [KernelOper/GraphOper: 17.370753%], Move/Operation ratio: [L3: 0.015385, L2: 0.041083]
	Successors:  6

    Living Dynamic Symbols: [S13_Output] [S16_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node   6, Channel   0  0:       UKer S19_Conv2d_276x1x3x3_Relu, Operations: 179400
 I                                     In =>                     S16_Output    --L2--    Size:   17940, L3_Move:         0, L2_Move:     17940, TileOverhead: 1.000000, L2Buff:     0, Addr: 0
CI Buff                            Filter => Dscnnconv_ds_3dw_convweights_q    --L2--    Size:    2484, L3_Move:         0, L2_Move:      2484, TileOverhead: 1.000000, L2Buff:     0, Addr: 12056
CI Buff                              Bias => Dscnnconv_ds_3dw_convdepthwise    --L2--    Size:    1104, L3_Move:         0, L2_Move:      1104, TileOverhead: 1.000000, L2Buff:     0, Addr: 10400
 O                                    Out =>                     S19_Output    --L2--    Size:   17940, L3_Move:         0, L2_Move:     17940, TileOverhead: 1.000000, L2Buff:     0, Addr: 14540
CI Buff                             Scale =>                  S19_Mul_scale    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 11504
CI Buff                            ScaleN =>                  S19_Mul_shift    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 11780
CI Buff                             Infos =>                      S19_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 45740
	Kernel Memory      : L3:       0, L2:   40029
	Kernel Total Memory:   40029, L3 moves:       0, L2 moves:   40029, Move overhead: 1.000000
	Kernel Operations  :  179400 [KernelOper/GraphOper: 0.629375%], Move/Operation ratio: [L3: 0.000000, L2: 0.223127]
	Successors:  7

    Living Dynamic Symbols: [S16_Output] [S19_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node   7, Channel   0  0:       UKer S22_Conv2d_276x276x1x1_Relu, Operations: 4951440
 I                                    In2 =>                     S19_Output    --L2--    Size:   17940, L3_Move:         0, L2_Move:    107640, TileOverhead: 6.000000, L2Buff:     0, Addr: 35496
CI                                    In1 => Dscnnconv_ds_3pw_convweights_q    --L2--    Size:   76176, L3_Move:     76176, L2_Move:     76176, TileOverhead: 1.000000, L2Buff:     0, Addr: 1104
CI Buff                              Bias => Dscnnconv_ds_3pw_convconv2d_fo    --L2--    Size:    1104, L3_Move:         0, L2_Move:      1104, TileOverhead: 1.000000, L2Buff:     0, Addr: 27600
 O                                    Out =>                     S22_Output    --L2--    Size:   17940, L3_Move:         0, L2_Move:     17940, TileOverhead: 1.000000, L2Buff:     0, Addr: 28704
CI Buff                             Scale =>                  S22_Mul_scale    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 34944
CI Buff                            ScaleN =>                  S22_Mul_shift    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 35220
CI Buff                             Infos =>                      S22_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 44328
	Kernel Memory      : L3:       0, L2:  113721
	Kernel Total Memory:  113721, L3 moves:   76176, L2 moves:  203421, Move overhead: 1.788772
	Kernel Operations  : 4951440 [KernelOper/GraphOper: 17.370753%], Move/Operation ratio: [L3: 0.015385, L2: 0.041083]
	Successors:  8

    Living Dynamic Symbols: [S19_Output] [S22_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node   8, Channel   0  0:       UKer S25_Conv2d_276x1x3x3_Relu, Operations: 179400
 I                                     In =>                     S22_Output    --L2--    Size:   17940, L3_Move:         0, L2_Move:     17940, TileOverhead: 1.000000, L2Buff:     0, Addr: 0
CI Buff                            Filter => Dscnnconv_ds_4dw_convweights_q    --L2--    Size:    2484, L3_Move:         0, L2_Move:      2484, TileOverhead: 1.000000, L2Buff:     0, Addr: 12056
CI Buff                              Bias => Dscnnconv_ds_4dw_convdepthwise    --L2--    Size:    1104, L3_Move:         0, L2_Move:      1104, TileOverhead: 1.000000, L2Buff:     0, Addr: 10400
 O                                    Out =>                     S25_Output    --L2--    Size:   17940, L3_Move:         0, L2_Move:     17940, TileOverhead: 1.000000, L2Buff:     0, Addr: 14540
CI Buff                             Scale =>                  S25_Mul_scale    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 11504
CI Buff                            ScaleN =>                  S25_Mul_shift    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 11780
CI Buff                             Infos =>                      S25_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 45740
	Kernel Memory      : L3:       0, L2:   40029
	Kernel Total Memory:   40029, L3 moves:       0, L2 moves:   40029, Move overhead: 1.000000
	Kernel Operations  :  179400 [KernelOper/GraphOper: 0.629375%], Move/Operation ratio: [L3: 0.000000, L2: 0.223127]
	Successors:  9

    Living Dynamic Symbols: [S22_Output] [S25_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node   9, Channel   0  0:       UKer S28_Conv2d_276x276x1x1_Relu, Operations: 4951440
 I                                    In2 =>                     S25_Output    --L2--    Size:   17940, L3_Move:         0, L2_Move:    107640, TileOverhead: 6.000000, L2Buff:     0, Addr: 35496
CI                                    In1 => Dscnnconv_ds_4pw_convweights_q    --L2--    Size:   76176, L3_Move:     76176, L2_Move:     76176, TileOverhead: 1.000000, L2Buff:     0, Addr: 1104
CI Buff                              Bias => Dscnnconv_ds_4pw_convconv2d_fo    --L2--    Size:    1104, L3_Move:         0, L2_Move:      1104, TileOverhead: 1.000000, L2Buff:     0, Addr: 27600
 O                                    Out =>                     S28_Output    --L2--    Size:   17940, L3_Move:         0, L2_Move:     17940, TileOverhead: 1.000000, L2Buff:     0, Addr: 28704
CI Buff                             Scale =>                  S28_Mul_scale    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 34944
CI Buff                            ScaleN =>                  S28_Mul_shift    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 35220
CI Buff                             Infos =>                      S28_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 44328
	Kernel Memory      : L3:       0, L2:  113721
	Kernel Total Memory:  113721, L3 moves:   76176, L2 moves:  203421, Move overhead: 1.788772
	Kernel Operations  : 4951440 [KernelOper/GraphOper: 17.370753%], Move/Operation ratio: [L3: 0.015385, L2: 0.041083]
	Successors:  10

    Living Dynamic Symbols: [S25_Output] [S28_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node  10, Channel   0  0:       UKer S31_Conv2d_276x1x3x3_Relu, Operations: 179400
 I                                     In =>                     S28_Output    --L2--    Size:   17940, L3_Move:         0, L2_Move:     17940, TileOverhead: 1.000000, L2Buff:     0, Addr: 0
CI Buff                            Filter => Dscnnconv_ds_5dw_convweights_q    --L2--    Size:    2484, L3_Move:         0, L2_Move:      2484, TileOverhead: 1.000000, L2Buff:     0, Addr: 12056
CI Buff                              Bias => Dscnnconv_ds_5dw_convdepthwise    --L2--    Size:    1104, L3_Move:         0, L2_Move:      1104, TileOverhead: 1.000000, L2Buff:     0, Addr: 10400
 O                                    Out =>                     S31_Output    --L2--    Size:   17940, L3_Move:         0, L2_Move:     17940, TileOverhead: 1.000000, L2Buff:     0, Addr: 14540
CI Buff                             Scale =>                  S31_Mul_scale    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 11504
CI Buff                            ScaleN =>                  S31_Mul_shift    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 11780
CI Buff                             Infos =>                      S31_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 45740
	Kernel Memory      : L3:       0, L2:   40029
	Kernel Total Memory:   40029, L3 moves:       0, L2 moves:   40029, Move overhead: 1.000000
	Kernel Operations  :  179400 [KernelOper/GraphOper: 0.629375%], Move/Operation ratio: [L3: 0.000000, L2: 0.223127]
	Successors:  11

    Living Dynamic Symbols: [S28_Output] [S31_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node  11, Channel   0  0:       UKer S34_Conv2d_276x276x1x1_Relu, Operations: 4951440
 I                                    In2 =>                     S31_Output    --L2--    Size:   17940, L3_Move:         0, L2_Move:    107640, TileOverhead: 6.000000, L2Buff:     0, Addr: 35496
CI                                    In1 => Dscnnconv_ds_5pw_convweights_q    --L2--    Size:   76176, L3_Move:         0, L2_Move:     76176, TileOverhead: 1.000000, L2Buff:     0, Addr: 1104
CI Buff                              Bias => Dscnnconv_ds_5pw_convconv2d_fo    --L2--    Size:    1104, L3_Move:         0, L2_Move:      1104, TileOverhead: 1.000000, L2Buff:     0, Addr: 27600
 O                                    Out =>                     S34_Output    --L2--    Size:   17940, L3_Move:         0, L2_Move:     17940, TileOverhead: 1.000000, L2Buff:     0, Addr: 28704
CI Buff                             Scale =>                  S34_Mul_scale    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 34944
CI Buff                            ScaleN =>                  S34_Mul_shift    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 35220
CI Buff                             Infos =>                      S34_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 44328
	Kernel Memory      : L3:       0, L2:  113721
	Kernel Total Memory:  113721, L3 moves:       0, L2 moves:  203421, Move overhead: 1.788772
	Kernel Operations  : 4951440 [KernelOper/GraphOper: 17.370753%], Move/Operation ratio: [L3: 0.000000, L2: 0.041083]
	Successors:  12

    Living Dynamic Symbols: [S31_Output] [S34_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node  12, Channel   0  0:       UKer S35_AveragePool_13x5, Operations: 17940
 I Buff                                In =>                     S34_Output    --L2--    Size:   17940, L3_Move:         0, L2_Move:     17940, TileOverhead: 1.000000, L2Buff:     0, Addr: 0
 O Buff                               Out =>                     S35_Output    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 17940
CI Buff                             Infos =>                      S35_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 18216
	Kernel Memory      : L3:       0, L2:   18225
	Kernel Total Memory:   18225, L3 moves:       0, L2 moves:   18225, Move overhead: 1.000000
	Kernel Operations  :   17940 [KernelOper/GraphOper: 0.062938%], Move/Operation ratio: [L3: 0.000000, L2: 1.015886]
	Successors:  13

    Living Dynamic Symbols: [S34_Output] [S35_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node  13, Channel   0  0:       UKer S38_Linear_12x276x1x1, Operations: 3312
 I Buff                                In =>                     S35_Output    --L2--    Size:     276, L3_Move:         0, L2_Move:       276, TileOverhead: 1.000000, L2Buff:     0, Addr: 0
CI Buff                            Filter => Dscnnfc1weights_quantfakequant    --L2--    Size:    3312, L3_Move:         0, L2_Move:      3312, TileOverhead: 1.000000, L2Buff:     0, Addr: 276
CI Buff                              Bias =>            Dscnnfc1matmul_bias    --L2--    Size:      48, L3_Move:         0, L2_Move:        48, TileOverhead: 1.000000, L2Buff:     0, Addr: 3588
 O Buff                               Out =>                     S38_Output    --L2--    Size:      12, L3_Move:         0, L2_Move:        12, TileOverhead: 1.000000, L2Buff:     0, Addr: 3636
CI Buff                             Scale =>                  S38_Mul_scale    --L2--    Size:      12, L3_Move:         0, L2_Move:        12, TileOverhead: 1.000000, L2Buff:     0, Addr: 3648
CI Buff                            ScaleN =>                  S38_Mul_shift    --L2--    Size:      12, L3_Move:         0, L2_Move:        12, TileOverhead: 1.000000, L2Buff:     0, Addr: 3660
CI Buff                             Infos =>                      S38_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 3672
	Kernel Memory      : L3:       0, L2:    3681
	Kernel Total Memory:    3681, L3 moves:       0, L2 moves:    3681, Move overhead: 1.000000
	Kernel Operations  :    3312 [KernelOper/GraphOper: 0.011619%], Move/Operation ratio: [L3: 0.000000, L2: 1.111413]
	Successors:  14

    Living Dynamic Symbols: [S35_Output] [S38_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node  14, Channel   0  0:       UKer S39_SoftMax, Operations: 12
 I Buff                                In =>                     S38_Output    --L2--    Size:      12, L3_Move:         0, L2_Move:        12, TileOverhead: 1.000000, L2Buff:     0, Addr: 0
 O Buff                               Out =>                       Output_1    --L2--    Size:      24, L3_Move:         0, L2_Move:        24, TileOverhead: 1.000000, L2Buff:     0, Addr: 12
CI Buff                             Infos =>                      S39_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 36
	Kernel Memory      : L3:       0, L2:      45
	Kernel Total Memory:      45, L3 moves:       0, L2 moves:      45, Move overhead: 1.000000
	Kernel Operations  :      12 [KernelOper/GraphOper: 0.000042%], Move/Operation ratio: [L3: 0.000000, L2: 3.750000]
	Successors:  15

    Living Dynamic Symbols: [Output_1] [S38_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node  15, Channel   0  0:  GraphExit __GraphExit__, Operations: 0
                                   (null) =>                       Output_1
	Kernel Memory      : L3:       0, L2:       0
	Kernel Total Memory:       0, L3 moves:       0, L2 moves:       0, Move overhead: 1.000000
	Kernel Operations  :       0 [KernelOper/GraphOper: 0.000000%], Move/Operation ratio: [L3: 0.000000, L2: 0.000000]
	Successors: 

    Living Dynamic Symbols: [Output_1] 

------------------------------------------------------------------------------------------------------------------------------------------------
	Graph nodes max local memory : L3:       0, L2:  113721
	Graph nodes min global memory: L3:   76176, L2:  113724
	Graph sum of kernel arguments size:  923956, L3 moves:  304704, L2 moves: 1373906, Move overhead: 1.486982
	Graph total operations: 28504464


------------------------------------------------------------------------------------------------------------------------------------------------

------------------------------------------------------------------------------------------------------------------------------------------------
Memory bandwidth report:

    Sum of All Kernel's arguments size:  923956, Total L3_Move:    304704, Total L2_Move:   1373906, Tiling Overhead Average: 1.486982

------------------------------------------------------------------------------------------------------------------------------------------------

------------------------------------------------------------------------------------------------------------------------------------------------
Total minimum memory requirement report:

                     L3 Memory       L2 Memory
       Dynamic         -228528           86940
         Const          304704           77844
         Total           76176          113724
------------------------------------------------------------------------------------------------------------------------------------------------

------------------------------------------------------------------------------------------------------------------------------------------------
Graph symbols allocation:

              Input_1  Externally allocated
 Dscnnconv_1weights_quantfakequ  INSTALL: HyperFlash[  0: 15]@ 380880    LOAD:         L2[  0: 15]@  76176    EXEC:         L2[  0: 15]@  76176 , Size:   11040
 Dscnnconv_1conv2d_fold_bias  INSTALL: HyperFlash[  0: 15]@ 407652    LOAD:         L2[  0: 15]@ 102948    EXEC:         L2[  0: 15]@ 102948 , Size:    1104
         S4_Mul_scale  INSTALL: HyperFlash[  0: 15]@ 419796    LOAD:         L2[  0: 15]@ 115092    EXEC:         L2[  0: 15]@ 115092 , Size:     276
         S4_Mul_shift  INSTALL: HyperFlash[  0: 15]@ 420072    LOAD:         L2[  0: 15]@ 115368    EXEC:         L2[  0: 15]@ 115368 , Size:     276
             S4_Infos  INSTALL: HyperFlash[  0: 15]@ 425916    LOAD:         L2[  0: 15]@ 121212    EXEC:         L2[  0: 15]@ 121212 , Size:       9
 Dscnnconv_ds_1dw_convweights_q  INSTALL: HyperFlash[  0: 15]@ 395232    LOAD:         L2[  0: 15]@  90528    EXEC:         L2[  0: 15]@  90528 , Size:    2484
 Dscnnconv_ds_1dw_convdepthwise  INSTALL: HyperFlash[  0: 15]@ 408756    LOAD:         L2[  0: 15]@ 104052    EXEC:         L2[  0: 15]@ 104052 , Size:    1104
         S7_Mul_scale  INSTALL: HyperFlash[  0: 15]@ 420348    LOAD:         L2[  0: 15]@ 115644    EXEC:         L2[  0: 15]@ 115644 , Size:     276
         S7_Mul_shift  INSTALL: HyperFlash[  0: 15]@ 420624    LOAD:         L2[  0: 15]@ 115920    EXEC:         L2[  0: 15]@ 115920 , Size:     276
             S7_Infos  INSTALL: HyperFlash[  0: 15]@ 425928    LOAD:         L2[  0: 15]@ 121224    EXEC:         L2[  0: 15]@ 121224 , Size:       9
 Dscnnconv_ds_1pw_convweights_q  INSTALL: HyperFlash[  0: 15]@      0    LOAD:   HyperRam[  0: 15]@      0    EXEC:         L2[  1:  3]@ 208344 , Size:   76176
 Dscnnconv_ds_1pw_convconv2d_fo  INSTALL: HyperFlash[  0: 15]@ 409860    LOAD:         L2[  0: 15]@ 105156    EXEC:         L2[  0: 15]@ 105156 , Size:    1104
        S10_Mul_scale  INSTALL: HyperFlash[  0: 15]@ 420900    LOAD:         L2[  0: 15]@ 116196    EXEC:         L2[  0: 15]@ 116196 , Size:     276
        S10_Mul_shift  INSTALL: HyperFlash[  0: 15]@ 421176    LOAD:         L2[  0: 15]@ 116472    EXEC:         L2[  0: 15]@ 116472 , Size:     276
            S10_Infos  INSTALL: HyperFlash[  0: 15]@ 425940    LOAD:         L2[  0: 15]@ 121236    EXEC:         L2[  0: 15]@ 121236 , Size:       9
 Dscnnconv_ds_2dw_convweights_q  INSTALL: HyperFlash[  0: 15]@ 397716    LOAD:         L2[  0: 15]@  93012    EXEC:         L2[  0: 15]@  93012 , Size:    2484
 Dscnnconv_ds_2dw_convdepthwise  INSTALL: HyperFlash[  0: 15]@ 410964    LOAD:         L2[  0: 15]@ 106260    EXEC:         L2[  0: 15]@ 106260 , Size:    1104
        S13_Mul_scale  INSTALL: HyperFlash[  0: 15]@ 421452    LOAD:         L2[  0: 15]@ 116748    EXEC:         L2[  0: 15]@ 116748 , Size:     276
        S13_Mul_shift  INSTALL: HyperFlash[  0: 15]@ 421728    LOAD:         L2[  0: 15]@ 117024    EXEC:         L2[  0: 15]@ 117024 , Size:     276
            S13_Infos  INSTALL: HyperFlash[  0: 15]@ 425952    LOAD:         L2[  0: 15]@ 121248    EXEC:         L2[  0: 15]@ 121248 , Size:       9
 Dscnnconv_ds_2pw_convweights_q  INSTALL: HyperFlash[  0: 15]@  76176    LOAD:   HyperRam[  0: 15]@  76176    EXEC:         L2[  4:  5]@ 157284 , Size:   76176
 Dscnnconv_ds_2pw_convconv2d_fo  INSTALL: HyperFlash[  0: 15]@ 412068    LOAD:         L2[  0: 15]@ 107364    EXEC:         L2[  0: 15]@ 107364 , Size:    1104
        S16_Mul_scale  INSTALL: HyperFlash[  0: 15]@ 422004    LOAD:         L2[  0: 15]@ 117300    EXEC:         L2[  0: 15]@ 117300 , Size:     276
        S16_Mul_shift  INSTALL: HyperFlash[  0: 15]@ 422280    LOAD:         L2[  0: 15]@ 117576    EXEC:         L2[  0: 15]@ 117576 , Size:     276
            S16_Infos  INSTALL: HyperFlash[  0: 15]@ 425964    LOAD:         L2[  0: 15]@ 121260    EXEC:         L2[  0: 15]@ 121260 , Size:       9
 Dscnnconv_ds_3dw_convweights_q  INSTALL: HyperFlash[  0: 15]@ 400200    LOAD:         L2[  0: 15]@  95496    EXEC:         L2[  0: 15]@  95496 , Size:    2484
 Dscnnconv_ds_3dw_convdepthwise  INSTALL: HyperFlash[  0: 15]@ 413172    LOAD:         L2[  0: 15]@ 108468    EXEC:         L2[  0: 15]@ 108468 , Size:    1104
        S19_Mul_scale  INSTALL: HyperFlash[  0: 15]@ 422556    LOAD:         L2[  0: 15]@ 117852    EXEC:         L2[  0: 15]@ 117852 , Size:     276
        S19_Mul_shift  INSTALL: HyperFlash[  0: 15]@ 422832    LOAD:         L2[  0: 15]@ 118128    EXEC:         L2[  0: 15]@ 118128 , Size:     276
            S19_Infos  INSTALL: HyperFlash[  0: 15]@ 425976    LOAD:         L2[  0: 15]@ 121272    EXEC:         L2[  0: 15]@ 121272 , Size:       9
 Dscnnconv_ds_3pw_convweights_q  INSTALL: HyperFlash[  0: 15]@ 152352    LOAD:   HyperRam[  0: 15]@ 152352    EXEC:         L2[  6:  7]@ 157284 , Size:   76176
 Dscnnconv_ds_3pw_convconv2d_fo  INSTALL: HyperFlash[  0: 15]@ 414276    LOAD:         L2[  0: 15]@ 109572    EXEC:         L2[  0: 15]@ 109572 , Size:    1104
        S22_Mul_scale  INSTALL: HyperFlash[  0: 15]@ 423108    LOAD:         L2[  0: 15]@ 118404    EXEC:         L2[  0: 15]@ 118404 , Size:     276
        S22_Mul_shift  INSTALL: HyperFlash[  0: 15]@ 423384    LOAD:         L2[  0: 15]@ 118680    EXEC:         L2[  0: 15]@ 118680 , Size:     276
            S22_Infos  INSTALL: HyperFlash[  0: 15]@ 425988    LOAD:         L2[  0: 15]@ 121284    EXEC:         L2[  0: 15]@ 121284 , Size:       9
 Dscnnconv_ds_4dw_convweights_q  INSTALL: HyperFlash[  0: 15]@ 402684    LOAD:         L2[  0: 15]@  97980    EXEC:         L2[  0: 15]@  97980 , Size:    2484
 Dscnnconv_ds_4dw_convdepthwise  INSTALL: HyperFlash[  0: 15]@ 415380    LOAD:         L2[  0: 15]@ 110676    EXEC:         L2[  0: 15]@ 110676 , Size:    1104
        S25_Mul_scale  INSTALL: HyperFlash[  0: 15]@ 423660    LOAD:         L2[  0: 15]@ 118956    EXEC:         L2[  0: 15]@ 118956 , Size:     276
        S25_Mul_shift  INSTALL: HyperFlash[  0: 15]@ 423936    LOAD:         L2[  0: 15]@ 119232    EXEC:         L2[  0: 15]@ 119232 , Size:     276
            S25_Infos  INSTALL: HyperFlash[  0: 15]@ 426000    LOAD:         L2[  0: 15]@ 121296    EXEC:         L2[  0: 15]@ 121296 , Size:       9
 Dscnnconv_ds_4pw_convweights_q  INSTALL: HyperFlash[  0: 15]@ 228528    LOAD:   HyperRam[  0: 15]@ 228528    EXEC:         L2[  8:  9]@ 175224 , Size:   76176
 Dscnnconv_ds_4pw_convconv2d_fo  INSTALL: HyperFlash[  0: 15]@ 416484    LOAD:         L2[  0: 15]@ 111780    EXEC:         L2[  0: 15]@ 111780 , Size:    1104
        S28_Mul_scale  INSTALL: HyperFlash[  0: 15]@ 424212    LOAD:         L2[  0: 15]@ 119508    EXEC:         L2[  0: 15]@ 119508 , Size:     276
        S28_Mul_shift  INSTALL: HyperFlash[  0: 15]@ 424488    LOAD:         L2[  0: 15]@ 119784    EXEC:         L2[  0: 15]@ 119784 , Size:     276
            S28_Infos  INSTALL: HyperFlash[  0: 15]@ 426012    LOAD:         L2[  0: 15]@ 121308    EXEC:         L2[  0: 15]@ 121308 , Size:       9
 Dscnnconv_ds_5dw_convweights_q  INSTALL: HyperFlash[  0: 15]@ 405168    LOAD:         L2[  0: 15]@ 100464    EXEC:         L2[  0: 15]@ 100464 , Size:    2484
 Dscnnconv_ds_5dw_convdepthwise  INSTALL: HyperFlash[  0: 15]@ 417588    LOAD:         L2[  0: 15]@ 112884    EXEC:         L2[  0: 15]@ 112884 , Size:    1104
        S31_Mul_scale  INSTALL: HyperFlash[  0: 15]@ 424764    LOAD:         L2[  0: 15]@ 120060    EXEC:         L2[  0: 15]@ 120060 , Size:     276
        S31_Mul_shift  INSTALL: HyperFlash[  0: 15]@ 425040    LOAD:         L2[  0: 15]@ 120336    EXEC:         L2[  0: 15]@ 120336 , Size:     276
            S31_Infos  INSTALL: HyperFlash[  0: 15]@ 426024    LOAD:         L2[  0: 15]@ 121320    EXEC:         L2[  0: 15]@ 121320 , Size:       9
 Dscnnconv_ds_5pw_convweights_q  INSTALL: HyperFlash[  0: 15]@ 304704    LOAD:         L2[  0: 15]@      0    EXEC:         L2[  0: 15]@      0 , Size:   76176
 Dscnnconv_ds_5pw_convconv2d_fo  INSTALL: HyperFlash[  0: 15]@ 418692    LOAD:         L2[  0: 15]@ 113988    EXEC:         L2[  0: 15]@ 113988 , Size:    1104
        S34_Mul_scale  INSTALL: HyperFlash[  0: 15]@ 425316    LOAD:         L2[  0: 15]@ 120612    EXEC:         L2[  0: 15]@ 120612 , Size:     276
        S34_Mul_shift  INSTALL: HyperFlash[  0: 15]@ 425592    LOAD:         L2[  0: 15]@ 120888    EXEC:         L2[  0: 15]@ 120888 , Size:     276
            S34_Infos  INSTALL: HyperFlash[  0: 15]@ 426036    LOAD:         L2[  0: 15]@ 121332    EXEC:         L2[  0: 15]@ 121332 , Size:       9
            S35_Infos  INSTALL: HyperFlash[  0: 15]@ 426048    LOAD:         L2[  0: 15]@ 121344    EXEC:         L2[  0: 15]@ 121344 , Size:       9
 Dscnnfc1weights_quantfakequant  INSTALL: HyperFlash[  0: 15]@ 391920    LOAD:         L2[  0: 15]@  87216    EXEC:         L2[  0: 15]@  87216 , Size:    3312
  Dscnnfc1matmul_bias  INSTALL: HyperFlash[  0: 15]@ 425868    LOAD:         L2[  0: 15]@ 121164    EXEC:         L2[  0: 15]@ 121164 , Size:      48
        S38_Mul_scale  INSTALL: HyperFlash[  0: 15]@ 426060    LOAD:         L2[  0: 15]@ 121356    EXEC:         L2[  0: 15]@ 121356 , Size:      12
        S38_Mul_shift  INSTALL: HyperFlash[  0: 15]@ 426072    LOAD:         L2[  0: 15]@ 121368    EXEC:         L2[  0: 15]@ 121368 , Size:      12
            S38_Infos  INSTALL: HyperFlash[  0: 15]@ 426084    LOAD:         L2[  0: 15]@ 121380    EXEC:         L2[  0: 15]@ 121380 , Size:       9
            S39_Infos  INSTALL: HyperFlash[  0: 15]@ 426096    LOAD:         L2[  0: 15]@ 121392    EXEC:         L2[  0: 15]@ 121392 , Size:       9
             Output_1  Externally allocated
            S4_Output     EXEC:         L2[  1:  2]@ 139344 , Size:   69000
            S7_Output     EXEC:         L2[  2:  3]@ 121404 , Size:   17940
           S10_Output     EXEC:         L2[  3:  4]@ 139344 , Size:   17940
           S13_Output     EXEC:         L2[  4:  5]@ 121404 , Size:   17940
           S16_Output     EXEC:         L2[  5:  6]@ 139344 , Size:   17940
           S19_Output     EXEC:         L2[  6:  7]@ 121404 , Size:   17940
           S22_Output     EXEC:         L2[  7:  8]@ 139344 , Size:   17940
           S25_Output     EXEC:         L2[  8:  9]@ 157284 , Size:   17940
           S28_Output     EXEC:         L2[  9: 10]@ 121404 , Size:   17940
           S31_Output     EXEC:         L2[ 10: 11]@ 139344 , Size:   17940
           S34_Output     EXEC:         L2[ 11: 12]@ 121404 , Size:   17940
           S35_Output     EXEC:         L2[ 12: 13]@ 139344 , Size:     276
           S38_Output     EXEC:         L2[ 13: 14]@ 121404 , Size:      12
------------------------------------------------------------------------------------------------------------------------------------------------

------------------------------------------------------------------------------------------------------------------------------------------------
Graph stacked tensors
------------------------------------------------------------------------------------------------------------------------------------------------

Generating Code For User Kernel:           S4_Conv2d_276x1x10x4_Relu
Generating Code For User Kernel:            S7_Conv2d_276x1x3x3_Relu
Generating Code For User Kernel:         S10_Conv2d_276x276x1x1_Relu
Generating Code For User Kernel:           S13_Conv2d_276x1x3x3_Relu
Generating Code For User Kernel:         S16_Conv2d_276x276x1x1_Relu
Generating Code For User Kernel:           S19_Conv2d_276x1x3x3_Relu
Generating Code For User Kernel:         S22_Conv2d_276x276x1x1_Relu
Generating Code For User Kernel:           S25_Conv2d_276x1x3x3_Relu
Generating Code For User Kernel:         S28_Conv2d_276x276x1x1_Relu
Generating Code For User Kernel:           S31_Conv2d_276x1x3x3_Relu
Generating Code For User Kernel:         S34_Conv2d_276x276x1x1_Relu
Generating Code For User Kernel:                S35_AveragePool_13x5
Generating Code For User Kernel:               S38_Linear_12x276x1x1
Generating Code For User Kernel:                         S39_SoftMax
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_1weights_quantfakequ.tensor: 11040 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_1conv2d_fold_bias.tensor: 276 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S4_Mul_scale.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S4_Mul_shift.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S4_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_1dw_convweights_q.tensor: 2484 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_1dw_convdepthwise.tensor: 276 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S7_Mul_scale.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S7_Mul_shift.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S7_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_1pw_convweights_q.tensor: 76176 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_1pw_convconv2d_fo.tensor: 276 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S10_Mul_scale.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S10_Mul_shift.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S10_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_2dw_convweights_q.tensor: 2484 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_2dw_convdepthwise.tensor: 276 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S13_Mul_scale.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S13_Mul_shift.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S13_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_2pw_convweights_q.tensor: 76176 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_2pw_convconv2d_fo.tensor: 276 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S16_Mul_scale.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S16_Mul_shift.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S16_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_3dw_convweights_q.tensor: 2484 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_3dw_convdepthwise.tensor: 276 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S19_Mul_scale.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S19_Mul_shift.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S19_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_3pw_convweights_q.tensor: 76176 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_3pw_convconv2d_fo.tensor: 276 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S22_Mul_scale.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S22_Mul_shift.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S22_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_4dw_convweights_q.tensor: 2484 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_4dw_convdepthwise.tensor: 276 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S25_Mul_scale.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S25_Mul_shift.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S25_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_4pw_convweights_q.tensor: 76176 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_4pw_convconv2d_fo.tensor: 276 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S28_Mul_scale.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S28_Mul_shift.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S28_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_5dw_convweights_q.tensor: 2484 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_5dw_convdepthwise.tensor: 276 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S31_Mul_scale.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S31_Mul_shift.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S31_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_5pw_convweights_q.tensor: 76176 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_5pw_convconv2d_fo.tensor: 276 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S34_Mul_scale.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S34_Mul_shift.tensor: 276 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S34_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S35_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnfc1weights_quantfakequant.tensor: 3312 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnfc1matmul_bias.tensor: 12 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S38_Mul_scale.tensor: 12 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S38_Mul_shift.tensor: 12 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S38_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S39_Infos.tensor: 9 Byte items
Flash image KWS_ds_cnn_l_quant_L3_Flash_Const.dat (size 426108) for device AT_MEM_L3_HFLASH successfuly generated

Shared L1 Memory size (Bytes)             : Given:      48736, Used:      48116
L2 Memory size (Bytes)                    : Given:     350000, Used:     284517
L3 Memory size (Bytes)                    : Given:    6388608, Used:     304704

L3 Memory bandwidth for 1 graph run       :     304704 Bytes
L2 Memory bandwidth for 1 graph run       :    1373906 Bytes
Sum of all Kernels arguments size         :     923956 Bytes
Tiling Bandwith overhead                  :   1.486982 Move/KerArgSize
Sum of baseline bandwidth                 :   50099340 Bytes
Percentage of baseline BW for L2          :    2.74236 %
Percentage of baseline BW for L3          :     0.6082 %
Sum of all Kernels operations             :   28504464 Operations
Total amount of flash coefficients        :     426108 Bytes

Basic kernels library                     : CNN_BasicKernels_SQ8.h
                                          : KWS_ds_cnn_l_quant.h
Output Directory                          : BUILD_MODEL_SQ8_EMUL

The following files have been generated:
	   KWS_ds_cnn_l_quantKernels.c Generated C code for the user kernels and the user kernels groups
	   KWS_ds_cnn_l_quantKernels.h Header file for the generated C code
	KWS_ds_cnn_l_quant_L3_Flash_Const.dat Flash content for Graph constants
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c main_emulation.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL/main_emulation.d -o BUILD_EMUL/main_emulation.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_l_quantKernels.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL/BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_l_quantKernels.d -o BUILD_EMUL/BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_l_quantKernels.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.d -o BUILD_EMUL//home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries/SSD_BasicKernels.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries/SSD_BasicKernels.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries/SSD_BasicKernels.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes/ResizeBasicKernels.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes/ResizeBasicKernels.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes/ResizeBasicKernels.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries/CNN_CopyBasicKernels.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries/CNN_CopyBasicKernels.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries/CNN_CopyBasicKernels.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_AT_Misc.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_AT_Misc.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_AT_Misc.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/math_funcs.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/math_funcs.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/math_funcs.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Activation_SQ8.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Activation_SQ8.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Activation_SQ8.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Activation_HWC_SQ8.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Activation_HWC_SQ8.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Activation_HWC_SQ8.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Bias_Linear_SQ8.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Bias_Linear_SQ8.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Bias_Linear_SQ8.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Conv_SQ8.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Conv_SQ8.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Conv_SQ8.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Pooling_SQ8.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Pooling_SQ8.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Pooling_SQ8.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Conv_DW_SQ8.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Conv_DW_SQ8.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Conv_DW_SQ8.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_MatAlgebra_SQ8.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_MatAlgebra_SQ8.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_MatAlgebra_SQ8.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_SoftMax_SQ8.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_SoftMax_SQ8.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_SoftMax_SQ8.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/RNN_SQ8.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/RNN_SQ8.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/RNN_SQ8.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/wav_io/wavIO.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/gap_sdk/libs/gap_lib/wav_io/wavIO.d -o BUILD_EMUL//home/marco-gwt/GWT/gap_sdk/libs/gap_lib/wav_io/wavIO.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL/MFCCKernels.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL/MFCCKernels.d -o BUILD_EMUL//home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL/MFCCKernels.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/TwiddlesDef.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/TwiddlesDef.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/TwiddlesDef.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/RFFTTwiddlesDef.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/RFFTTwiddlesDef.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/RFFTTwiddlesDef.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/SwapTablesDef.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/SwapTablesDef.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/SwapTablesDef.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/MfccBasicKernels.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/MfccBasicKernels.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/MfccBasicKernels.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/FFT_Library.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/FFT_Library.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/FFT_Library.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/CmplxFunctions.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/CmplxFunctions.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/CmplxFunctions.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/PreProcessing.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/PreProcessing.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/PreProcessing.o
gcc -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -MMD -MP -DLARGE -DWITH_MFCC -g -O3 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -o kws_ds_cnn_emul  BUILD_EMUL/main_emulation.o  BUILD_EMUL/BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_l_quantKernels.o  BUILD_EMUL//home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries/SSD_BasicKernels.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes/ResizeBasicKernels.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries/CNN_CopyBasicKernels.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_AT_Misc.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/math_funcs.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Activation_SQ8.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Activation_HWC_SQ8.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Bias_Linear_SQ8.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Conv_SQ8.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Pooling_SQ8.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Conv_DW_SQ8.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_MatAlgebra_SQ8.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_SoftMax_SQ8.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/RNN_SQ8.o  BUILD_EMUL//home/marco-gwt/GWT/gap_sdk/libs/gap_lib/wav_io/wavIO.o  BUILD_EMUL//home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL/MFCCKernels.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/TwiddlesDef.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/RFFTTwiddlesDef.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/SwapTablesDef.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/MfccBasicKernels.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/FFT_Library.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/CmplxFunctions.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/PreProcessing.o  -lm
WARNING:tensorflow:From utils/test_accuracy_emul.py:95: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

W0608 14:05:09.750986 140676110542656 module_wrapper.py:139] From utils/test_accuracy_emul.py:95: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

WARNING:tensorflow:From utils/test_accuracy_emul.py:95: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

W0608 14:05:09.751533 140676110542656 module_wrapper.py:139] From utils/test_accuracy_emul.py:95: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

WARNING:tensorflow:From utils/test_accuracy_emul.py:96: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.

W0608 14:05:09.751920 140676110542656 module_wrapper.py:139] From utils/test_accuracy_emul.py:96: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.

2021-06-08 14:05:09.753730: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-06-08 14:05:09.766392: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2096060000 Hz
2021-06-08 14:05:09.766994: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563ae5bcc430 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-06-08 14:05:09.767063: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-06-08 14:05:09.770229: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/marco-gwt/GWT/gap_sdk/install/workstation/lib
2021-06-08 14:05:09.770311: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)
2021-06-08 14:05:09.770367: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu): /proc/driver/nvidia/version does not exist
WARNING:tensorflow:From /home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/utils/input_data.py:347: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

W0608 14:07:50.166987 140676110542656 module_wrapper.py:139] From /home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/utils/input_data.py:347: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/utils/input_data.py:348: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0608 14:07:50.172054 140676110542656 module_wrapper.py:139] From /home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/utils/input_data.py:348: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From utils/test_accuracy_emul.py:111: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.

W0608 14:07:50.812488 140676110542656 module_wrapper.py:139] From utils/test_accuracy_emul.py:111: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.

INFO:tensorflow:Validation set size:4445
I0608 14:07:50.813130 140676110542656 test_accuracy_emul.py:111] Validation set size:4445
INFO:tensorflow:Test set size:4890
I0608 14:13:23.403684 140676110542656 test_accuracy_emul.py:157] Test set size:4890
rm: cannot remove 'test.pgm': No such file or directory
make[1]: Leaving directory '/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting'
{'desired_samples': 16000, 'window_size_samples': 640, 'window_stride_samples': 320, 'spectrogram_length': 49, 'dct_coefficient_count': 10, 'fingerprint_width': 10, 'fingerprint_size': 490, 'label_count': 12, 'sample_rate': 16000, 'preprocess': 'mfcc', 'average_window_width': -1, 'use_power': True}
Pred/Tot:	  97/ 100	Accuracy:	97.00%
Pred/Tot:	 194/ 200	Accuracy:	97.00%
Pred/Tot:	 286/ 300	Accuracy:	95.33%
Pred/Tot:	 380/ 400	Accuracy:	95.00%
Pred/Tot:	 477/ 500	Accuracy:	95.40%
Pred/Tot:	 571/ 600	Accuracy:	95.17%
Pred/Tot:	 669/ 700	Accuracy:	95.57%
Pred/Tot:	 760/ 800	Accuracy:	95.00%
Pred/Tot:	 853/ 900	Accuracy:	94.78%
Pred/Tot:	 947/1000	Accuracy:	94.70%
Pred/Tot:	1039/1100	Accuracy:	94.45%
Pred/Tot:	1136/1200	Accuracy:	94.67%
Pred/Tot:	1228/1300	Accuracy:	94.46%
Pred/Tot:	1322/1400	Accuracy:	94.43%
Pred/Tot:	1416/1500	Accuracy:	94.40%
Pred/Tot:	1513/1600	Accuracy:	94.56%
Pred/Tot:	1609/1700	Accuracy:	94.65%
Pred/Tot:	1706/1800	Accuracy:	94.78%
Pred/Tot:	1801/1900	Accuracy:	94.79%
Pred/Tot:	1898/2000	Accuracy:	94.90%
Pred/Tot:	1995/2100	Accuracy:	95.00%
Pred/Tot:	2091/2200	Accuracy:	95.05%
Pred/Tot:	2186/2300	Accuracy:	95.04%
Pred/Tot:	2278/2400	Accuracy:	94.92%
Pred/Tot:	2368/2500	Accuracy:	94.72%
Pred/Tot:	2460/2600	Accuracy:	94.62%
Pred/Tot:	2557/2700	Accuracy:	94.70%
Pred/Tot:	2653/2800	Accuracy:	94.75%
Pred/Tot:	2748/2900	Accuracy:	94.76%
Pred/Tot:	2843/3000	Accuracy:	94.77%
Pred/Tot:	2940/3100	Accuracy:	94.84%
Pred/Tot:	3035/3200	Accuracy:	94.84%
Pred/Tot:	3131/3300	Accuracy:	94.88%
Pred/Tot:	3222/3400	Accuracy:	94.76%
Pred/Tot:	3318/3500	Accuracy:	94.80%
Pred/Tot:	3414/3600	Accuracy:	94.83%
Pred/Tot:	3507/3700	Accuracy:	94.78%
Pred/Tot:	3604/3800	Accuracy:	94.84%
Pred/Tot:	3694/3900	Accuracy:	94.72%
Pred/Tot:	3788/4000	Accuracy:	94.70%
Pred/Tot:	3884/4100	Accuracy:	94.73%
Pred/Tot:	3979/4200	Accuracy:	94.74%
Pred/Tot:	4072/4300	Accuracy:	94.70%
Pred/Tot:	4169/4400	Accuracy:	94.75%

FINAL VALIDATION ACCURACY:
Pred/Tot:	4211/4444	Accuracy:	94.76%

Confusion matrix:
[[371   0   0   0   0   0   0   0   0   0   0   0]
 [  0 337   0   1   4   7   3   3   7   2   3   4]
 [  0   2 384   3   0   1   3   2   0   0   0   2]
 [  1  11   0 381   0   1   3   0   0   1   1   7]
 [  0   2   1   0 327   0   2   0   2  10   5   1]
 [  0   0   0   9   0 360   1   0   0   1   3   3]
 [  0   3   5   0   0   0 341   2   0   0   0   1]
 [  0   6   0   1   0   0   2 352   0   1   1   0]
 [  0   8   0   2   3   0   0   1 341   8   0   0]
 [  0   2   1   1  17   0   1   1   6 340   3   1]
 [  1   3   0   0   7   1   1   0   0   3 334   0]
 [  0  11   0   9   0   4   0   0   1   3   1 343]]
Pred/Tot:	  91/ 100	Accuracy:	91.00%
Pred/Tot:	 187/ 200	Accuracy:	93.50%
Pred/Tot:	 282/ 300	Accuracy:	94.00%
Pred/Tot:	 377/ 400	Accuracy:	94.25%
Pred/Tot:	 469/ 500	Accuracy:	93.80%
Pred/Tot:	 563/ 600	Accuracy:	93.83%
Pred/Tot:	 659/ 700	Accuracy:	94.14%
Pred/Tot:	 755/ 800	Accuracy:	94.38%
Pred/Tot:	 852/ 900	Accuracy:	94.67%
Pred/Tot:	 947/1000	Accuracy:	94.70%
Pred/Tot:	1038/1100	Accuracy:	94.36%
Pred/Tot:	1136/1200	Accuracy:	94.67%
Pred/Tot:	1229/1300	Accuracy:	94.54%
Pred/Tot:	1324/1400	Accuracy:	94.57%
Pred/Tot:	1417/1500	Accuracy:	94.47%
Pred/Tot:	1514/1600	Accuracy:	94.62%
Pred/Tot:	1607/1700	Accuracy:	94.53%
Pred/Tot:	1705/1800	Accuracy:	94.72%
Pred/Tot:	1795/1900	Accuracy:	94.47%
Pred/Tot:	1890/2000	Accuracy:	94.50%
Pred/Tot:	1985/2100	Accuracy:	94.52%
Pred/Tot:	2081/2200	Accuracy:	94.59%
Pred/Tot:	2175/2300	Accuracy:	94.57%
Pred/Tot:	2272/2400	Accuracy:	94.67%
Pred/Tot:	2367/2500	Accuracy:	94.68%
Pred/Tot:	2462/2600	Accuracy:	94.69%
Pred/Tot:	2557/2700	Accuracy:	94.70%
Pred/Tot:	2653/2800	Accuracy:	94.75%
Pred/Tot:	2749/2900	Accuracy:	94.79%
Pred/Tot:	2843/3000	Accuracy:	94.77%
Pred/Tot:	2937/3100	Accuracy:	94.74%
Pred/Tot:	3034/3200	Accuracy:	94.81%
Pred/Tot:	3133/3300	Accuracy:	94.94%
Pred/Tot:	3225/3400	Accuracy:	94.85%
Pred/Tot:	3319/3500	Accuracy:	94.83%
Pred/Tot:	3416/3600	Accuracy:	94.89%
Pred/Tot:	3508/3700	Accuracy:	94.81%
Pred/Tot:	3604/3800	Accuracy:	94.84%
Pred/Tot:	3701/3900	Accuracy:	94.90%
Pred/Tot:	3796/4000	Accuracy:	94.90%
Pred/Tot:	3888/4100	Accuracy:	94.83%
Pred/Tot:	3985/4200	Accuracy:	94.88%
Pred/Tot:	4080/4300	Accuracy:	94.88%
Pred/Tot:	4174/4400	Accuracy:	94.86%
Pred/Tot:	4264/4500	Accuracy:	94.76%
Pred/Tot:	4357/4600	Accuracy:	94.72%
Pred/Tot:	4453/4700	Accuracy:	94.74%
Pred/Tot:	4547/4800	Accuracy:	94.73%

FINAL TESTING ACCURACY:
Pred/Tot:	4633/4889	Accuracy:	94.76%

Confusion matrix:
[[408   0   0   0   0   0   0   0   0   0   0   0]
 [  0 374   0   3   1   9   2   4   7   1   4   3]
 [  0   5 409   4   0   0   1   0   0   0   0   0]
 [  0   3   1 393   0   3   1   0   0   1   0   3]
 [  0   5   0   0 398   3   0   0   3   9   5   2]
 [  0   7   1  10   0 380   2   1   0   0   2   3]
 [  0   8   3   1   2   0 396   2   0   0   0   0]
 [  0   9   0   0   0   0   4 380   1   0   2   0]
 [  0   8   0   0   1   3   1   0 368  11   1   3]
 [  0   4   0   1  20   0   1   0   4 367   1   4]
 [  0   2   0   0   2   3   0   2   0   1 396   5]
 [  0  10   1  21   0   3   0   1   0   1   1 364]]
