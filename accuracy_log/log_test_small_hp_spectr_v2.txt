script model/nntool_script
GEN ... /home/marco-gwt/GWT/AutotilerV2/CNN_Generators/CNN_Generator_Util.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators/CNN_Copy_Generators.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators/SSD_Generators.c /home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes/ResizeGenerator.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators_SQ8/CNN_Generators_SQ8.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators_SQ8/RNN_Generators_SQ8.c
python3 utils/test_accuracy_emul.py --tflite_model model/KWS_ds_cnn_s_quant.tflite --dct_coefficient_count 10 --window_size_ms 40 --window_stride_ms 20 --test_with_wav 1 --use_power_spectrogram 0
make -f emul.mk clean_model clean all DUMP_TENSORS=0 SMALL=1 MEDIUM=0 LARGE=0 WITH_MFCC=1 USE_POWER=0
make[1]: Entering directory '/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting'
script model/nntool_script
GEN ... /home/marco-gwt/GWT/AutotilerV2/CNN_Generators/CNN_Generator_Util.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators/CNN_Copy_Generators.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators/SSD_Generators.c /home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes/ResizeGenerator.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators_SQ8/CNN_Generators_SQ8.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators_SQ8/RNN_Generators_SQ8.c
rm -f -rf BUILD_MODEL_SQ8_EMUL
rm -f -r BUILD_EMUL
rm -f kws_ds_cnn_emul
mkdir BUILD_MODEL_SQ8_EMUL
cp model/KWS_ds_cnn_s_quant.tflite BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_s_quant.tflite
echo "GENERATING NNTOOL STATE FILE"
GENERATING NNTOOL STATE FILE
nntool -s model/nntool_script BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_s_quant.tflite -q
settings - set log level to INFO
log_level - was: 'INFO'
now: 'INFO'
open - opening graph file BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_s_quant.tflite load_quantization = True
tflite - Importing TFLITE model version 3
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
unified_quantizer - forwards SOFTMAX_0_11 in: -12.19<(i8-0.00)*0.09519558<12.09 out: None stop [] fusion False
unified_quantizer - handler SoftmaxTanHMult selected for SoftMaxParameters(SOFTMAX_0_11)
unified_quantizer - forwards in edge 0 does not match was -12.19<(i8-0.00)*0.09519558<12.09 need -16.00<(i8-0.00)*0.12500000<15.88 forced
unified_quantizer - backwards FULLY_CONNECTED_0_10 in: -11.38<(i8-0.00)*0.08886796<11.29,chan<(i8-0.00)*chan<chan,chan<(i32-0.00)*chan<chan out: -16.00<(i8-0.00)*0.12500000<15.88 forced stop SOFTMAX_0_11 fusion False
unified_quantizer - handler FilterMult selected for FcParameters(FULLY_CONNECTED_0_10)
filter_mult - selecting SQ8 software kernel filter quantizer
filter_mult - node FULLY_CONNECTED_0_10 output forced to range [-16.]/[15.875] - actual range [-12.185035]/[12.089829] symmetric
unified_quantization_handler - indicating change of FULLY_CONNECTED_0_10 input from c, out_cin_c, out_c to chw, out_cin_chw, out_c order - rerun adjust command
unified_quantization_handler - indicating change of FULLY_CONNECTED_0_10 output from c to chw order - rerun adjust command
unified_quantizer - backwards finished in_edges FULLY_CONNECTED_0_10
unified_quantizer - backwards out edge 0 does not match was -12.19<(i8-0.00)*0.09519558<12.09 need -16.00<(i8-0.00)*0.12500000<15.88 forced
unified_quantizer - ---- STOPPED AT SOFTMAX_0_11
unified_quantizer - backwards finished out_edges FULLY_CONNECTED_0_10
unified_quantizer - forwards finished in edges SOFTMAX_0_11
unified_quantizer - forwards at SOFTMAX_0_11 on out edge 0
unified_quantizer - forwards output_1 in: -1.00<(i16-0.00)*0.00003052<1.00 out: None stop [] fusion False
unified_quantizer - handler OutputMult selected for OutputParameters(output_1)
unified_quantizer - forwards finished in edges output_1
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
debug - was: False
now: True
adjust_order - adding transposes to correct tensor order for AT kernels
set_aliases - looking for aliased edges
eliminate_transposes - eliminating unnecessary transposes
eliminate_transposes - search for transposes
eliminate_transposes - ++ Starting up from DEPTHWISE_CONV_2D_0_0[1]
eliminate_transposes - looking up at DSCNNconv_1weights_quantFakeQu[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_1weights_quantFakeQu - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_0[1]
eliminate_transposes - ++ Starting down from DEPTHWISE_CONV_2D_0_0[0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_0_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_1[0] transpose [1, 2, 0]
eliminate_transposes - accepted DEPTHWISE_CONV_2D_0_1 - transpose in (1)
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_0[0]
eliminate_transposes - ++ Starting up from DEPTHWISE_CONV_2D_0_1[1]
eliminate_transposes - looking up at DSCNNconv_ds_1dw_convweights_q[0] transpose [3, 0, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_1dw_convweights_q - constant input - transpose constant [3, 0, 1, 2]
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_1[1]
eliminate_transposes - ++ Starting down from DEPTHWISE_CONV_2D_0_1[0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_1_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at CONV_2D_0_2[0] transpose [1, 2, 0]
eliminate_transposes - accepted CONV_2D_0_2 - transpose in (1)
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_1[0]
eliminate_transposes - ++ Starting up from CONV_2D_0_2[1]
eliminate_transposes - looking up at DSCNNconv_ds_1pw_convweights_q[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_1pw_convweights_q - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for CONV_2D_0_2[1]
eliminate_transposes - ++ Starting down from CONV_2D_0_2[0]
eliminate_transposes - looking down at CONV_2D_0_2_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_3[0] transpose [1, 2, 0]
eliminate_transposes - accepted DEPTHWISE_CONV_2D_0_3 - transpose in (1)
eliminate_transposes - ++ Found results for CONV_2D_0_2[0]
eliminate_transposes - ++ Starting up from DEPTHWISE_CONV_2D_0_3[1]
eliminate_transposes - looking up at DSCNNconv_ds_2dw_convweights_q[0] transpose [3, 0, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_2dw_convweights_q - constant input - transpose constant [3, 0, 1, 2]
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_3[1]
eliminate_transposes - ++ Starting down from DEPTHWISE_CONV_2D_0_3[0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_3_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at CONV_2D_0_4[0] transpose [1, 2, 0]
eliminate_transposes - accepted CONV_2D_0_4 - transpose in (1)
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_3[0]
eliminate_transposes - ++ Starting up from CONV_2D_0_4[1]
eliminate_transposes - looking up at DSCNNconv_ds_2pw_convweights_q[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_2pw_convweights_q - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for CONV_2D_0_4[1]
eliminate_transposes - ++ Starting down from CONV_2D_0_4[0]
eliminate_transposes - looking down at CONV_2D_0_4_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_5[0] transpose [1, 2, 0]
eliminate_transposes - accepted DEPTHWISE_CONV_2D_0_5 - transpose in (1)
eliminate_transposes - ++ Found results for CONV_2D_0_4[0]
eliminate_transposes - ++ Starting up from DEPTHWISE_CONV_2D_0_5[1]
eliminate_transposes - looking up at DSCNNconv_ds_3dw_convweights_q[0] transpose [3, 0, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_3dw_convweights_q - constant input - transpose constant [3, 0, 1, 2]
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_5[1]
eliminate_transposes - ++ Starting down from DEPTHWISE_CONV_2D_0_5[0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_5_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at CONV_2D_0_6[0] transpose [1, 2, 0]
eliminate_transposes - accepted CONV_2D_0_6 - transpose in (1)
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_5[0]
eliminate_transposes - ++ Starting up from CONV_2D_0_6[1]
eliminate_transposes - looking up at DSCNNconv_ds_3pw_convweights_q[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_3pw_convweights_q - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for CONV_2D_0_6[1]
eliminate_transposes - ++ Starting down from CONV_2D_0_6[0]
eliminate_transposes - looking down at CONV_2D_0_6_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_7[0] transpose [1, 2, 0]
eliminate_transposes - accepted DEPTHWISE_CONV_2D_0_7 - transpose in (1)
eliminate_transposes - ++ Found results for CONV_2D_0_6[0]
eliminate_transposes - ++ Starting up from DEPTHWISE_CONV_2D_0_7[1]
eliminate_transposes - looking up at DSCNNconv_ds_4dw_convweights_q[0] transpose [3, 0, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_4dw_convweights_q - constant input - transpose constant [3, 0, 1, 2]
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_7[1]
eliminate_transposes - ++ Starting down from DEPTHWISE_CONV_2D_0_7[0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_7_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at CONV_2D_0_8[0] transpose [1, 2, 0]
eliminate_transposes - accepted CONV_2D_0_8 - transpose in (1)
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_7[0]
eliminate_transposes - ++ Starting up from CONV_2D_0_8[1]
eliminate_transposes - looking up at DSCNNconv_ds_4pw_convweights_q[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_4pw_convweights_q - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for CONV_2D_0_8[1]
eliminate_transposes - ++ Starting down from CONV_2D_0_8[0]
eliminate_transposes - looking down at CONV_2D_0_8_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at AVERAGE_POOL_2D_0_9[0] transpose [1, 2, 0]
eliminate_transposes - accepted AVERAGE_POOL_2D_0_9 - transpose in (1)
eliminate_transposes - ++ Found results for CONV_2D_0_8[0]
eliminate_transposes - ++ Starting down from AVERAGE_POOL_2D_0_9[0]
eliminate_transposes - looking down at FULLY_CONNECTED_0_10[0] transpose [1, 2, 0]
eliminate_transposes - accepted FULLY_CONNECTED_0_10 - linear layer reorder input
eliminate_transposes - ++ Found results for AVERAGE_POOL_2D_0_9[0]
eliminate_transposes - eliminate transposes
eliminate_transposes_actions - Start Action (up): DEPTHWISE_CONV_2D_0_0
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_0 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_0 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_1weights_quantFakeQu reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_1weights_quantFakeQu
eliminate_transposes_actions - Start Action (down): DEPTHWISE_CONV_2D_0_0
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_0 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_0 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [3, 0, 1, 2], None]
eliminate_transposes_actions - End Action (down): DEPTHWISE_CONV_2D_0_1
eliminate_transposes_actions - Start Action (up): DEPTHWISE_CONV_2D_0_1
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_1dw_convweights_q reorder constant input to (3, 0, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_1dw_convweights_q
eliminate_transposes_actions - Start Action (down): DEPTHWISE_CONV_2D_0_1
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_2 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_2 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [0, 3, 1, 2], None]
eliminate_transposes_actions - End Action (down): CONV_2D_0_2
eliminate_transposes_actions - Start Action (up): CONV_2D_0_2
eliminate_transposes_actions - CONV_2D_0_2 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_2 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_1pw_convweights_q reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_1pw_convweights_q
eliminate_transposes_actions - Start Action (down): CONV_2D_0_2
eliminate_transposes_actions - CONV_2D_0_2 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_2 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [3, 0, 1, 2], None]
eliminate_transposes_actions - End Action (down): DEPTHWISE_CONV_2D_0_3
eliminate_transposes_actions - Start Action (up): DEPTHWISE_CONV_2D_0_3
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_2dw_convweights_q reorder constant input to (3, 0, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_2dw_convweights_q
eliminate_transposes_actions - Start Action (down): DEPTHWISE_CONV_2D_0_3
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_4 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_4 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [0, 3, 1, 2], None]
eliminate_transposes_actions - End Action (down): CONV_2D_0_4
eliminate_transposes_actions - Start Action (up): CONV_2D_0_4
eliminate_transposes_actions - CONV_2D_0_4 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_4 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_2pw_convweights_q reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_2pw_convweights_q
eliminate_transposes_actions - Start Action (down): CONV_2D_0_4
eliminate_transposes_actions - CONV_2D_0_4 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_4 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [3, 0, 1, 2], None]
eliminate_transposes_actions - End Action (down): DEPTHWISE_CONV_2D_0_5
eliminate_transposes_actions - Start Action (up): DEPTHWISE_CONV_2D_0_5
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_3dw_convweights_q reorder constant input to (3, 0, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_3dw_convweights_q
eliminate_transposes_actions - Start Action (down): DEPTHWISE_CONV_2D_0_5
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_6 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_6 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [0, 3, 1, 2], None]
eliminate_transposes_actions - End Action (down): CONV_2D_0_6
eliminate_transposes_actions - Start Action (up): CONV_2D_0_6
eliminate_transposes_actions - CONV_2D_0_6 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_6 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_3pw_convweights_q reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_3pw_convweights_q
eliminate_transposes_actions - Start Action (down): CONV_2D_0_6
eliminate_transposes_actions - CONV_2D_0_6 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_6 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [3, 0, 1, 2], None]
eliminate_transposes_actions - End Action (down): DEPTHWISE_CONV_2D_0_7
eliminate_transposes_actions - Start Action (up): DEPTHWISE_CONV_2D_0_7
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_4dw_convweights_q reorder constant input to (3, 0, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_4dw_convweights_q
eliminate_transposes_actions - Start Action (down): DEPTHWISE_CONV_2D_0_7
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_8 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_8 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [0, 3, 1, 2], None]
eliminate_transposes_actions - End Action (down): CONV_2D_0_8
eliminate_transposes_actions - Start Action (up): CONV_2D_0_8
eliminate_transposes_actions - CONV_2D_0_8 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_8 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_4pw_convweights_q reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_4pw_convweights_q
eliminate_transposes_actions - Start Action (down): CONV_2D_0_8
eliminate_transposes_actions - CONV_2D_0_8 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_8 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - AVERAGE_POOL_2D_0_9 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - AVERAGE_POOL_2D_0_9 delete transpose in[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - End Action (down): AVERAGE_POOL_2D_0_9
eliminate_transposes_actions - Start Action (down): AVERAGE_POOL_2D_0_9
eliminate_transposes_actions - AVERAGE_POOL_2D_0_9 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - AVERAGE_POOL_2D_0_9 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - reorder linear layer FULLY_CONNECTED_0_10 in with shape 1x1x64 transposed (2, 0, 1)
eliminate_transposes_actions - End Action (down): FULLY_CONNECTED_0_10
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
eliminate_transposes - search for transposes
eliminate_transposes - no transposes to eliminate found
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
eliminate_transposes - no further transpose sequences found
set_aliases - looking for aliased edges
nngraph - adjusted order
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
duplicate_operations - match_duplicate_operations does not handle quantized graphs
match_gap_conv - fusing nodes DEPTHWISE_CONV_2D_0_0,DEPTHWISE_CONV_2D_0_0_activation
match_gap_conv - fusing nodes DEPTHWISE_CONV_2D_0_1,DEPTHWISE_CONV_2D_0_1_activation
match_gap_conv - fusing nodes CONV_2D_0_2,CONV_2D_0_2_activation
match_gap_conv - fusing nodes DEPTHWISE_CONV_2D_0_3,DEPTHWISE_CONV_2D_0_3_activation
match_gap_conv - fusing nodes CONV_2D_0_4,CONV_2D_0_4_activation
match_gap_conv - fusing nodes DEPTHWISE_CONV_2D_0_5,DEPTHWISE_CONV_2D_0_5_activation
match_gap_conv - fusing nodes CONV_2D_0_6,CONV_2D_0_6_activation
match_gap_conv - fusing nodes DEPTHWISE_CONV_2D_0_7,DEPTHWISE_CONV_2D_0_7_activation
match_gap_conv - fusing nodes CONV_2D_0_8,CONV_2D_0_8_activation
matcher - ++ fusion fuse_gap_convs modified graph
set_aliases - looking for aliased edges
set_aliases - looking for aliased edges
duplicate_operations - match_duplicate_operations does not handle quantized graphs
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
graph_produce_node_names - was: False
now: True
graph_produce_operinfos - was: False
now: True
graph_monitor_cycles - was: False
now: True
graph_const_exec_from_flash - was: False
now: False
save_state - saved state to /home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_s_quant.json
echo "GENERATING AUTOTILER MODEL"
GENERATING AUTOTILER MODEL
nntool -g -M BUILD_MODEL_SQ8_EMUL -m KWS_ds_cnn_s_quantModel.c -T BUILD_MODEL_SQ8_EMUL/tensors -H KWS_ds_cnn_s_quantInfo.h  BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_s_quant.json
settings - set log level to INFO
log_level - was: 'INFO'
now: 'INFO'
open - opening graph file BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_s_quant.tflite load_quantization = True
tflite - Importing TFLITE model version 3
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
unified_quantizer - forwards SOFTMAX_0_11 in: -12.19<(i8-0.00)*0.09519558<12.09 out: None stop [] fusion False
unified_quantizer - handler SoftmaxTanHMult selected for SoftMaxParameters(SOFTMAX_0_11)
unified_quantizer - forwards in edge 0 does not match was -12.19<(i8-0.00)*0.09519558<12.09 need -16.00<(i8-0.00)*0.12500000<15.88 forced
unified_quantizer - backwards FULLY_CONNECTED_0_10 in: -11.38<(i8-0.00)*0.08886796<11.29,chan<(i8-0.00)*chan<chan,chan<(i32-0.00)*chan<chan out: -16.00<(i8-0.00)*0.12500000<15.88 forced stop SOFTMAX_0_11 fusion False
unified_quantizer - handler FilterMult selected for FcParameters(FULLY_CONNECTED_0_10)
filter_mult - selecting SQ8 software kernel filter quantizer
filter_mult - node FULLY_CONNECTED_0_10 output forced to range [-16.]/[15.875] - actual range [-12.185035]/[12.089829] symmetric
unified_quantization_handler - indicating change of FULLY_CONNECTED_0_10 input from c, out_cin_c, out_c to chw, out_cin_chw, out_c order - rerun adjust command
unified_quantization_handler - indicating change of FULLY_CONNECTED_0_10 output from c to chw order - rerun adjust command
unified_quantizer - backwards finished in_edges FULLY_CONNECTED_0_10
unified_quantizer - backwards out edge 0 does not match was -12.19<(i8-0.00)*0.09519558<12.09 need -16.00<(i8-0.00)*0.12500000<15.88 forced
unified_quantizer - ---- STOPPED AT SOFTMAX_0_11
unified_quantizer - backwards finished out_edges FULLY_CONNECTED_0_10
unified_quantizer - forwards finished in edges SOFTMAX_0_11
unified_quantizer - forwards at SOFTMAX_0_11 on out edge 0
unified_quantizer - forwards output_1 in: -1.00<(i16-0.00)*0.00003052<1.00 out: None stop [] fusion False
unified_quantizer - handler OutputMult selected for OutputParameters(output_1)
unified_quantizer - forwards finished in edges output_1
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
debug - was: False
now: True
adjust_order - adding transposes to correct tensor order for AT kernels
set_aliases - looking for aliased edges
eliminate_transposes - eliminating unnecessary transposes
eliminate_transposes - search for transposes
eliminate_transposes - ++ Starting up from DEPTHWISE_CONV_2D_0_0[1]
eliminate_transposes - looking up at DSCNNconv_1weights_quantFakeQu[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_1weights_quantFakeQu - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_0[1]
eliminate_transposes - ++ Starting down from DEPTHWISE_CONV_2D_0_0[0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_0_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_1[0] transpose [1, 2, 0]
eliminate_transposes - accepted DEPTHWISE_CONV_2D_0_1 - transpose in (1)
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_0[0]
eliminate_transposes - ++ Starting up from DEPTHWISE_CONV_2D_0_1[1]
eliminate_transposes - looking up at DSCNNconv_ds_1dw_convweights_q[0] transpose [3, 0, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_1dw_convweights_q - constant input - transpose constant [3, 0, 1, 2]
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_1[1]
eliminate_transposes - ++ Starting down from DEPTHWISE_CONV_2D_0_1[0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_1_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at CONV_2D_0_2[0] transpose [1, 2, 0]
eliminate_transposes - accepted CONV_2D_0_2 - transpose in (1)
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_1[0]
eliminate_transposes - ++ Starting up from CONV_2D_0_2[1]
eliminate_transposes - looking up at DSCNNconv_ds_1pw_convweights_q[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_1pw_convweights_q - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for CONV_2D_0_2[1]
eliminate_transposes - ++ Starting down from CONV_2D_0_2[0]
eliminate_transposes - looking down at CONV_2D_0_2_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_3[0] transpose [1, 2, 0]
eliminate_transposes - accepted DEPTHWISE_CONV_2D_0_3 - transpose in (1)
eliminate_transposes - ++ Found results for CONV_2D_0_2[0]
eliminate_transposes - ++ Starting up from DEPTHWISE_CONV_2D_0_3[1]
eliminate_transposes - looking up at DSCNNconv_ds_2dw_convweights_q[0] transpose [3, 0, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_2dw_convweights_q - constant input - transpose constant [3, 0, 1, 2]
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_3[1]
eliminate_transposes - ++ Starting down from DEPTHWISE_CONV_2D_0_3[0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_3_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at CONV_2D_0_4[0] transpose [1, 2, 0]
eliminate_transposes - accepted CONV_2D_0_4 - transpose in (1)
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_3[0]
eliminate_transposes - ++ Starting up from CONV_2D_0_4[1]
eliminate_transposes - looking up at DSCNNconv_ds_2pw_convweights_q[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_2pw_convweights_q - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for CONV_2D_0_4[1]
eliminate_transposes - ++ Starting down from CONV_2D_0_4[0]
eliminate_transposes - looking down at CONV_2D_0_4_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_5[0] transpose [1, 2, 0]
eliminate_transposes - accepted DEPTHWISE_CONV_2D_0_5 - transpose in (1)
eliminate_transposes - ++ Found results for CONV_2D_0_4[0]
eliminate_transposes - ++ Starting up from DEPTHWISE_CONV_2D_0_5[1]
eliminate_transposes - looking up at DSCNNconv_ds_3dw_convweights_q[0] transpose [3, 0, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_3dw_convweights_q - constant input - transpose constant [3, 0, 1, 2]
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_5[1]
eliminate_transposes - ++ Starting down from DEPTHWISE_CONV_2D_0_5[0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_5_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at CONV_2D_0_6[0] transpose [1, 2, 0]
eliminate_transposes - accepted CONV_2D_0_6 - transpose in (1)
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_5[0]
eliminate_transposes - ++ Starting up from CONV_2D_0_6[1]
eliminate_transposes - looking up at DSCNNconv_ds_3pw_convweights_q[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_3pw_convweights_q - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for CONV_2D_0_6[1]
eliminate_transposes - ++ Starting down from CONV_2D_0_6[0]
eliminate_transposes - looking down at CONV_2D_0_6_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_7[0] transpose [1, 2, 0]
eliminate_transposes - accepted DEPTHWISE_CONV_2D_0_7 - transpose in (1)
eliminate_transposes - ++ Found results for CONV_2D_0_6[0]
eliminate_transposes - ++ Starting up from DEPTHWISE_CONV_2D_0_7[1]
eliminate_transposes - looking up at DSCNNconv_ds_4dw_convweights_q[0] transpose [3, 0, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_4dw_convweights_q - constant input - transpose constant [3, 0, 1, 2]
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_7[1]
eliminate_transposes - ++ Starting down from DEPTHWISE_CONV_2D_0_7[0]
eliminate_transposes - looking down at DEPTHWISE_CONV_2D_0_7_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at CONV_2D_0_8[0] transpose [1, 2, 0]
eliminate_transposes - accepted CONV_2D_0_8 - transpose in (1)
eliminate_transposes - ++ Found results for DEPTHWISE_CONV_2D_0_7[0]
eliminate_transposes - ++ Starting up from CONV_2D_0_8[1]
eliminate_transposes - looking up at DSCNNconv_ds_4pw_convweights_q[0] transpose [0, 3, 1, 2]
eliminate_transposes - accepted DSCNNconv_ds_4pw_convweights_q - constant input - transpose constant [0, 3, 1, 2]
eliminate_transposes - ++ Found results for CONV_2D_0_8[1]
eliminate_transposes - ++ Starting down from CONV_2D_0_8[0]
eliminate_transposes - looking down at CONV_2D_0_8_activation[0] transpose [1, 2, 0]
eliminate_transposes - looking down at AVERAGE_POOL_2D_0_9[0] transpose [1, 2, 0]
eliminate_transposes - accepted AVERAGE_POOL_2D_0_9 - transpose in (1)
eliminate_transposes - ++ Found results for CONV_2D_0_8[0]
eliminate_transposes - ++ Starting down from AVERAGE_POOL_2D_0_9[0]
eliminate_transposes - looking down at FULLY_CONNECTED_0_10[0] transpose [1, 2, 0]
eliminate_transposes - accepted FULLY_CONNECTED_0_10 - linear layer reorder input
eliminate_transposes - ++ Found results for AVERAGE_POOL_2D_0_9[0]
eliminate_transposes - eliminate transposes
eliminate_transposes_actions - Start Action (up): DEPTHWISE_CONV_2D_0_0
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_0 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_0 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_1weights_quantFakeQu reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_1weights_quantFakeQu
eliminate_transposes_actions - Start Action (down): DEPTHWISE_CONV_2D_0_0
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_0 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_0 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [3, 0, 1, 2], None]
eliminate_transposes_actions - End Action (down): DEPTHWISE_CONV_2D_0_1
eliminate_transposes_actions - Start Action (up): DEPTHWISE_CONV_2D_0_1
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_1dw_convweights_q reorder constant input to (3, 0, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_1dw_convweights_q
eliminate_transposes_actions - Start Action (down): DEPTHWISE_CONV_2D_0_1
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_1 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_2 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_2 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [0, 3, 1, 2], None]
eliminate_transposes_actions - End Action (down): CONV_2D_0_2
eliminate_transposes_actions - Start Action (up): CONV_2D_0_2
eliminate_transposes_actions - CONV_2D_0_2 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_2 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_1pw_convweights_q reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_1pw_convweights_q
eliminate_transposes_actions - Start Action (down): CONV_2D_0_2
eliminate_transposes_actions - CONV_2D_0_2 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_2 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [3, 0, 1, 2], None]
eliminate_transposes_actions - End Action (down): DEPTHWISE_CONV_2D_0_3
eliminate_transposes_actions - Start Action (up): DEPTHWISE_CONV_2D_0_3
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_2dw_convweights_q reorder constant input to (3, 0, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_2dw_convweights_q
eliminate_transposes_actions - Start Action (down): DEPTHWISE_CONV_2D_0_3
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_3 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_4 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_4 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [0, 3, 1, 2], None]
eliminate_transposes_actions - End Action (down): CONV_2D_0_4
eliminate_transposes_actions - Start Action (up): CONV_2D_0_4
eliminate_transposes_actions - CONV_2D_0_4 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_4 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_2pw_convweights_q reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_2pw_convweights_q
eliminate_transposes_actions - Start Action (down): CONV_2D_0_4
eliminate_transposes_actions - CONV_2D_0_4 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_4 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [3, 0, 1, 2], None]
eliminate_transposes_actions - End Action (down): DEPTHWISE_CONV_2D_0_5
eliminate_transposes_actions - Start Action (up): DEPTHWISE_CONV_2D_0_5
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_3dw_convweights_q reorder constant input to (3, 0, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_3dw_convweights_q
eliminate_transposes_actions - Start Action (down): DEPTHWISE_CONV_2D_0_5
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_5 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_6 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_6 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [0, 3, 1, 2], None]
eliminate_transposes_actions - End Action (down): CONV_2D_0_6
eliminate_transposes_actions - Start Action (up): CONV_2D_0_6
eliminate_transposes_actions - CONV_2D_0_6 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_6 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_3pw_convweights_q reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_3pw_convweights_q
eliminate_transposes_actions - Start Action (down): CONV_2D_0_6
eliminate_transposes_actions - CONV_2D_0_6 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_6 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [3, 0, 1, 2], None]
eliminate_transposes_actions - End Action (down): DEPTHWISE_CONV_2D_0_7
eliminate_transposes_actions - Start Action (up): DEPTHWISE_CONV_2D_0_7
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_4dw_convweights_q reorder constant input to (3, 0, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_4dw_convweights_q
eliminate_transposes_actions - Start Action (down): DEPTHWISE_CONV_2D_0_7
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - DEPTHWISE_CONV_2D_0_7 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_8 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - CONV_2D_0_8 delete transpose in[0]
eliminate_transposes_actions - transpose is now [None, [0, 3, 1, 2], None]
eliminate_transposes_actions - End Action (down): CONV_2D_0_8
eliminate_transposes_actions - Start Action (up): CONV_2D_0_8
eliminate_transposes_actions - CONV_2D_0_8 delete transpose in[1]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_8 set hint in[1] to ['out_c', 'in_c', 'h', 'w']
eliminate_transposes_actions - DSCNNconv_ds_4pw_convweights_q reorder constant input to (0, 3, 1, 2)
eliminate_transposes_actions - End Action (up): DSCNNconv_ds_4pw_convweights_q
eliminate_transposes_actions - Start Action (down): CONV_2D_0_8
eliminate_transposes_actions - CONV_2D_0_8 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - CONV_2D_0_8 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - AVERAGE_POOL_2D_0_9 set hint in[0] to ['c', 'h', 'w']
eliminate_transposes_actions - AVERAGE_POOL_2D_0_9 delete transpose in[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - End Action (down): AVERAGE_POOL_2D_0_9
eliminate_transposes_actions - Start Action (down): AVERAGE_POOL_2D_0_9
eliminate_transposes_actions - AVERAGE_POOL_2D_0_9 delete transpose out[0]
eliminate_transposes_actions - transpose is now None
eliminate_transposes_actions - AVERAGE_POOL_2D_0_9 set hint out[0] to ['c', 'h', 'w']
eliminate_transposes_actions - reorder linear layer FULLY_CONNECTED_0_10 in with shape 1x1x64 transposed (2, 0, 1)
eliminate_transposes_actions - End Action (down): FULLY_CONNECTED_0_10
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
eliminate_transposes - search for transposes
eliminate_transposes - no transposes to eliminate found
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
eliminate_transposes - no further transpose sequences found
set_aliases - looking for aliased edges
nngraph - adjusted order
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
duplicate_operations - match_duplicate_operations does not handle quantized graphs
match_gap_conv - fusing nodes DEPTHWISE_CONV_2D_0_0,DEPTHWISE_CONV_2D_0_0_activation
match_gap_conv - fusing nodes DEPTHWISE_CONV_2D_0_1,DEPTHWISE_CONV_2D_0_1_activation
match_gap_conv - fusing nodes CONV_2D_0_2,CONV_2D_0_2_activation
match_gap_conv - fusing nodes DEPTHWISE_CONV_2D_0_3,DEPTHWISE_CONV_2D_0_3_activation
match_gap_conv - fusing nodes CONV_2D_0_4,CONV_2D_0_4_activation
match_gap_conv - fusing nodes DEPTHWISE_CONV_2D_0_5,DEPTHWISE_CONV_2D_0_5_activation
match_gap_conv - fusing nodes CONV_2D_0_6,CONV_2D_0_6_activation
match_gap_conv - fusing nodes DEPTHWISE_CONV_2D_0_7,DEPTHWISE_CONV_2D_0_7_activation
match_gap_conv - fusing nodes CONV_2D_0_8,CONV_2D_0_8_activation
matcher - ++ fusion fuse_gap_convs modified graph
set_aliases - looking for aliased edges
set_aliases - looking for aliased edges
duplicate_operations - match_duplicate_operations does not handle quantized graphs
nngraph - update graph dimensions
set_aliases - looking for aliased edges
nngraph - calculate liveness
graph_produce_node_names - was: False
now: True
graph_produce_operinfos - was: False
now: True
graph_monitor_cycles - was: False
now: True
graph_const_exec_from_flash - was: False
now: False
generator - Saving model to BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_s_quantModel.c
code_generator - edge from step 1 DEPTHWISE_CONV_2D_0_0_r_hwc_chw is not used and is replaced with edge from step input_1:0 0 cname: Input_1
generator - Writing constants to BUILD_MODEL_SQ8_EMUL
echo "COMPILING AUTOTILER MODEL"
COMPILING AUTOTILER MODEL
gcc -g -o BUILD_MODEL_SQ8_EMUL/GenTile -I. -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/CNN_Generators -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Generators_SQ8 -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 /home/marco-gwt/GWT/AutotilerV2/CNN_Generators/CNN_Generator_Util.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators/CNN_Copy_Generators.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators/SSD_Generators.c /home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes/ResizeGenerator.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators_SQ8/CNN_Generators_SQ8.c /home/marco-gwt/GWT/AutotilerV2/CNN_Generators_SQ8/RNN_Generators_SQ8.c BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_s_quantModel.c /home/marco-gwt/GWT/AutotilerV2/install/lib/libtile.a -lSDL2 -lSDL2_ttf 
echo "RUNNING AUTOTILER MODEL"
RUNNING AUTOTILER MODEL
BUILD_MODEL_SQ8_EMUL/GenTile -o BUILD_MODEL_SQ8_EMUL -c BUILD_MODEL_SQ8_EMUL -f BUILD_MODEL_SQ8_EMUL --L1 48736 --L2 350000 --L3 6388608
InFeat: 1, OutFeat: 64
Conv => W:  10, Pad:[1,1] PadT:[1,1] => Wc: 5, Filter:[4,10]
     => H:  49, Pad:[4,5] PadT:[4,5] => Hc: 25
Pool => Wc: 5, Pad:[0,0] => Wo: 5, Filter:[1,1]
     => Hc: 25, Pad:[0,0] => Ho: 25
OverlapC: 8
OverlapP: 0
TileCons: 2
UsedIn  : [10 x 49]
UsedC   : [5 x 25]
      SetBiasKerName: KerParSetBiasB32_SQ8
         ConvKerName: KerParConvNxMStrideSxSy_SQ8
  DPReductionKerName: KerParReduct_CC_ReLU_SQ8
Nb Oper : 328000

==== Process Tiling For User Kernel:             S4_Conv2d_64x1x10x4_Relu =======================
S4_Conv2d_64x1x10x4_Relu Partition[0] Size =  39393 (Min:    200, Max:  55185), Fraction:       1.00, Giving:  48736 Bytes out of  48736 Bytes

Reference object:                  Out, Dim=25
	                  In Dim:  58, TileOverlap:  8, Ratio: 2.000000
	                 Out Dim:  25, TileOverlap:  0, Ratio: 1.000000
	             ConvOut Dim:  25, TileOverlap:  0, Ratio: 1.000000

Kernel: S4_Conv2d_64x1x10x4_Relu, Total Raw Memory: 43448 fits into L1 memory 48736. Promoting all kernel arguments to initialized buffers.
Ker: S4_Conv2d_64x1x10x4_Relu, Arg:         In, Size:    492, Base1:      0, Base2:      0
Ker: S4_Conv2d_64x1x10x4_Relu, Arg:       Bias, Size:    256, Base1:    492, Base2:      0
Ker: S4_Conv2d_64x1x10x4_Relu, Arg:      Scale, Size:     64, Base1:    748, Base2:      0
Ker: S4_Conv2d_64x1x10x4_Relu, Arg:     ScaleN, Size:     64, Base1:    812, Base2:      0
Ker: S4_Conv2d_64x1x10x4_Relu, Arg:     Filter, Size:   2560, Base1:    876, Base2:      0
Ker: S4_Conv2d_64x1x10x4_Relu, Arg:        Out, Size:   8000, Base1:   3436, Base2:      0
Ker: S4_Conv2d_64x1x10x4_Relu, Arg:    ConvOut, Size:  32000, Base1:  11436, Base2:      0
Ker: S4_Conv2d_64x1x10x4_Relu, Arg:      Infos, Size:     12, Base1:  43436, Base2:      0
S4_Conv2d_64x1x10x4_Relu For Iter Space: 0 Iteration count:   1, Given L1 Memory:  48736, Used L1 Memory:  43448, Reusable Memory: 5288, Used L2 Memory: 0
=================================================================================================

InFeat: 64, OutFeat: 64
Conv => W:  5, Pad:[1,1] PadT:[1,1] => Wc: 5, Filter:[3,3]
     => H:  25, Pad:[1,1] PadT:[1,1] => Hc: 25
Pool => Wc: 5, Pad:[0,0] => Wo: 5, Filter:[1,1]
     => Hc: 25, Pad:[0,0] => Ho: 25
OverlapC: 2
OverlapP: 0
TileCons: 1
UsedIn  : [5 x 25]
UsedC   : [5 x 25]
         ConvKerName: KerParConvDW3x3Stride1B32_SQ8
  DPReductionKerName: KerParReduct_CC_ReLU_SQ8
Nb Oper : 80000

==== Process Tiling For User Kernel:              S7_Conv2d_64x1x3x3_Relu =======================
S7_Conv2d_64x1x3x3_Relu Partition[0] Size =  36505 (Min:     30, Max:  67337), Fraction:       1.00, Giving:  48736 Bytes out of  48736 Bytes

Reference object:                   In, Dim=25
	                  In Dim:  27, TileOverlap:  2, Ratio: 1.000000
	                 Out Dim:  25, TileOverlap:  0, Ratio: 1.000000
	             ConvOut Dim:  25, TileOverlap:  0, Ratio: 1.000000

S7_Conv2d_64x1x3x3_Relu Full buffering on Arg: Bias, was using 320 Bytes will require 256 Bytes buffer
S7_Conv2d_64x1x3x3_Relu Full buffering on Arg: Scale, was using 80 Bytes will require 64 Bytes buffer
S7_Conv2d_64x1x3x3_Relu Full buffering on Arg: ScaleN, was using 80 Bytes will require 64 Bytes buffer
S7_Conv2d_64x1x3x3_Relu Full buffering on Arg: Filter, was using 720 Bytes will require 576 Bytes buffer
S7_Conv2d_64x1x3x3_Relu, TiledSpace: Tile0 Iteration Count: 1 Parametric Space: [D0, M0=40]
              In : Ratio: 1.000000, FixDim:      5, VarDim:     25 [    25], Size:  10000, Total:   10000, Move:       8000 (Decl x 1.000000) L2
*           Bias : Ratio: 0.000000,                                          Size:    256, Total:   10256, Move:        256 (Decl x 1.000000) L2
*          Scale : Ratio: 0.000000,                                          Size:     64, Total:   10320, Move:         64 (Decl x 1.000000) L2
*         ScaleN : Ratio: 0.000000,                                          Size:     64, Total:   10384, Move:         64 (Decl x 1.000000) L2
*         Filter : Ratio: 0.000000,                                          Size:    576, Total:   10960, Move:        576 (Decl x 1.000000) L2
             Out : Ratio: 1.000000, FixDim:      5, VarDim:     25 [    25], Size:  10000, Total:   20960, Move:       8000 (Decl x 1.000000) L2
*        ConvOut : Ratio: 1.000000, FixDim:      5, VarDim:     25 [    25], Size:  20000, Total:   40960, Move:          0 (Decl x 0.000000) L2
*          Infos : Ratio: 0.000000,                                          Size:     12, Total:   40972, Move:          9 (Decl x 1.000000) L2
S7_Conv2d_64x1x3x3_Relu - IterSpace: Tile0 - L1 Memory:  40972, L2Move: 16969, L3Move: 0, Tiling Overhead: 1.000000
S7_Conv2d_64x1x3x3_Relu Found Parametric value for space D0 (Initial: 64, Div: 8) = 40 [40*1 + 24], Iteration for Tiled Space: 1
Ker: S7_Conv2d_64x1x3x3_Relu, Arg:         In, Size:   5000, Base1:      0, Base2:   5000
Ker: S7_Conv2d_64x1x3x3_Relu, Arg:       Bias, Size:    256, Base1:  10000, Base2:      0
Ker: S7_Conv2d_64x1x3x3_Relu, Arg:      Scale, Size:     64, Base1:  10256, Base2:      0
Ker: S7_Conv2d_64x1x3x3_Relu, Arg:     ScaleN, Size:     64, Base1:  10320, Base2:      0
Ker: S7_Conv2d_64x1x3x3_Relu, Arg:     Filter, Size:    576, Base1:  10384, Base2:      0
Ker: S7_Conv2d_64x1x3x3_Relu, Arg:        Out, Size:   5000, Base1:  10960, Base2:  15960
Ker: S7_Conv2d_64x1x3x3_Relu, Arg:    ConvOut, Size:  20000, Base1:  20960, Base2:      0
Ker: S7_Conv2d_64x1x3x3_Relu, Arg:      Infos, Size:     12, Base1:  40960, Base2:      0
S7_Conv2d_64x1x3x3_Relu For Iter Space: 0 Iteration count:   1, Given L1 Memory:  48736, Used L1 Memory:  40972, Reusable Memory: 7764, Used L2 Memory: 0
=================================================================================================

InFeat: 64, OutFeat: 64
Conv => W:  5, Pad:[0,0] PadT:[0,0] => Wc: 5, Filter:[1,1]
     => H:  25, Pad:[0,0] PadT:[0,0] => Hc: 25
Pool => Wc: 5, Pad:[0,0] => Wo: 5, Filter:[1,1]
     => Hc: 25, Pad:[0,0] => Ho: 25
OverlapC: 0
OverlapP: 0
TileCons: 1
UsedIn  : [5 x 25]
UsedC   : [5 x 25]
      SetBiasKerName: KerParSetBiasB32_SQ8
         ConvKerName: KerParConv1x1Stride1_SQ8
  DPReductionKerName: KerParReduct_CC_ReLU_SQ8
Nb Oper : 520000
Mapping this convolution to matrix multiplication
CNN_MatMul_SQ8: S10_Conv2d_64x64x1x1_Relu
In1  => W:   64, H:   64 
In2  => W:  125, H:   64, w:    5, h:   25, Sx: 1, Sy: 1
Out  => W:  125, H:   64 => Column first
       MatMulKerName: KerParMatMulB32_2x4_ReLU_SQ8

==== Process Tiling For User Kernel:            S10_Conv2d_64x64x1x1_Relu =======================
S10_Conv2d_64x64x1x1_Relu Partition[0] Size =   1424 (Min:   1024, Max:   8640), Fraction:       0.52, Giving:   8640 Bytes out of  48736 Bytes
S10_Conv2d_64x64x1x1_Relu Partition[1] Size =   1305 (Min:    512, Max:  32329), Fraction:       0.48, Giving:  40095 Bytes out of  48736 Bytes

Reference object:                  In2, Dim=125
	                 In2 Dim: 125, TileOverlap:  0, Ratio: 1.000000
	                 Out Dim: 125, TileOverlap:  0, Ratio: 1.000000

Kernel: S10_Conv2d_64x64x1x1_Relu, Total Raw Memory: 16268 fits into L1 memory 40095. Promoting all kernel arguments to initialized buffers.
Ker: S10_Conv2d_64x64x1x1_Relu, Arg:    KerBuff, Size:    256, Base1:      0, Base2:      0
Ker: S10_Conv2d_64x64x1x1_Relu, Arg:        In2, Size:   8000, Base1:    256, Base2:      0
Ker: S10_Conv2d_64x64x1x1_Relu, Arg:        Out, Size:   8000, Base1:   8256, Base2:      0
Ker: S10_Conv2d_64x64x1x1_Relu, Arg:      Infos, Size:     12, Base1:  16256, Base2:      0
S10_Conv2d_64x64x1x1_Relu For Iter Space: 1 Iteration count:   1, Given L1 Memory:  40095, Used L1 Memory:  16268, Reusable Memory: 23824, Used L2 Memory: 0

Reference object:                  In1, Dim=64
	                 In1 Dim:  64, TileOverlap:  0, Ratio: 1.000000
	                Bias Dim:  64, TileOverlap:  0, Ratio: 1.000000
	               Scale Dim:  64, TileOverlap:  0, Ratio: 1.000000
	              ScaleN Dim:  64, TileOverlap:  0, Ratio: 1.000000

Kernel: S10_Conv2d_64x64x1x1_Relu, Total Raw Memory: 4480 fits into L1 memory 8640. Promoting all kernel arguments to initialized buffers.
Ker: S10_Conv2d_64x64x1x1_Relu, Arg:        In1, Size:   4096, Base1:  16268, Base2:      0
Ker: S10_Conv2d_64x64x1x1_Relu, Arg:       Bias, Size:    256, Base1:  20364, Base2:      0
Ker: S10_Conv2d_64x64x1x1_Relu, Arg:      Scale, Size:     64, Base1:  20620, Base2:      0
Ker: S10_Conv2d_64x64x1x1_Relu, Arg:     ScaleN, Size:     64, Base1:  20684, Base2:      0
S10_Conv2d_64x64x1x1_Relu For Iter Space: 0 Iteration count:   1, Given L1 Memory:   8640, Used L1 Memory:   4480, Reusable Memory: 4160, Used L2 Memory: 0
=================================================================================================

InFeat: 64, OutFeat: 64
Conv => W:  5, Pad:[1,1] PadT:[1,1] => Wc: 5, Filter:[3,3]
     => H:  25, Pad:[1,1] PadT:[1,1] => Hc: 25
Pool => Wc: 5, Pad:[0,0] => Wo: 5, Filter:[1,1]
     => Hc: 25, Pad:[0,0] => Ho: 25
OverlapC: 2
OverlapP: 0
TileCons: 1
UsedIn  : [5 x 25]
UsedC   : [5 x 25]
         ConvKerName: KerParConvDW3x3Stride1B32_SQ8
  DPReductionKerName: KerParReduct_CC_ReLU_SQ8
Nb Oper : 80000

==== Process Tiling For User Kernel:             S13_Conv2d_64x1x3x3_Relu =======================
S13_Conv2d_64x1x3x3_Relu Partition[0] Size =  36505 (Min:     30, Max:  67337), Fraction:       1.00, Giving:  48736 Bytes out of  48736 Bytes

Reference object:                   In, Dim=25
	                  In Dim:  27, TileOverlap:  2, Ratio: 1.000000
	                 Out Dim:  25, TileOverlap:  0, Ratio: 1.000000
	             ConvOut Dim:  25, TileOverlap:  0, Ratio: 1.000000

S13_Conv2d_64x1x3x3_Relu Full buffering on Arg: Bias, was using 320 Bytes will require 256 Bytes buffer
S13_Conv2d_64x1x3x3_Relu Full buffering on Arg: Scale, was using 80 Bytes will require 64 Bytes buffer
S13_Conv2d_64x1x3x3_Relu Full buffering on Arg: ScaleN, was using 80 Bytes will require 64 Bytes buffer
S13_Conv2d_64x1x3x3_Relu Full buffering on Arg: Filter, was using 720 Bytes will require 576 Bytes buffer
S13_Conv2d_64x1x3x3_Relu, TiledSpace: Tile0 Iteration Count: 1 Parametric Space: [D0, M0=40]
              In : Ratio: 1.000000, FixDim:      5, VarDim:     25 [    25], Size:  10000, Total:   10000, Move:       8000 (Decl x 1.000000) L2
*           Bias : Ratio: 0.000000,                                          Size:    256, Total:   10256, Move:        256 (Decl x 1.000000) L2
*          Scale : Ratio: 0.000000,                                          Size:     64, Total:   10320, Move:         64 (Decl x 1.000000) L2
*         ScaleN : Ratio: 0.000000,                                          Size:     64, Total:   10384, Move:         64 (Decl x 1.000000) L2
*         Filter : Ratio: 0.000000,                                          Size:    576, Total:   10960, Move:        576 (Decl x 1.000000) L2
             Out : Ratio: 1.000000, FixDim:      5, VarDim:     25 [    25], Size:  10000, Total:   20960, Move:       8000 (Decl x 1.000000) L2
*        ConvOut : Ratio: 1.000000, FixDim:      5, VarDim:     25 [    25], Size:  20000, Total:   40960, Move:          0 (Decl x 0.000000) L2
*          Infos : Ratio: 0.000000,                                          Size:     12, Total:   40972, Move:          9 (Decl x 1.000000) L2
S13_Conv2d_64x1x3x3_Relu - IterSpace: Tile0 - L1 Memory:  40972, L2Move: 16969, L3Move: 0, Tiling Overhead: 1.000000
S13_Conv2d_64x1x3x3_Relu Found Parametric value for space D0 (Initial: 64, Div: 8) = 40 [40*1 + 24], Iteration for Tiled Space: 1
Ker: S13_Conv2d_64x1x3x3_Relu, Arg:         In, Size:   5000, Base1:      0, Base2:   5000
Ker: S13_Conv2d_64x1x3x3_Relu, Arg:       Bias, Size:    256, Base1:  10000, Base2:      0
Ker: S13_Conv2d_64x1x3x3_Relu, Arg:      Scale, Size:     64, Base1:  10256, Base2:      0
Ker: S13_Conv2d_64x1x3x3_Relu, Arg:     ScaleN, Size:     64, Base1:  10320, Base2:      0
Ker: S13_Conv2d_64x1x3x3_Relu, Arg:     Filter, Size:    576, Base1:  10384, Base2:      0
Ker: S13_Conv2d_64x1x3x3_Relu, Arg:        Out, Size:   5000, Base1:  10960, Base2:  15960
Ker: S13_Conv2d_64x1x3x3_Relu, Arg:    ConvOut, Size:  20000, Base1:  20960, Base2:      0
Ker: S13_Conv2d_64x1x3x3_Relu, Arg:      Infos, Size:     12, Base1:  40960, Base2:      0
S13_Conv2d_64x1x3x3_Relu For Iter Space: 0 Iteration count:   1, Given L1 Memory:  48736, Used L1 Memory:  40972, Reusable Memory: 7764, Used L2 Memory: 0
=================================================================================================

InFeat: 64, OutFeat: 64
Conv => W:  5, Pad:[0,0] PadT:[0,0] => Wc: 5, Filter:[1,1]
     => H:  25, Pad:[0,0] PadT:[0,0] => Hc: 25
Pool => Wc: 5, Pad:[0,0] => Wo: 5, Filter:[1,1]
     => Hc: 25, Pad:[0,0] => Ho: 25
OverlapC: 0
OverlapP: 0
TileCons: 1
UsedIn  : [5 x 25]
UsedC   : [5 x 25]
      SetBiasKerName: KerParSetBiasB32_SQ8
         ConvKerName: KerParConv1x1Stride1_SQ8
  DPReductionKerName: KerParReduct_CC_ReLU_SQ8
Nb Oper : 520000
Mapping this convolution to matrix multiplication
CNN_MatMul_SQ8: S16_Conv2d_64x64x1x1_Relu
In1  => W:   64, H:   64 
In2  => W:  125, H:   64, w:    5, h:   25, Sx: 1, Sy: 1
Out  => W:  125, H:   64 => Column first
       MatMulKerName: KerParMatMulB32_2x4_ReLU_SQ8

==== Process Tiling For User Kernel:            S16_Conv2d_64x64x1x1_Relu =======================
S16_Conv2d_64x64x1x1_Relu Partition[0] Size =   1424 (Min:   1024, Max:   8640), Fraction:       0.52, Giving:   8640 Bytes out of  48736 Bytes
S16_Conv2d_64x64x1x1_Relu Partition[1] Size =   1305 (Min:    512, Max:  32329), Fraction:       0.48, Giving:  40095 Bytes out of  48736 Bytes

Reference object:                  In2, Dim=125
	                 In2 Dim: 125, TileOverlap:  0, Ratio: 1.000000
	                 Out Dim: 125, TileOverlap:  0, Ratio: 1.000000

Kernel: S16_Conv2d_64x64x1x1_Relu, Total Raw Memory: 16268 fits into L1 memory 40095. Promoting all kernel arguments to initialized buffers.
Ker: S16_Conv2d_64x64x1x1_Relu, Arg:    KerBuff, Size:    256, Base1:      0, Base2:      0
Ker: S16_Conv2d_64x64x1x1_Relu, Arg:        In2, Size:   8000, Base1:    256, Base2:      0
Ker: S16_Conv2d_64x64x1x1_Relu, Arg:        Out, Size:   8000, Base1:   8256, Base2:      0
Ker: S16_Conv2d_64x64x1x1_Relu, Arg:      Infos, Size:     12, Base1:  16256, Base2:      0
S16_Conv2d_64x64x1x1_Relu For Iter Space: 1 Iteration count:   1, Given L1 Memory:  40095, Used L1 Memory:  16268, Reusable Memory: 23824, Used L2 Memory: 0

Reference object:                  In1, Dim=64
	                 In1 Dim:  64, TileOverlap:  0, Ratio: 1.000000
	                Bias Dim:  64, TileOverlap:  0, Ratio: 1.000000
	               Scale Dim:  64, TileOverlap:  0, Ratio: 1.000000
	              ScaleN Dim:  64, TileOverlap:  0, Ratio: 1.000000

Kernel: S16_Conv2d_64x64x1x1_Relu, Total Raw Memory: 4480 fits into L1 memory 8640. Promoting all kernel arguments to initialized buffers.
Ker: S16_Conv2d_64x64x1x1_Relu, Arg:        In1, Size:   4096, Base1:  16268, Base2:      0
Ker: S16_Conv2d_64x64x1x1_Relu, Arg:       Bias, Size:    256, Base1:  20364, Base2:      0
Ker: S16_Conv2d_64x64x1x1_Relu, Arg:      Scale, Size:     64, Base1:  20620, Base2:      0
Ker: S16_Conv2d_64x64x1x1_Relu, Arg:     ScaleN, Size:     64, Base1:  20684, Base2:      0
S16_Conv2d_64x64x1x1_Relu For Iter Space: 0 Iteration count:   1, Given L1 Memory:   8640, Used L1 Memory:   4480, Reusable Memory: 4160, Used L2 Memory: 0
=================================================================================================

InFeat: 64, OutFeat: 64
Conv => W:  5, Pad:[1,1] PadT:[1,1] => Wc: 5, Filter:[3,3]
     => H:  25, Pad:[1,1] PadT:[1,1] => Hc: 25
Pool => Wc: 5, Pad:[0,0] => Wo: 5, Filter:[1,1]
     => Hc: 25, Pad:[0,0] => Ho: 25
OverlapC: 2
OverlapP: 0
TileCons: 1
UsedIn  : [5 x 25]
UsedC   : [5 x 25]
         ConvKerName: KerParConvDW3x3Stride1B32_SQ8
  DPReductionKerName: KerParReduct_CC_ReLU_SQ8
Nb Oper : 80000

==== Process Tiling For User Kernel:             S19_Conv2d_64x1x3x3_Relu =======================
S19_Conv2d_64x1x3x3_Relu Partition[0] Size =  36505 (Min:     30, Max:  67337), Fraction:       1.00, Giving:  48736 Bytes out of  48736 Bytes

Reference object:                   In, Dim=25
	                  In Dim:  27, TileOverlap:  2, Ratio: 1.000000
	                 Out Dim:  25, TileOverlap:  0, Ratio: 1.000000
	             ConvOut Dim:  25, TileOverlap:  0, Ratio: 1.000000

S19_Conv2d_64x1x3x3_Relu Full buffering on Arg: Bias, was using 320 Bytes will require 256 Bytes buffer
S19_Conv2d_64x1x3x3_Relu Full buffering on Arg: Scale, was using 80 Bytes will require 64 Bytes buffer
S19_Conv2d_64x1x3x3_Relu Full buffering on Arg: ScaleN, was using 80 Bytes will require 64 Bytes buffer
S19_Conv2d_64x1x3x3_Relu Full buffering on Arg: Filter, was using 720 Bytes will require 576 Bytes buffer
S19_Conv2d_64x1x3x3_Relu, TiledSpace: Tile0 Iteration Count: 1 Parametric Space: [D0, M0=40]
              In : Ratio: 1.000000, FixDim:      5, VarDim:     25 [    25], Size:  10000, Total:   10000, Move:       8000 (Decl x 1.000000) L2
*           Bias : Ratio: 0.000000,                                          Size:    256, Total:   10256, Move:        256 (Decl x 1.000000) L2
*          Scale : Ratio: 0.000000,                                          Size:     64, Total:   10320, Move:         64 (Decl x 1.000000) L2
*         ScaleN : Ratio: 0.000000,                                          Size:     64, Total:   10384, Move:         64 (Decl x 1.000000) L2
*         Filter : Ratio: 0.000000,                                          Size:    576, Total:   10960, Move:        576 (Decl x 1.000000) L2
             Out : Ratio: 1.000000, FixDim:      5, VarDim:     25 [    25], Size:  10000, Total:   20960, Move:       8000 (Decl x 1.000000) L2
*        ConvOut : Ratio: 1.000000, FixDim:      5, VarDim:     25 [    25], Size:  20000, Total:   40960, Move:          0 (Decl x 0.000000) L2
*          Infos : Ratio: 0.000000,                                          Size:     12, Total:   40972, Move:          9 (Decl x 1.000000) L2
S19_Conv2d_64x1x3x3_Relu - IterSpace: Tile0 - L1 Memory:  40972, L2Move: 16969, L3Move: 0, Tiling Overhead: 1.000000
S19_Conv2d_64x1x3x3_Relu Found Parametric value for space D0 (Initial: 64, Div: 8) = 40 [40*1 + 24], Iteration for Tiled Space: 1
Ker: S19_Conv2d_64x1x3x3_Relu, Arg:         In, Size:   5000, Base1:      0, Base2:   5000
Ker: S19_Conv2d_64x1x3x3_Relu, Arg:       Bias, Size:    256, Base1:  10000, Base2:      0
Ker: S19_Conv2d_64x1x3x3_Relu, Arg:      Scale, Size:     64, Base1:  10256, Base2:      0
Ker: S19_Conv2d_64x1x3x3_Relu, Arg:     ScaleN, Size:     64, Base1:  10320, Base2:      0
Ker: S19_Conv2d_64x1x3x3_Relu, Arg:     Filter, Size:    576, Base1:  10384, Base2:      0
Ker: S19_Conv2d_64x1x3x3_Relu, Arg:        Out, Size:   5000, Base1:  10960, Base2:  15960
Ker: S19_Conv2d_64x1x3x3_Relu, Arg:    ConvOut, Size:  20000, Base1:  20960, Base2:      0
Ker: S19_Conv2d_64x1x3x3_Relu, Arg:      Infos, Size:     12, Base1:  40960, Base2:      0
S19_Conv2d_64x1x3x3_Relu For Iter Space: 0 Iteration count:   1, Given L1 Memory:  48736, Used L1 Memory:  40972, Reusable Memory: 7764, Used L2 Memory: 0
=================================================================================================

InFeat: 64, OutFeat: 64
Conv => W:  5, Pad:[0,0] PadT:[0,0] => Wc: 5, Filter:[1,1]
     => H:  25, Pad:[0,0] PadT:[0,0] => Hc: 25
Pool => Wc: 5, Pad:[0,0] => Wo: 5, Filter:[1,1]
     => Hc: 25, Pad:[0,0] => Ho: 25
OverlapC: 0
OverlapP: 0
TileCons: 1
UsedIn  : [5 x 25]
UsedC   : [5 x 25]
      SetBiasKerName: KerParSetBiasB32_SQ8
         ConvKerName: KerParConv1x1Stride1_SQ8
  DPReductionKerName: KerParReduct_CC_ReLU_SQ8
Nb Oper : 520000
Mapping this convolution to matrix multiplication
CNN_MatMul_SQ8: S22_Conv2d_64x64x1x1_Relu
In1  => W:   64, H:   64 
In2  => W:  125, H:   64, w:    5, h:   25, Sx: 1, Sy: 1
Out  => W:  125, H:   64 => Column first
       MatMulKerName: KerParMatMulB32_2x4_ReLU_SQ8

==== Process Tiling For User Kernel:            S22_Conv2d_64x64x1x1_Relu =======================
S22_Conv2d_64x64x1x1_Relu Partition[0] Size =   1424 (Min:   1024, Max:   8640), Fraction:       0.52, Giving:   8640 Bytes out of  48736 Bytes
S22_Conv2d_64x64x1x1_Relu Partition[1] Size =   1305 (Min:    512, Max:  32329), Fraction:       0.48, Giving:  40095 Bytes out of  48736 Bytes

Reference object:                  In2, Dim=125
	                 In2 Dim: 125, TileOverlap:  0, Ratio: 1.000000
	                 Out Dim: 125, TileOverlap:  0, Ratio: 1.000000

Kernel: S22_Conv2d_64x64x1x1_Relu, Total Raw Memory: 16268 fits into L1 memory 40095. Promoting all kernel arguments to initialized buffers.
Ker: S22_Conv2d_64x64x1x1_Relu, Arg:    KerBuff, Size:    256, Base1:      0, Base2:      0
Ker: S22_Conv2d_64x64x1x1_Relu, Arg:        In2, Size:   8000, Base1:    256, Base2:      0
Ker: S22_Conv2d_64x64x1x1_Relu, Arg:        Out, Size:   8000, Base1:   8256, Base2:      0
Ker: S22_Conv2d_64x64x1x1_Relu, Arg:      Infos, Size:     12, Base1:  16256, Base2:      0
S22_Conv2d_64x64x1x1_Relu For Iter Space: 1 Iteration count:   1, Given L1 Memory:  40095, Used L1 Memory:  16268, Reusable Memory: 23824, Used L2 Memory: 0

Reference object:                  In1, Dim=64
	                 In1 Dim:  64, TileOverlap:  0, Ratio: 1.000000
	                Bias Dim:  64, TileOverlap:  0, Ratio: 1.000000
	               Scale Dim:  64, TileOverlap:  0, Ratio: 1.000000
	              ScaleN Dim:  64, TileOverlap:  0, Ratio: 1.000000

Kernel: S22_Conv2d_64x64x1x1_Relu, Total Raw Memory: 4480 fits into L1 memory 8640. Promoting all kernel arguments to initialized buffers.
Ker: S22_Conv2d_64x64x1x1_Relu, Arg:        In1, Size:   4096, Base1:  16268, Base2:      0
Ker: S22_Conv2d_64x64x1x1_Relu, Arg:       Bias, Size:    256, Base1:  20364, Base2:      0
Ker: S22_Conv2d_64x64x1x1_Relu, Arg:      Scale, Size:     64, Base1:  20620, Base2:      0
Ker: S22_Conv2d_64x64x1x1_Relu, Arg:     ScaleN, Size:     64, Base1:  20684, Base2:      0
S22_Conv2d_64x64x1x1_Relu For Iter Space: 0 Iteration count:   1, Given L1 Memory:   8640, Used L1 Memory:   4480, Reusable Memory: 4160, Used L2 Memory: 0
=================================================================================================

InFeat: 64, OutFeat: 64
Conv => W:  5, Pad:[1,1] PadT:[1,1] => Wc: 5, Filter:[3,3]
     => H:  25, Pad:[1,1] PadT:[1,1] => Hc: 25
Pool => Wc: 5, Pad:[0,0] => Wo: 5, Filter:[1,1]
     => Hc: 25, Pad:[0,0] => Ho: 25
OverlapC: 2
OverlapP: 0
TileCons: 1
UsedIn  : [5 x 25]
UsedC   : [5 x 25]
         ConvKerName: KerParConvDW3x3Stride1B32_SQ8
  DPReductionKerName: KerParReduct_CC_ReLU_SQ8
Nb Oper : 80000

==== Process Tiling For User Kernel:             S25_Conv2d_64x1x3x3_Relu =======================
S25_Conv2d_64x1x3x3_Relu Partition[0] Size =  36505 (Min:     30, Max:  67337), Fraction:       1.00, Giving:  48736 Bytes out of  48736 Bytes

Reference object:                   In, Dim=25
	                  In Dim:  27, TileOverlap:  2, Ratio: 1.000000
	                 Out Dim:  25, TileOverlap:  0, Ratio: 1.000000
	             ConvOut Dim:  25, TileOverlap:  0, Ratio: 1.000000

S25_Conv2d_64x1x3x3_Relu Full buffering on Arg: Bias, was using 320 Bytes will require 256 Bytes buffer
S25_Conv2d_64x1x3x3_Relu Full buffering on Arg: Scale, was using 80 Bytes will require 64 Bytes buffer
S25_Conv2d_64x1x3x3_Relu Full buffering on Arg: ScaleN, was using 80 Bytes will require 64 Bytes buffer
S25_Conv2d_64x1x3x3_Relu Full buffering on Arg: Filter, was using 720 Bytes will require 576 Bytes buffer
S25_Conv2d_64x1x3x3_Relu, TiledSpace: Tile0 Iteration Count: 1 Parametric Space: [D0, M0=40]
              In : Ratio: 1.000000, FixDim:      5, VarDim:     25 [    25], Size:  10000, Total:   10000, Move:       8000 (Decl x 1.000000) L2
*           Bias : Ratio: 0.000000,                                          Size:    256, Total:   10256, Move:        256 (Decl x 1.000000) L2
*          Scale : Ratio: 0.000000,                                          Size:     64, Total:   10320, Move:         64 (Decl x 1.000000) L2
*         ScaleN : Ratio: 0.000000,                                          Size:     64, Total:   10384, Move:         64 (Decl x 1.000000) L2
*         Filter : Ratio: 0.000000,                                          Size:    576, Total:   10960, Move:        576 (Decl x 1.000000) L2
             Out : Ratio: 1.000000, FixDim:      5, VarDim:     25 [    25], Size:  10000, Total:   20960, Move:       8000 (Decl x 1.000000) L2
*        ConvOut : Ratio: 1.000000, FixDim:      5, VarDim:     25 [    25], Size:  20000, Total:   40960, Move:          0 (Decl x 0.000000) L2
*          Infos : Ratio: 0.000000,                                          Size:     12, Total:   40972, Move:          9 (Decl x 1.000000) L2
S25_Conv2d_64x1x3x3_Relu - IterSpace: Tile0 - L1 Memory:  40972, L2Move: 16969, L3Move: 0, Tiling Overhead: 1.000000
S25_Conv2d_64x1x3x3_Relu Found Parametric value for space D0 (Initial: 64, Div: 8) = 40 [40*1 + 24], Iteration for Tiled Space: 1
Ker: S25_Conv2d_64x1x3x3_Relu, Arg:         In, Size:   5000, Base1:      0, Base2:   5000
Ker: S25_Conv2d_64x1x3x3_Relu, Arg:       Bias, Size:    256, Base1:  10000, Base2:      0
Ker: S25_Conv2d_64x1x3x3_Relu, Arg:      Scale, Size:     64, Base1:  10256, Base2:      0
Ker: S25_Conv2d_64x1x3x3_Relu, Arg:     ScaleN, Size:     64, Base1:  10320, Base2:      0
Ker: S25_Conv2d_64x1x3x3_Relu, Arg:     Filter, Size:    576, Base1:  10384, Base2:      0
Ker: S25_Conv2d_64x1x3x3_Relu, Arg:        Out, Size:   5000, Base1:  10960, Base2:  15960
Ker: S25_Conv2d_64x1x3x3_Relu, Arg:    ConvOut, Size:  20000, Base1:  20960, Base2:      0
Ker: S25_Conv2d_64x1x3x3_Relu, Arg:      Infos, Size:     12, Base1:  40960, Base2:      0
S25_Conv2d_64x1x3x3_Relu For Iter Space: 0 Iteration count:   1, Given L1 Memory:  48736, Used L1 Memory:  40972, Reusable Memory: 7764, Used L2 Memory: 0
=================================================================================================

InFeat: 64, OutFeat: 64
Conv => W:  5, Pad:[0,0] PadT:[0,0] => Wc: 5, Filter:[1,1]
     => H:  25, Pad:[0,0] PadT:[0,0] => Hc: 25
Pool => Wc: 5, Pad:[0,0] => Wo: 5, Filter:[1,1]
     => Hc: 25, Pad:[0,0] => Ho: 25
OverlapC: 0
OverlapP: 0
TileCons: 1
UsedIn  : [5 x 25]
UsedC   : [5 x 25]
      SetBiasKerName: KerParSetBiasB32_SQ8
         ConvKerName: KerParConv1x1Stride1_SQ8
  DPReductionKerName: KerParReduct_CC_ReLU_SQ8
Nb Oper : 520000
Mapping this convolution to matrix multiplication
CNN_MatMul_SQ8: S28_Conv2d_64x64x1x1_Relu
In1  => W:   64, H:   64 
In2  => W:  125, H:   64, w:    5, h:   25, Sx: 1, Sy: 1
Out  => W:  125, H:   64 => Column first
       MatMulKerName: KerParMatMulB32_2x4_ReLU_SQ8

==== Process Tiling For User Kernel:            S28_Conv2d_64x64x1x1_Relu =======================
S28_Conv2d_64x64x1x1_Relu Partition[0] Size =   1424 (Min:   1024, Max:   8640), Fraction:       0.52, Giving:   8640 Bytes out of  48736 Bytes
S28_Conv2d_64x64x1x1_Relu Partition[1] Size =   1305 (Min:    512, Max:  32329), Fraction:       0.48, Giving:  40095 Bytes out of  48736 Bytes

Reference object:                  In2, Dim=125
	                 In2 Dim: 125, TileOverlap:  0, Ratio: 1.000000
	                 Out Dim: 125, TileOverlap:  0, Ratio: 1.000000

Kernel: S28_Conv2d_64x64x1x1_Relu, Total Raw Memory: 16268 fits into L1 memory 40095. Promoting all kernel arguments to initialized buffers.
Ker: S28_Conv2d_64x64x1x1_Relu, Arg:    KerBuff, Size:    256, Base1:      0, Base2:      0
Ker: S28_Conv2d_64x64x1x1_Relu, Arg:        In2, Size:   8000, Base1:    256, Base2:      0
Ker: S28_Conv2d_64x64x1x1_Relu, Arg:        Out, Size:   8000, Base1:   8256, Base2:      0
Ker: S28_Conv2d_64x64x1x1_Relu, Arg:      Infos, Size:     12, Base1:  16256, Base2:      0
S28_Conv2d_64x64x1x1_Relu For Iter Space: 1 Iteration count:   1, Given L1 Memory:  40095, Used L1 Memory:  16268, Reusable Memory: 23824, Used L2 Memory: 0

Reference object:                  In1, Dim=64
	                 In1 Dim:  64, TileOverlap:  0, Ratio: 1.000000
	                Bias Dim:  64, TileOverlap:  0, Ratio: 1.000000
	               Scale Dim:  64, TileOverlap:  0, Ratio: 1.000000
	              ScaleN Dim:  64, TileOverlap:  0, Ratio: 1.000000

Kernel: S28_Conv2d_64x64x1x1_Relu, Total Raw Memory: 4480 fits into L1 memory 8640. Promoting all kernel arguments to initialized buffers.
Ker: S28_Conv2d_64x64x1x1_Relu, Arg:        In1, Size:   4096, Base1:  16268, Base2:      0
Ker: S28_Conv2d_64x64x1x1_Relu, Arg:       Bias, Size:    256, Base1:  20364, Base2:      0
Ker: S28_Conv2d_64x64x1x1_Relu, Arg:      Scale, Size:     64, Base1:  20620, Base2:      0
Ker: S28_Conv2d_64x64x1x1_Relu, Arg:     ScaleN, Size:     64, Base1:  20684, Base2:      0
S28_Conv2d_64x64x1x1_Relu For Iter Space: 0 Iteration count:   1, Given L1 Memory:   8640, Used L1 Memory:   4480, Reusable Memory: 4160, Used L2 Memory: 0
=================================================================================================

Pool => W: 5, Pad:[0,0] => Wo: 1
     => H: 25, Pad:[0,0] => Ho: 1
OverlapP: 23
TileCons: 2
UsedIn  : [5 x 25]
         PoolKerName: KerParPoolNxMStrideSxSy_SQ8
Nb Oper : 8000

==== Process Tiling For User Kernel:                 S29_AveragePool_25x5 =======================
S29_AveragePool_25x5 Partition[0] Size =  16271 (Min:    250, Max:  16155), Fraction:       1.00, Giving:  48736 Bytes out of  48736 Bytes

Reference object:                  Out, Dim=1
	                  In Dim:  25, TileOverlap: 23, Ratio: 2.000000
	                 Out Dim:   1, TileOverlap:  0, Ratio: 1.000000

Kernel: S29_AveragePool_25x5, Total Raw Memory: 8076 fits into L1 memory 48736. Promoting all kernel arguments to initialized buffers.
Ker: S29_AveragePool_25x5, Arg:         In, Size:   8000, Base1:      0, Base2:      0
Ker: S29_AveragePool_25x5, Arg:        Out, Size:     64, Base1:   8000, Base2:      0
Ker: S29_AveragePool_25x5, Arg:      Infos, Size:     12, Base1:   8064, Base2:      0
S29_AveragePool_25x5 For Iter Space: 0 Iteration count:   1, Given L1 Memory:  48736, Used L1 Memory:   8076, Reusable Memory: 40660, Used L2 Memory: 0
=================================================================================================

Linear Layer S32_Linear_12x64x1x1, Linear: InDim: 64, OutDim: 12, Activation: None
Linear Kernel: KerParLinearLayerFullFeatB32_SQ8

==== Process Tiling For User Kernel:                 S32_Linear_12x64x1x1 =======================
S32_Linear_12x64x1x1 Partition[0] Size =   1791 (Min:      0, Max:   1875), Fraction:       1.00, Giving:  48736 Bytes out of  48736 Bytes

Reference object:                   In, Dim=1

Kernel: S32_Linear_12x64x1x1, Total Raw Memory: 928 fits into L1 memory 48736. Promoting all kernel arguments to initialized buffers.
Ker: S32_Linear_12x64x1x1, Arg:         In, Size:     64, Base1:      0, Base2:      0
Ker: S32_Linear_12x64x1x1, Arg:     Filter, Size:    768, Base1:     64, Base2:      0
Ker: S32_Linear_12x64x1x1, Arg:       Bias, Size:     48, Base1:    832, Base2:      0
Ker: S32_Linear_12x64x1x1, Arg:        Out, Size:     12, Base1:    880, Base2:      0
Ker: S32_Linear_12x64x1x1, Arg:      Scale, Size:     12, Base1:    892, Base2:      0
Ker: S32_Linear_12x64x1x1, Arg:     ScaleN, Size:     12, Base1:    904, Base2:      0
Ker: S32_Linear_12x64x1x1, Arg:      Infos, Size:     12, Base1:    916, Base2:      0
S32_Linear_12x64x1x1 For Iter Space: 0 Iteration count:   1, Given L1 Memory:  48736, Used L1 Memory:    928, Reusable Memory: 47808, Used L2 Memory: 0
=================================================================================================


==== Process Tiling For User Kernel:                          S33_SoftMax =======================
         S33_SoftMax Partition[0] Size =     51 (Min:      8, Max:     63), Fraction:       1.00, Giving:  48736 Bytes out of  48736 Bytes

Reference object:                   In, Dim=12
	                  In Dim:  12, TileOverlap:  0, Ratio: 1.000000
	                 Out Dim:  12, TileOverlap:  0, Ratio: 1.000000

Kernel:          S33_SoftMax, Total Raw Memory: 48 fits into L1 memory 48736. Promoting all kernel arguments to initialized buffers.
Ker: S33_SoftMax, Arg:         In, Size:     12, Base1:      0, Base2:      0
Ker: S33_SoftMax, Arg:        Out, Size:     24, Base1:     12, Base2:      0
Ker: S33_SoftMax, Arg:      Infos, Size:     12, Base1:     36, Base2:      0
         S33_SoftMax For Iter Space: 0 Iteration count:   1, Given L1 Memory:  48736, Used L1 Memory:     48, Reusable Memory: 48688, Used L2 Memory: 0
=================================================================================================

   Symbol:           S32_Output[   In] Adding   Edge From                S32_Linear_12x64x1x1 To                         S33_SoftMax New
   Symbol:           S29_Output[   In] Adding   Edge From                S29_AveragePool_25x5 To                S32_Linear_12x64x1x1 New
   Symbol:           S28_Output[   In] Adding   Edge From           S28_Conv2d_64x64x1x1_Relu To                S29_AveragePool_25x5 New
   Symbol:           S25_Output[   In] Adding   Edge From            S25_Conv2d_64x1x3x3_Relu To           S28_Conv2d_64x64x1x1_Relu New
   Symbol:           S22_Output[   In] Adding   Edge From           S22_Conv2d_64x64x1x1_Relu To            S25_Conv2d_64x1x3x3_Relu New
   Symbol:           S19_Output[   In] Adding   Edge From            S19_Conv2d_64x1x3x3_Relu To           S22_Conv2d_64x64x1x1_Relu New
   Symbol:           S16_Output[   In] Adding   Edge From           S16_Conv2d_64x64x1x1_Relu To            S19_Conv2d_64x1x3x3_Relu New
   Symbol:           S13_Output[   In] Adding   Edge From            S13_Conv2d_64x1x3x3_Relu To           S16_Conv2d_64x64x1x1_Relu New
   Symbol:           S10_Output[   In] Adding   Edge From           S10_Conv2d_64x64x1x1_Relu To            S13_Conv2d_64x1x3x3_Relu New
   Symbol:            S7_Output[   In] Adding   Edge From             S7_Conv2d_64x1x3x3_Relu To           S10_Conv2d_64x64x1x1_Relu New
   Symbol:            S4_Output[   In] Adding   Edge From            S4_Conv2d_64x1x10x4_Relu To             S7_Conv2d_64x1x3x3_Relu New
   Symbol:             Output_1[  Out] Adding   Edge From                         S33_SoftMax To                       __GraphExit__ New
   Symbol:            S33_Infos[   In] Adding   Edge From                      __GraphEntry__ To                         S33_SoftMax New
   Symbol:            S32_Infos[   In] Adding   Edge From                      __GraphEntry__ To                S32_Linear_12x64x1x1 New
   Symbol:        S32_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To                S32_Linear_12x64x1x1 Exists
   Symbol:        S32_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To                S32_Linear_12x64x1x1 Exists
   Symbol:  Dscnnfc1matmul_bias[   In] Adding   Edge From                      __GraphEntry__ To                S32_Linear_12x64x1x1 Exists
   Symbol: Dscnnfc1weights_quantfakequant[   In] Adding   Edge From                      __GraphEntry__ To                S32_Linear_12x64x1x1 Exists
   Symbol:            S29_Infos[   In] Adding   Edge From                      __GraphEntry__ To                S29_AveragePool_25x5 New
   Symbol:            S28_Infos[   In] Adding   Edge From                      __GraphEntry__ To           S28_Conv2d_64x64x1x1_Relu New
   Symbol:        S28_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To           S28_Conv2d_64x64x1x1_Relu Exists
   Symbol:        S28_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To           S28_Conv2d_64x64x1x1_Relu Exists
   Symbol: Dscnnconv_ds_4pw_convconv2d_fo[   In] Adding   Edge From                      __GraphEntry__ To           S28_Conv2d_64x64x1x1_Relu Exists
   Symbol: Dscnnconv_ds_4pw_convweights_q[   In] Adding   Edge From                      __GraphEntry__ To           S28_Conv2d_64x64x1x1_Relu Exists
   Symbol:            S25_Infos[   In] Adding   Edge From                      __GraphEntry__ To            S25_Conv2d_64x1x3x3_Relu New
   Symbol:        S25_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To            S25_Conv2d_64x1x3x3_Relu Exists
   Symbol:        S25_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To            S25_Conv2d_64x1x3x3_Relu Exists
   Symbol: Dscnnconv_ds_4dw_convdepthwise[   In] Adding   Edge From                      __GraphEntry__ To            S25_Conv2d_64x1x3x3_Relu Exists
   Symbol: Dscnnconv_ds_4dw_convweights_q[   In] Adding   Edge From                      __GraphEntry__ To            S25_Conv2d_64x1x3x3_Relu Exists
   Symbol:            S22_Infos[   In] Adding   Edge From                      __GraphEntry__ To           S22_Conv2d_64x64x1x1_Relu New
   Symbol:        S22_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To           S22_Conv2d_64x64x1x1_Relu Exists
   Symbol:        S22_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To           S22_Conv2d_64x64x1x1_Relu Exists
   Symbol: Dscnnconv_ds_3pw_convconv2d_fo[   In] Adding   Edge From                      __GraphEntry__ To           S22_Conv2d_64x64x1x1_Relu Exists
   Symbol: Dscnnconv_ds_3pw_convweights_q[   In] Adding   Edge From                      __GraphEntry__ To           S22_Conv2d_64x64x1x1_Relu Exists
   Symbol:            S19_Infos[   In] Adding   Edge From                      __GraphEntry__ To            S19_Conv2d_64x1x3x3_Relu New
   Symbol:        S19_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To            S19_Conv2d_64x1x3x3_Relu Exists
   Symbol:        S19_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To            S19_Conv2d_64x1x3x3_Relu Exists
   Symbol: Dscnnconv_ds_3dw_convdepthwise[   In] Adding   Edge From                      __GraphEntry__ To            S19_Conv2d_64x1x3x3_Relu Exists
   Symbol: Dscnnconv_ds_3dw_convweights_q[   In] Adding   Edge From                      __GraphEntry__ To            S19_Conv2d_64x1x3x3_Relu Exists
   Symbol:            S16_Infos[   In] Adding   Edge From                      __GraphEntry__ To           S16_Conv2d_64x64x1x1_Relu New
   Symbol:        S16_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To           S16_Conv2d_64x64x1x1_Relu Exists
   Symbol:        S16_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To           S16_Conv2d_64x64x1x1_Relu Exists
   Symbol: Dscnnconv_ds_2pw_convconv2d_fo[   In] Adding   Edge From                      __GraphEntry__ To           S16_Conv2d_64x64x1x1_Relu Exists
   Symbol: Dscnnconv_ds_2pw_convweights_q[   In] Adding   Edge From                      __GraphEntry__ To           S16_Conv2d_64x64x1x1_Relu Exists
   Symbol:            S13_Infos[   In] Adding   Edge From                      __GraphEntry__ To            S13_Conv2d_64x1x3x3_Relu New
   Symbol:        S13_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To            S13_Conv2d_64x1x3x3_Relu Exists
   Symbol:        S13_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To            S13_Conv2d_64x1x3x3_Relu Exists
   Symbol: Dscnnconv_ds_2dw_convdepthwise[   In] Adding   Edge From                      __GraphEntry__ To            S13_Conv2d_64x1x3x3_Relu Exists
   Symbol: Dscnnconv_ds_2dw_convweights_q[   In] Adding   Edge From                      __GraphEntry__ To            S13_Conv2d_64x1x3x3_Relu Exists
   Symbol:            S10_Infos[   In] Adding   Edge From                      __GraphEntry__ To           S10_Conv2d_64x64x1x1_Relu New
   Symbol:        S10_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To           S10_Conv2d_64x64x1x1_Relu Exists
   Symbol:        S10_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To           S10_Conv2d_64x64x1x1_Relu Exists
   Symbol: Dscnnconv_ds_1pw_convconv2d_fo[   In] Adding   Edge From                      __GraphEntry__ To           S10_Conv2d_64x64x1x1_Relu Exists
   Symbol: Dscnnconv_ds_1pw_convweights_q[   In] Adding   Edge From                      __GraphEntry__ To           S10_Conv2d_64x64x1x1_Relu Exists
   Symbol:             S7_Infos[   In] Adding   Edge From                      __GraphEntry__ To             S7_Conv2d_64x1x3x3_Relu New
   Symbol:         S7_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To             S7_Conv2d_64x1x3x3_Relu Exists
   Symbol:         S7_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To             S7_Conv2d_64x1x3x3_Relu Exists
   Symbol: Dscnnconv_ds_1dw_convdepthwise[   In] Adding   Edge From                      __GraphEntry__ To             S7_Conv2d_64x1x3x3_Relu Exists
   Symbol: Dscnnconv_ds_1dw_convweights_q[   In] Adding   Edge From                      __GraphEntry__ To             S7_Conv2d_64x1x3x3_Relu Exists
   Symbol:             S4_Infos[   In] Adding   Edge From                      __GraphEntry__ To            S4_Conv2d_64x1x10x4_Relu New
   Symbol:         S4_Mul_shift[   In] Adding   Edge From                      __GraphEntry__ To            S4_Conv2d_64x1x10x4_Relu Exists
   Symbol:         S4_Mul_scale[   In] Adding   Edge From                      __GraphEntry__ To            S4_Conv2d_64x1x10x4_Relu Exists
   Symbol: Dscnnconv_1conv2d_fold_bias[   In] Adding   Edge From                      __GraphEntry__ To            S4_Conv2d_64x1x10x4_Relu Exists
   Symbol: Dscnnconv_1weights_quantfakequ[   In] Adding   Edge From                      __GraphEntry__ To            S4_Conv2d_64x1x10x4_Relu Exists
   Symbol:              Input_1[   In] Adding   Edge From                      __GraphEntry__ To            S4_Conv2d_64x1x10x4_Relu Exists
After Dynamic Allocation, TopL3: 0, TopL2: 24000 => Alloc: OK

After Const Allocation, TopL3: 0, TopL2: 49685 => Alloc: OK

[FULL] Remapping [24000 .. 49684] to [0 .. 25684] Align compensation: 3
[PART] Remapping [0 .. 23999] to [25688 .. 49687] Align compensation: 0
[PART] Remapping [49685 .. 349999] to [49688 .. 350002] Align compensation: 1
Symbol allocation for graph KWS_ds_cnn_s_quantCNN is sucessfull, L2: 49685 out of 350000, L3: 0 out of 6388608
------------------------------------------------------------------------------------------------------------------------------------------------
Graph structure:

Node   0, Channel   0  0: GraphEntry __GraphEntry__, Operations: 0
                                   (null) =>                        Input_1
                                   (null) => Dscnnconv_1weights_quantfakequ
                                   (null) =>    Dscnnconv_1conv2d_fold_bias
                                   (null) =>                   S4_Mul_scale
                                   (null) =>                   S4_Mul_shift
                                   (null) =>                       S4_Infos
                                   (null) => Dscnnconv_ds_1dw_convweights_q
                                   (null) => Dscnnconv_ds_1dw_convdepthwise
                                   (null) =>                   S7_Mul_scale
                                   (null) =>                   S7_Mul_shift
                                   (null) =>                       S7_Infos
                                   (null) => Dscnnconv_ds_1pw_convweights_q
                                   (null) => Dscnnconv_ds_1pw_convconv2d_fo
                                   (null) =>                  S10_Mul_scale
                                   (null) =>                  S10_Mul_shift
                                   (null) =>                      S10_Infos
                                   (null) => Dscnnconv_ds_2dw_convweights_q
                                   (null) => Dscnnconv_ds_2dw_convdepthwise
                                   (null) =>                  S13_Mul_scale
                                   (null) =>                  S13_Mul_shift
                                   (null) =>                      S13_Infos
                                   (null) => Dscnnconv_ds_2pw_convweights_q
                                   (null) => Dscnnconv_ds_2pw_convconv2d_fo
                                   (null) =>                  S16_Mul_scale
                                   (null) =>                  S16_Mul_shift
                                   (null) =>                      S16_Infos
                                   (null) => Dscnnconv_ds_3dw_convweights_q
                                   (null) => Dscnnconv_ds_3dw_convdepthwise
                                   (null) =>                  S19_Mul_scale
                                   (null) =>                  S19_Mul_shift
                                   (null) =>                      S19_Infos
                                   (null) => Dscnnconv_ds_3pw_convweights_q
                                   (null) => Dscnnconv_ds_3pw_convconv2d_fo
                                   (null) =>                  S22_Mul_scale
                                   (null) =>                  S22_Mul_shift
                                   (null) =>                      S22_Infos
                                   (null) => Dscnnconv_ds_4dw_convweights_q
                                   (null) => Dscnnconv_ds_4dw_convdepthwise
                                   (null) =>                  S25_Mul_scale
                                   (null) =>                  S25_Mul_shift
                                   (null) =>                      S25_Infos
                                   (null) => Dscnnconv_ds_4pw_convweights_q
                                   (null) => Dscnnconv_ds_4pw_convconv2d_fo
                                   (null) =>                  S28_Mul_scale
                                   (null) =>                  S28_Mul_shift
                                   (null) =>                      S28_Infos
                                   (null) =>                      S29_Infos
                                   (null) => Dscnnfc1weights_quantfakequant
                                   (null) =>            Dscnnfc1matmul_bias
                                   (null) =>                  S32_Mul_scale
                                   (null) =>                  S32_Mul_shift
                                   (null) =>                      S32_Infos
                                   (null) =>                      S33_Infos
	Kernel Memory      : L3:       0, L2:       0
	Kernel Total Memory:       0, L3 moves:       0, L2 moves:       0, Move overhead: 1.000000
	Kernel Operations  :       0 [KernelOper/GraphOper: 0.000000%], Move/Operation ratio: [L3: 0.000000, L2: 0.000000]
	Successors:  2 3 4 5 6 7 8 9 1 10 11 12

    Living Dynamic Symbols: [Input_1] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node   1, Channel   1  8:       UKer S4_Conv2d_64x1x10x4_Relu, Operations: 328000
 I Buff                                In =>                        Input_1    --L2--    Size:     490, L3_Move:         0, L2_Move:       490, TileOverhead: 1.000000, L2Buff:     0, Addr: 0
CI Buff                            Filter => Dscnnconv_1weights_quantfakequ    --L2--    Size:    2560, L3_Move:         0, L2_Move:      2560, TileOverhead: 1.000000, L2Buff:     0, Addr: 876
CI Buff                              Bias =>    Dscnnconv_1conv2d_fold_bias    --L2--    Size:     256, L3_Move:         0, L2_Move:       256, TileOverhead: 1.000000, L2Buff:     0, Addr: 492
 O Buff                               Out =>                      S4_Output    --L2--    Size:    8000, L3_Move:         0, L2_Move:      8000, TileOverhead: 1.000000, L2Buff:     0, Addr: 3436
CI Buff                             Scale =>                   S4_Mul_scale    --L2--    Size:      64, L3_Move:         0, L2_Move:        64, TileOverhead: 1.000000, L2Buff:     0, Addr: 748
CI Buff                            ScaleN =>                   S4_Mul_shift    --L2--    Size:      64, L3_Move:         0, L2_Move:        64, TileOverhead: 1.000000, L2Buff:     0, Addr: 812
CI Buff                             Infos =>                       S4_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 43436
	Kernel Memory      : L3:       0, L2:   11443
	Kernel Total Memory:   11443, L3 moves:       0, L2 moves:   11443, Move overhead: 1.000000
	Kernel Operations  :  328000 [KernelOper/GraphOper: 12.126680%], Move/Operation ratio: [L3: 0.000000, L2: 0.034887]
	Successors:  2

    Living Dynamic Symbols: [Input_1] [S4_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node   2, Channel   0  0:       UKer S7_Conv2d_64x1x3x3_Relu, Operations: 80000
 I                                     In =>                      S4_Output    --L2--    Size:    8000, L3_Move:         0, L2_Move:      8000, TileOverhead: 1.000000, L2Buff:     0, Addr: 0
CI Buff                            Filter => Dscnnconv_ds_1dw_convweights_q    --L2--    Size:     576, L3_Move:         0, L2_Move:       576, TileOverhead: 1.000000, L2Buff:     0, Addr: 10384
CI Buff                              Bias => Dscnnconv_ds_1dw_convdepthwise    --L2--    Size:     256, L3_Move:         0, L2_Move:       256, TileOverhead: 1.000000, L2Buff:     0, Addr: 10000
 O                                    Out =>                      S7_Output    --L2--    Size:    8000, L3_Move:         0, L2_Move:      8000, TileOverhead: 1.000000, L2Buff:     0, Addr: 10960
CI Buff                             Scale =>                   S7_Mul_scale    --L2--    Size:      64, L3_Move:         0, L2_Move:        64, TileOverhead: 1.000000, L2Buff:     0, Addr: 10256
CI Buff                            ScaleN =>                   S7_Mul_shift    --L2--    Size:      64, L3_Move:         0, L2_Move:        64, TileOverhead: 1.000000, L2Buff:     0, Addr: 10320
CI Buff                             Infos =>                       S7_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 40960
	Kernel Memory      : L3:       0, L2:   16969
	Kernel Total Memory:   16969, L3 moves:       0, L2 moves:   16969, Move overhead: 1.000000
	Kernel Operations  :   80000 [KernelOper/GraphOper: 2.957727%], Move/Operation ratio: [L3: 0.000000, L2: 0.212113]
	Successors:  3

    Living Dynamic Symbols: [S4_Output] [S7_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node   3, Channel   0  0:       UKer S10_Conv2d_64x64x1x1_Relu, Operations: 512000
 I Buff                               In2 =>                      S7_Output    --L2--    Size:    8000, L3_Move:         0, L2_Move:      8000, TileOverhead: 1.000000, L2Buff:     0, Addr: 256
CI Buff                               In1 => Dscnnconv_ds_1pw_convweights_q    --L2--    Size:    4096, L3_Move:         0, L2_Move:      4096, TileOverhead: 1.000000, L2Buff:     0, Addr: 16268
CI Buff                              Bias => Dscnnconv_ds_1pw_convconv2d_fo    --L2--    Size:     256, L3_Move:         0, L2_Move:       256, TileOverhead: 1.000000, L2Buff:     0, Addr: 20364
 O Buff                               Out =>                     S10_Output    --L2--    Size:    8000, L3_Move:         0, L2_Move:      8000, TileOverhead: 1.000000, L2Buff:     0, Addr: 8256
CI Buff                             Scale =>                  S10_Mul_scale    --L2--    Size:      64, L3_Move:         0, L2_Move:        64, TileOverhead: 1.000000, L2Buff:     0, Addr: 20620
CI Buff                            ScaleN =>                  S10_Mul_shift    --L2--    Size:      64, L3_Move:         0, L2_Move:        64, TileOverhead: 1.000000, L2Buff:     0, Addr: 20684
CI Buff                             Infos =>                      S10_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 16256
	Kernel Memory      : L3:       0, L2:   20489
	Kernel Total Memory:   20489, L3 moves:       0, L2 moves:   20489, Move overhead: 1.000000
	Kernel Operations  :  512000 [KernelOper/GraphOper: 18.929450%], Move/Operation ratio: [L3: 0.000000, L2: 0.040018]
	Successors:  4

    Living Dynamic Symbols: [S7_Output] [S10_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node   4, Channel   0  0:       UKer S13_Conv2d_64x1x3x3_Relu, Operations: 80000
 I                                     In =>                     S10_Output    --L2--    Size:    8000, L3_Move:         0, L2_Move:      8000, TileOverhead: 1.000000, L2Buff:     0, Addr: 0
CI Buff                            Filter => Dscnnconv_ds_2dw_convweights_q    --L2--    Size:     576, L3_Move:         0, L2_Move:       576, TileOverhead: 1.000000, L2Buff:     0, Addr: 10384
CI Buff                              Bias => Dscnnconv_ds_2dw_convdepthwise    --L2--    Size:     256, L3_Move:         0, L2_Move:       256, TileOverhead: 1.000000, L2Buff:     0, Addr: 10000
 O                                    Out =>                     S13_Output    --L2--    Size:    8000, L3_Move:         0, L2_Move:      8000, TileOverhead: 1.000000, L2Buff:     0, Addr: 10960
CI Buff                             Scale =>                  S13_Mul_scale    --L2--    Size:      64, L3_Move:         0, L2_Move:        64, TileOverhead: 1.000000, L2Buff:     0, Addr: 10256
CI Buff                            ScaleN =>                  S13_Mul_shift    --L2--    Size:      64, L3_Move:         0, L2_Move:        64, TileOverhead: 1.000000, L2Buff:     0, Addr: 10320
CI Buff                             Infos =>                      S13_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 40960
	Kernel Memory      : L3:       0, L2:   16969
	Kernel Total Memory:   16969, L3 moves:       0, L2 moves:   16969, Move overhead: 1.000000
	Kernel Operations  :   80000 [KernelOper/GraphOper: 2.957727%], Move/Operation ratio: [L3: 0.000000, L2: 0.212113]
	Successors:  5

    Living Dynamic Symbols: [S10_Output] [S13_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node   5, Channel   0  0:       UKer S16_Conv2d_64x64x1x1_Relu, Operations: 512000
 I Buff                               In2 =>                     S13_Output    --L2--    Size:    8000, L3_Move:         0, L2_Move:      8000, TileOverhead: 1.000000, L2Buff:     0, Addr: 256
CI Buff                               In1 => Dscnnconv_ds_2pw_convweights_q    --L2--    Size:    4096, L3_Move:         0, L2_Move:      4096, TileOverhead: 1.000000, L2Buff:     0, Addr: 16268
CI Buff                              Bias => Dscnnconv_ds_2pw_convconv2d_fo    --L2--    Size:     256, L3_Move:         0, L2_Move:       256, TileOverhead: 1.000000, L2Buff:     0, Addr: 20364
 O Buff                               Out =>                     S16_Output    --L2--    Size:    8000, L3_Move:         0, L2_Move:      8000, TileOverhead: 1.000000, L2Buff:     0, Addr: 8256
CI Buff                             Scale =>                  S16_Mul_scale    --L2--    Size:      64, L3_Move:         0, L2_Move:        64, TileOverhead: 1.000000, L2Buff:     0, Addr: 20620
CI Buff                            ScaleN =>                  S16_Mul_shift    --L2--    Size:      64, L3_Move:         0, L2_Move:        64, TileOverhead: 1.000000, L2Buff:     0, Addr: 20684
CI Buff                             Infos =>                      S16_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 16256
	Kernel Memory      : L3:       0, L2:   20489
	Kernel Total Memory:   20489, L3 moves:       0, L2 moves:   20489, Move overhead: 1.000000
	Kernel Operations  :  512000 [KernelOper/GraphOper: 18.929450%], Move/Operation ratio: [L3: 0.000000, L2: 0.040018]
	Successors:  6

    Living Dynamic Symbols: [S13_Output] [S16_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node   6, Channel   0  0:       UKer S19_Conv2d_64x1x3x3_Relu, Operations: 80000
 I                                     In =>                     S16_Output    --L2--    Size:    8000, L3_Move:         0, L2_Move:      8000, TileOverhead: 1.000000, L2Buff:     0, Addr: 0
CI Buff                            Filter => Dscnnconv_ds_3dw_convweights_q    --L2--    Size:     576, L3_Move:         0, L2_Move:       576, TileOverhead: 1.000000, L2Buff:     0, Addr: 10384
CI Buff                              Bias => Dscnnconv_ds_3dw_convdepthwise    --L2--    Size:     256, L3_Move:         0, L2_Move:       256, TileOverhead: 1.000000, L2Buff:     0, Addr: 10000
 O                                    Out =>                     S19_Output    --L2--    Size:    8000, L3_Move:         0, L2_Move:      8000, TileOverhead: 1.000000, L2Buff:     0, Addr: 10960
CI Buff                             Scale =>                  S19_Mul_scale    --L2--    Size:      64, L3_Move:         0, L2_Move:        64, TileOverhead: 1.000000, L2Buff:     0, Addr: 10256
CI Buff                            ScaleN =>                  S19_Mul_shift    --L2--    Size:      64, L3_Move:         0, L2_Move:        64, TileOverhead: 1.000000, L2Buff:     0, Addr: 10320
CI Buff                             Infos =>                      S19_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 40960
	Kernel Memory      : L3:       0, L2:   16969
	Kernel Total Memory:   16969, L3 moves:       0, L2 moves:   16969, Move overhead: 1.000000
	Kernel Operations  :   80000 [KernelOper/GraphOper: 2.957727%], Move/Operation ratio: [L3: 0.000000, L2: 0.212113]
	Successors:  7

    Living Dynamic Symbols: [S16_Output] [S19_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node   7, Channel   0  0:       UKer S22_Conv2d_64x64x1x1_Relu, Operations: 512000
 I Buff                               In2 =>                     S19_Output    --L2--    Size:    8000, L3_Move:         0, L2_Move:      8000, TileOverhead: 1.000000, L2Buff:     0, Addr: 256
CI Buff                               In1 => Dscnnconv_ds_3pw_convweights_q    --L2--    Size:    4096, L3_Move:         0, L2_Move:      4096, TileOverhead: 1.000000, L2Buff:     0, Addr: 16268
CI Buff                              Bias => Dscnnconv_ds_3pw_convconv2d_fo    --L2--    Size:     256, L3_Move:         0, L2_Move:       256, TileOverhead: 1.000000, L2Buff:     0, Addr: 20364
 O Buff                               Out =>                     S22_Output    --L2--    Size:    8000, L3_Move:         0, L2_Move:      8000, TileOverhead: 1.000000, L2Buff:     0, Addr: 8256
CI Buff                             Scale =>                  S22_Mul_scale    --L2--    Size:      64, L3_Move:         0, L2_Move:        64, TileOverhead: 1.000000, L2Buff:     0, Addr: 20620
CI Buff                            ScaleN =>                  S22_Mul_shift    --L2--    Size:      64, L3_Move:         0, L2_Move:        64, TileOverhead: 1.000000, L2Buff:     0, Addr: 20684
CI Buff                             Infos =>                      S22_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 16256
	Kernel Memory      : L3:       0, L2:   20489
	Kernel Total Memory:   20489, L3 moves:       0, L2 moves:   20489, Move overhead: 1.000000
	Kernel Operations  :  512000 [KernelOper/GraphOper: 18.929450%], Move/Operation ratio: [L3: 0.000000, L2: 0.040018]
	Successors:  8

    Living Dynamic Symbols: [S19_Output] [S22_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node   8, Channel   0  0:       UKer S25_Conv2d_64x1x3x3_Relu, Operations: 80000
 I                                     In =>                     S22_Output    --L2--    Size:    8000, L3_Move:         0, L2_Move:      8000, TileOverhead: 1.000000, L2Buff:     0, Addr: 0
CI Buff                            Filter => Dscnnconv_ds_4dw_convweights_q    --L2--    Size:     576, L3_Move:         0, L2_Move:       576, TileOverhead: 1.000000, L2Buff:     0, Addr: 10384
CI Buff                              Bias => Dscnnconv_ds_4dw_convdepthwise    --L2--    Size:     256, L3_Move:         0, L2_Move:       256, TileOverhead: 1.000000, L2Buff:     0, Addr: 10000
 O                                    Out =>                     S25_Output    --L2--    Size:    8000, L3_Move:         0, L2_Move:      8000, TileOverhead: 1.000000, L2Buff:     0, Addr: 10960
CI Buff                             Scale =>                  S25_Mul_scale    --L2--    Size:      64, L3_Move:         0, L2_Move:        64, TileOverhead: 1.000000, L2Buff:     0, Addr: 10256
CI Buff                            ScaleN =>                  S25_Mul_shift    --L2--    Size:      64, L3_Move:         0, L2_Move:        64, TileOverhead: 1.000000, L2Buff:     0, Addr: 10320
CI Buff                             Infos =>                      S25_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 40960
	Kernel Memory      : L3:       0, L2:   16969
	Kernel Total Memory:   16969, L3 moves:       0, L2 moves:   16969, Move overhead: 1.000000
	Kernel Operations  :   80000 [KernelOper/GraphOper: 2.957727%], Move/Operation ratio: [L3: 0.000000, L2: 0.212113]
	Successors:  9

    Living Dynamic Symbols: [S22_Output] [S25_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node   9, Channel   0  0:       UKer S28_Conv2d_64x64x1x1_Relu, Operations: 512000
 I Buff                               In2 =>                     S25_Output    --L2--    Size:    8000, L3_Move:         0, L2_Move:      8000, TileOverhead: 1.000000, L2Buff:     0, Addr: 256
CI Buff                               In1 => Dscnnconv_ds_4pw_convweights_q    --L2--    Size:    4096, L3_Move:         0, L2_Move:      4096, TileOverhead: 1.000000, L2Buff:     0, Addr: 16268
CI Buff                              Bias => Dscnnconv_ds_4pw_convconv2d_fo    --L2--    Size:     256, L3_Move:         0, L2_Move:       256, TileOverhead: 1.000000, L2Buff:     0, Addr: 20364
 O Buff                               Out =>                     S28_Output    --L2--    Size:    8000, L3_Move:         0, L2_Move:      8000, TileOverhead: 1.000000, L2Buff:     0, Addr: 8256
CI Buff                             Scale =>                  S28_Mul_scale    --L2--    Size:      64, L3_Move:         0, L2_Move:        64, TileOverhead: 1.000000, L2Buff:     0, Addr: 20620
CI Buff                            ScaleN =>                  S28_Mul_shift    --L2--    Size:      64, L3_Move:         0, L2_Move:        64, TileOverhead: 1.000000, L2Buff:     0, Addr: 20684
CI Buff                             Infos =>                      S28_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 16256
	Kernel Memory      : L3:       0, L2:   20489
	Kernel Total Memory:   20489, L3 moves:       0, L2 moves:   20489, Move overhead: 1.000000
	Kernel Operations  :  512000 [KernelOper/GraphOper: 18.929450%], Move/Operation ratio: [L3: 0.000000, L2: 0.040018]
	Successors:  10

    Living Dynamic Symbols: [S25_Output] [S28_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node  10, Channel   0  0:       UKer S29_AveragePool_25x5, Operations: 8000
 I Buff                                In =>                     S28_Output    --L2--    Size:    8000, L3_Move:         0, L2_Move:      8000, TileOverhead: 1.000000, L2Buff:     0, Addr: 0
 O Buff                               Out =>                     S29_Output    --L2--    Size:      64, L3_Move:         0, L2_Move:        64, TileOverhead: 1.000000, L2Buff:     0, Addr: 8000
CI Buff                             Infos =>                      S29_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 8064
	Kernel Memory      : L3:       0, L2:    8073
	Kernel Total Memory:    8073, L3 moves:       0, L2 moves:    8073, Move overhead: 1.000000
	Kernel Operations  :    8000 [KernelOper/GraphOper: 0.295773%], Move/Operation ratio: [L3: 0.000000, L2: 1.009125]
	Successors:  11

    Living Dynamic Symbols: [S28_Output] [S29_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node  11, Channel   0  0:       UKer S32_Linear_12x64x1x1, Operations: 768
 I Buff                                In =>                     S29_Output    --L2--    Size:      64, L3_Move:         0, L2_Move:        64, TileOverhead: 1.000000, L2Buff:     0, Addr: 0
CI Buff                            Filter => Dscnnfc1weights_quantfakequant    --L2--    Size:     768, L3_Move:         0, L2_Move:       768, TileOverhead: 1.000000, L2Buff:     0, Addr: 64
CI Buff                              Bias =>            Dscnnfc1matmul_bias    --L2--    Size:      48, L3_Move:         0, L2_Move:        48, TileOverhead: 1.000000, L2Buff:     0, Addr: 832
 O Buff                               Out =>                     S32_Output    --L2--    Size:      12, L3_Move:         0, L2_Move:        12, TileOverhead: 1.000000, L2Buff:     0, Addr: 880
CI Buff                             Scale =>                  S32_Mul_scale    --L2--    Size:      12, L3_Move:         0, L2_Move:        12, TileOverhead: 1.000000, L2Buff:     0, Addr: 892
CI Buff                            ScaleN =>                  S32_Mul_shift    --L2--    Size:      12, L3_Move:         0, L2_Move:        12, TileOverhead: 1.000000, L2Buff:     0, Addr: 904
CI Buff                             Infos =>                      S32_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 916
	Kernel Memory      : L3:       0, L2:     925
	Kernel Total Memory:     925, L3 moves:       0, L2 moves:     925, Move overhead: 1.000000
	Kernel Operations  :     768 [KernelOper/GraphOper: 0.028394%], Move/Operation ratio: [L3: 0.000000, L2: 1.204427]
	Successors:  12

    Living Dynamic Symbols: [S29_Output] [S32_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node  12, Channel   0  0:       UKer S33_SoftMax, Operations: 12
 I Buff                                In =>                     S32_Output    --L2--    Size:      12, L3_Move:         0, L2_Move:        12, TileOverhead: 1.000000, L2Buff:     0, Addr: 0
 O Buff                               Out =>                       Output_1    --L2--    Size:      24, L3_Move:         0, L2_Move:        24, TileOverhead: 1.000000, L2Buff:     0, Addr: 12
CI Buff                             Infos =>                      S33_Infos    --L2--    Size:       9, L3_Move:         0, L2_Move:         9, TileOverhead: 1.000000, L2Buff:     0, Addr: 36
	Kernel Memory      : L3:       0, L2:      45
	Kernel Total Memory:      45, L3 moves:       0, L2 moves:      45, Move overhead: 1.000000
	Kernel Operations  :      12 [KernelOper/GraphOper: 0.000444%], Move/Operation ratio: [L3: 0.000000, L2: 3.750000]
	Successors:  13

    Living Dynamic Symbols: [Output_1] [S32_Output] 

------------------------------------------------------------------------------------------------------------------------------------------------
Node  13, Channel   0  0:  GraphExit __GraphExit__, Operations: 0
                                   (null) =>                       Output_1
	Kernel Memory      : L3:       0, L2:       0
	Kernel Total Memory:       0, L3 moves:       0, L2 moves:       0, Move overhead: 1.000000
	Kernel Operations  :       0 [KernelOper/GraphOper: 0.000000%], Move/Operation ratio: [L3: 0.000000, L2: 0.000000]
	Successors: 

    Living Dynamic Symbols: [Output_1] 

------------------------------------------------------------------------------------------------------------------------------------------------
	Graph nodes max local memory : L3:       0, L2:   20489
	Graph nodes min global memory: L3:       0, L2:   20492
	Graph sum of kernel arguments size:  170318, L3 moves:       0, L2 moves:  170318, Move overhead: 1.000000
	Graph total operations: 2704780


------------------------------------------------------------------------------------------------------------------------------------------------

------------------------------------------------------------------------------------------------------------------------------------------------
Memory bandwidth report:

    Sum of All Kernel's arguments size:  170318, Total L3_Move:         0, Total L2_Move:    170318, Tiling Overhead Average: 1.000000

------------------------------------------------------------------------------------------------------------------------------------------------

------------------------------------------------------------------------------------------------------------------------------------------------
Total minimum memory requirement report:

                     L3 Memory       L2 Memory
       Dynamic               0           16000
         Const               0            4492
         Total               0           20492
------------------------------------------------------------------------------------------------------------------------------------------------

------------------------------------------------------------------------------------------------------------------------------------------------
Graph symbols allocation:

              Input_1  Externally allocated
 Dscnnconv_1weights_quantfakequ  INSTALL: HyperFlash[  0: 13]@  16384    LOAD:         L2[  0: 13]@  16384    EXEC:         L2[  0: 13]@  16384 , Size:    2560
 Dscnnconv_1conv2d_fold_bias  INSTALL: HyperFlash[  0: 13]@  22016    LOAD:         L2[  0: 13]@  22016    EXEC:         L2[  0: 13]@  22016 , Size:     256
         S4_Mul_scale  INSTALL: HyperFlash[  0: 13]@  24320    LOAD:         L2[  0: 13]@  24320    EXEC:         L2[  0: 13]@  24320 , Size:      64
         S4_Mul_shift  INSTALL: HyperFlash[  0: 13]@  24384    LOAD:         L2[  0: 13]@  24384    EXEC:         L2[  0: 13]@  24384 , Size:      64
             S4_Infos  INSTALL: HyperFlash[  0: 13]@  25520    LOAD:         L2[  0: 13]@  25520    EXEC:         L2[  0: 13]@  25520 , Size:       9
 Dscnnconv_ds_1dw_convweights_q  INSTALL: HyperFlash[  0: 13]@  19712    LOAD:         L2[  0: 13]@  19712    EXEC:         L2[  0: 13]@  19712 , Size:     576
 Dscnnconv_ds_1dw_convdepthwise  INSTALL: HyperFlash[  0: 13]@  22272    LOAD:         L2[  0: 13]@  22272    EXEC:         L2[  0: 13]@  22272 , Size:     256
         S7_Mul_scale  INSTALL: HyperFlash[  0: 13]@  24448    LOAD:         L2[  0: 13]@  24448    EXEC:         L2[  0: 13]@  24448 , Size:      64
         S7_Mul_shift  INSTALL: HyperFlash[  0: 13]@  24512    LOAD:         L2[  0: 13]@  24512    EXEC:         L2[  0: 13]@  24512 , Size:      64
             S7_Infos  INSTALL: HyperFlash[  0: 13]@  25532    LOAD:         L2[  0: 13]@  25532    EXEC:         L2[  0: 13]@  25532 , Size:       9
 Dscnnconv_ds_1pw_convweights_q  INSTALL: HyperFlash[  0: 13]@      0    LOAD:         L2[  0: 13]@      0    EXEC:         L2[  0: 13]@      0 , Size:    4096
 Dscnnconv_ds_1pw_convconv2d_fo  INSTALL: HyperFlash[  0: 13]@  22528    LOAD:         L2[  0: 13]@  22528    EXEC:         L2[  0: 13]@  22528 , Size:     256
        S10_Mul_scale  INSTALL: HyperFlash[  0: 13]@  24576    LOAD:         L2[  0: 13]@  24576    EXEC:         L2[  0: 13]@  24576 , Size:      64
        S10_Mul_shift  INSTALL: HyperFlash[  0: 13]@  24640    LOAD:         L2[  0: 13]@  24640    EXEC:         L2[  0: 13]@  24640 , Size:      64
            S10_Infos  INSTALL: HyperFlash[  0: 13]@  25544    LOAD:         L2[  0: 13]@  25544    EXEC:         L2[  0: 13]@  25544 , Size:       9
 Dscnnconv_ds_2dw_convweights_q  INSTALL: HyperFlash[  0: 13]@  20288    LOAD:         L2[  0: 13]@  20288    EXEC:         L2[  0: 13]@  20288 , Size:     576
 Dscnnconv_ds_2dw_convdepthwise  INSTALL: HyperFlash[  0: 13]@  22784    LOAD:         L2[  0: 13]@  22784    EXEC:         L2[  0: 13]@  22784 , Size:     256
        S13_Mul_scale  INSTALL: HyperFlash[  0: 13]@  24704    LOAD:         L2[  0: 13]@  24704    EXEC:         L2[  0: 13]@  24704 , Size:      64
        S13_Mul_shift  INSTALL: HyperFlash[  0: 13]@  24768    LOAD:         L2[  0: 13]@  24768    EXEC:         L2[  0: 13]@  24768 , Size:      64
            S13_Infos  INSTALL: HyperFlash[  0: 13]@  25556    LOAD:         L2[  0: 13]@  25556    EXEC:         L2[  0: 13]@  25556 , Size:       9
 Dscnnconv_ds_2pw_convweights_q  INSTALL: HyperFlash[  0: 13]@   4096    LOAD:         L2[  0: 13]@   4096    EXEC:         L2[  0: 13]@   4096 , Size:    4096
 Dscnnconv_ds_2pw_convconv2d_fo  INSTALL: HyperFlash[  0: 13]@  23040    LOAD:         L2[  0: 13]@  23040    EXEC:         L2[  0: 13]@  23040 , Size:     256
        S16_Mul_scale  INSTALL: HyperFlash[  0: 13]@  24832    LOAD:         L2[  0: 13]@  24832    EXEC:         L2[  0: 13]@  24832 , Size:      64
        S16_Mul_shift  INSTALL: HyperFlash[  0: 13]@  24896    LOAD:         L2[  0: 13]@  24896    EXEC:         L2[  0: 13]@  24896 , Size:      64
            S16_Infos  INSTALL: HyperFlash[  0: 13]@  25568    LOAD:         L2[  0: 13]@  25568    EXEC:         L2[  0: 13]@  25568 , Size:       9
 Dscnnconv_ds_3dw_convweights_q  INSTALL: HyperFlash[  0: 13]@  20864    LOAD:         L2[  0: 13]@  20864    EXEC:         L2[  0: 13]@  20864 , Size:     576
 Dscnnconv_ds_3dw_convdepthwise  INSTALL: HyperFlash[  0: 13]@  23296    LOAD:         L2[  0: 13]@  23296    EXEC:         L2[  0: 13]@  23296 , Size:     256
        S19_Mul_scale  INSTALL: HyperFlash[  0: 13]@  24960    LOAD:         L2[  0: 13]@  24960    EXEC:         L2[  0: 13]@  24960 , Size:      64
        S19_Mul_shift  INSTALL: HyperFlash[  0: 13]@  25024    LOAD:         L2[  0: 13]@  25024    EXEC:         L2[  0: 13]@  25024 , Size:      64
            S19_Infos  INSTALL: HyperFlash[  0: 13]@  25580    LOAD:         L2[  0: 13]@  25580    EXEC:         L2[  0: 13]@  25580 , Size:       9
 Dscnnconv_ds_3pw_convweights_q  INSTALL: HyperFlash[  0: 13]@   8192    LOAD:         L2[  0: 13]@   8192    EXEC:         L2[  0: 13]@   8192 , Size:    4096
 Dscnnconv_ds_3pw_convconv2d_fo  INSTALL: HyperFlash[  0: 13]@  23552    LOAD:         L2[  0: 13]@  23552    EXEC:         L2[  0: 13]@  23552 , Size:     256
        S22_Mul_scale  INSTALL: HyperFlash[  0: 13]@  25088    LOAD:         L2[  0: 13]@  25088    EXEC:         L2[  0: 13]@  25088 , Size:      64
        S22_Mul_shift  INSTALL: HyperFlash[  0: 13]@  25152    LOAD:         L2[  0: 13]@  25152    EXEC:         L2[  0: 13]@  25152 , Size:      64
            S22_Infos  INSTALL: HyperFlash[  0: 13]@  25592    LOAD:         L2[  0: 13]@  25592    EXEC:         L2[  0: 13]@  25592 , Size:       9
 Dscnnconv_ds_4dw_convweights_q  INSTALL: HyperFlash[  0: 13]@  21440    LOAD:         L2[  0: 13]@  21440    EXEC:         L2[  0: 13]@  21440 , Size:     576
 Dscnnconv_ds_4dw_convdepthwise  INSTALL: HyperFlash[  0: 13]@  23808    LOAD:         L2[  0: 13]@  23808    EXEC:         L2[  0: 13]@  23808 , Size:     256
        S25_Mul_scale  INSTALL: HyperFlash[  0: 13]@  25216    LOAD:         L2[  0: 13]@  25216    EXEC:         L2[  0: 13]@  25216 , Size:      64
        S25_Mul_shift  INSTALL: HyperFlash[  0: 13]@  25280    LOAD:         L2[  0: 13]@  25280    EXEC:         L2[  0: 13]@  25280 , Size:      64
            S25_Infos  INSTALL: HyperFlash[  0: 13]@  25604    LOAD:         L2[  0: 13]@  25604    EXEC:         L2[  0: 13]@  25604 , Size:       9
 Dscnnconv_ds_4pw_convweights_q  INSTALL: HyperFlash[  0: 13]@  12288    LOAD:         L2[  0: 13]@  12288    EXEC:         L2[  0: 13]@  12288 , Size:    4096
 Dscnnconv_ds_4pw_convconv2d_fo  INSTALL: HyperFlash[  0: 13]@  24064    LOAD:         L2[  0: 13]@  24064    EXEC:         L2[  0: 13]@  24064 , Size:     256
        S28_Mul_scale  INSTALL: HyperFlash[  0: 13]@  25344    LOAD:         L2[  0: 13]@  25344    EXEC:         L2[  0: 13]@  25344 , Size:      64
        S28_Mul_shift  INSTALL: HyperFlash[  0: 13]@  25408    LOAD:         L2[  0: 13]@  25408    EXEC:         L2[  0: 13]@  25408 , Size:      64
            S28_Infos  INSTALL: HyperFlash[  0: 13]@  25616    LOAD:         L2[  0: 13]@  25616    EXEC:         L2[  0: 13]@  25616 , Size:       9
            S29_Infos  INSTALL: HyperFlash[  0: 13]@  25628    LOAD:         L2[  0: 13]@  25628    EXEC:         L2[  0: 13]@  25628 , Size:       9
 Dscnnfc1weights_quantfakequant  INSTALL: HyperFlash[  0: 13]@  18944    LOAD:         L2[  0: 13]@  18944    EXEC:         L2[  0: 13]@  18944 , Size:     768
  Dscnnfc1matmul_bias  INSTALL: HyperFlash[  0: 13]@  25472    LOAD:         L2[  0: 13]@  25472    EXEC:         L2[  0: 13]@  25472 , Size:      48
        S32_Mul_scale  INSTALL: HyperFlash[  0: 13]@  25640    LOAD:         L2[  0: 13]@  25640    EXEC:         L2[  0: 13]@  25640 , Size:      12
        S32_Mul_shift  INSTALL: HyperFlash[  0: 13]@  25652    LOAD:         L2[  0: 13]@  25652    EXEC:         L2[  0: 13]@  25652 , Size:      12
            S32_Infos  INSTALL: HyperFlash[  0: 13]@  25664    LOAD:         L2[  0: 13]@  25664    EXEC:         L2[  0: 13]@  25664 , Size:       9
            S33_Infos  INSTALL: HyperFlash[  0: 13]@  25676    LOAD:         L2[  0: 13]@  25676    EXEC:         L2[  0: 13]@  25676 , Size:       9
             Output_1  Externally allocated
            S4_Output     EXEC:         L2[  1:  2]@  33688 , Size:    8000
            S7_Output     EXEC:         L2[  2:  3]@  25688 , Size:    8000
           S10_Output     EXEC:         L2[  3:  4]@  33688 , Size:    8000
           S13_Output     EXEC:         L2[  4:  5]@  25688 , Size:    8000
           S16_Output     EXEC:         L2[  5:  6]@  33688 , Size:    8000
           S19_Output     EXEC:         L2[  6:  7]@  25688 , Size:    8000
           S22_Output     EXEC:         L2[  7:  8]@  33688 , Size:    8000
           S25_Output     EXEC:         L2[  8:  9]@  41688 , Size:    8000
           S28_Output     EXEC:         L2[  9: 10]@  25688 , Size:    8000
           S29_Output     EXEC:         L2[ 10: 11]@  33688 , Size:      64
           S32_Output     EXEC:         L2[ 11: 12]@  25688 , Size:      12
------------------------------------------------------------------------------------------------------------------------------------------------

------------------------------------------------------------------------------------------------------------------------------------------------
Graph stacked tensors
------------------------------------------------------------------------------------------------------------------------------------------------

Generating Code For User Kernel:            S4_Conv2d_64x1x10x4_Relu
Generating Code For User Kernel:             S7_Conv2d_64x1x3x3_Relu
Generating Code For User Kernel:           S10_Conv2d_64x64x1x1_Relu
Generating Code For User Kernel:            S13_Conv2d_64x1x3x3_Relu
Generating Code For User Kernel:           S16_Conv2d_64x64x1x1_Relu
Generating Code For User Kernel:            S19_Conv2d_64x1x3x3_Relu
Generating Code For User Kernel:           S22_Conv2d_64x64x1x1_Relu
Generating Code For User Kernel:            S25_Conv2d_64x1x3x3_Relu
Generating Code For User Kernel:           S28_Conv2d_64x64x1x1_Relu
Generating Code For User Kernel:                S29_AveragePool_25x5
Generating Code For User Kernel:                S32_Linear_12x64x1x1
Generating Code For User Kernel:                         S33_SoftMax
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_1weights_quantfakequ.tensor: 2560 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_1conv2d_fold_bias.tensor: 64 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S4_Mul_scale.tensor: 64 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S4_Mul_shift.tensor: 64 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S4_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_1dw_convweights_q.tensor: 576 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_1dw_convdepthwise.tensor: 64 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S7_Mul_scale.tensor: 64 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S7_Mul_shift.tensor: 64 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S7_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_1pw_convweights_q.tensor: 4096 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_1pw_convconv2d_fo.tensor: 64 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S10_Mul_scale.tensor: 64 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S10_Mul_shift.tensor: 64 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S10_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_2dw_convweights_q.tensor: 576 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_2dw_convdepthwise.tensor: 64 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S13_Mul_scale.tensor: 64 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S13_Mul_shift.tensor: 64 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S13_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_2pw_convweights_q.tensor: 4096 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_2pw_convconv2d_fo.tensor: 64 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S16_Mul_scale.tensor: 64 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S16_Mul_shift.tensor: 64 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S16_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_3dw_convweights_q.tensor: 576 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_3dw_convdepthwise.tensor: 64 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S19_Mul_scale.tensor: 64 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S19_Mul_shift.tensor: 64 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S19_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_3pw_convweights_q.tensor: 4096 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_3pw_convconv2d_fo.tensor: 64 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S22_Mul_scale.tensor: 64 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S22_Mul_shift.tensor: 64 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S22_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_4dw_convweights_q.tensor: 576 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_4dw_convdepthwise.tensor: 64 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S25_Mul_scale.tensor: 64 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S25_Mul_shift.tensor: 64 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S25_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_4pw_convweights_q.tensor: 4096 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnconv_ds_4pw_convconv2d_fo.tensor: 64 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S28_Mul_scale.tensor: 64 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S28_Mul_shift.tensor: 64 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S28_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S29_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnfc1weights_quantfakequant.tensor: 768 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/Dscnnfc1matmul_bias.tensor: 12 Word items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S32_Mul_scale.tensor: 12 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S32_Mul_shift.tensor: 12 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S32_Infos.tensor: 9 Byte items
Loading coefficient file ./BUILD_MODEL_SQ8_EMUL/tensors/S33_Infos.tensor: 9 Byte items
Flash image KWS_ds_cnn_s_quant_L3_Flash_Const.dat (size 25688) for device AT_MEM_L3_HFLASH successfuly generated

Shared L1 Memory size (Bytes)             : Given:      48736, Used:      43448
L2 Memory size (Bytes)                    : Given:     350000, Used:      49685
L3 Memory size (Bytes)                    : Given:    6388608, Used:          0

L3 Memory bandwidth for 1 graph run       :          0 Bytes
L2 Memory bandwidth for 1 graph run       :     170318 Bytes
Sum of all Kernels arguments size         :     170318 Bytes
Tiling Bandwith overhead                  :   1.000000 Move/KerArgSize
Sum of baseline bandwidth                 :    4248224 Bytes
Percentage of baseline BW for L2          :    4.00916 %
Percentage of baseline BW for L3          :          0 %
Sum of all Kernels operations             :    2704780 Operations
Total amount of flash coefficients        :      25688 Bytes

Basic kernels library                     : CNN_BasicKernels_SQ8.h
                                          : KWS_ds_cnn_s_quant.h
Output Directory                          : BUILD_MODEL_SQ8_EMUL

The following files have been generated:
	   KWS_ds_cnn_s_quantKernels.c Generated C code for the user kernels and the user kernels groups
	   KWS_ds_cnn_s_quantKernels.h Header file for the generated C code
	KWS_ds_cnn_s_quant_L3_Flash_Const.dat Flash content for Graph constants
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c main_emulation.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL/main_emulation.d -o BUILD_EMUL/main_emulation.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_s_quantKernels.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL/BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_s_quantKernels.d -o BUILD_EMUL/BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_s_quantKernels.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.d -o BUILD_EMUL//home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries/SSD_BasicKernels.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries/SSD_BasicKernels.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries/SSD_BasicKernels.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes/ResizeBasicKernels.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes/ResizeBasicKernels.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes/ResizeBasicKernels.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries/CNN_CopyBasicKernels.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries/CNN_CopyBasicKernels.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries/CNN_CopyBasicKernels.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_AT_Misc.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_AT_Misc.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_AT_Misc.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/math_funcs.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/math_funcs.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/math_funcs.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Activation_SQ8.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Activation_SQ8.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Activation_SQ8.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Activation_HWC_SQ8.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Activation_HWC_SQ8.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Activation_HWC_SQ8.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Bias_Linear_SQ8.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Bias_Linear_SQ8.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Bias_Linear_SQ8.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Conv_SQ8.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Conv_SQ8.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Conv_SQ8.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Pooling_SQ8.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Pooling_SQ8.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Pooling_SQ8.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Conv_DW_SQ8.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Conv_DW_SQ8.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Conv_DW_SQ8.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_MatAlgebra_SQ8.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_MatAlgebra_SQ8.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_MatAlgebra_SQ8.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_SoftMax_SQ8.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_SoftMax_SQ8.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_SoftMax_SQ8.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/RNN_SQ8.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/RNN_SQ8.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/RNN_SQ8.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/gap_sdk/libs/gap_lib/wav_io/wavIO.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/gap_sdk/libs/gap_lib/wav_io/wavIO.d -o BUILD_EMUL//home/marco-gwt/GWT/gap_sdk/libs/gap_lib/wav_io/wavIO.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL/MFCCKernels.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL/MFCCKernels.d -o BUILD_EMUL//home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL/MFCCKernels.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/TwiddlesDef.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/TwiddlesDef.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/TwiddlesDef.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/RFFTTwiddlesDef.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/RFFTTwiddlesDef.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/RFFTTwiddlesDef.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/SwapTablesDef.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/SwapTablesDef.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/SwapTablesDef.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/MfccBasicKernels.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/MfccBasicKernels.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/MfccBasicKernels.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/FFT_Library.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/FFT_Library.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/FFT_Library.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/CmplxFunctions.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/CmplxFunctions.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/CmplxFunctions.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -c /home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/PreProcessing.c -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -MD -MF BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/PreProcessing.d -o BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/PreProcessing.o
gcc -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -MMD -MP -DSMALL -DWITH_MFCC -g -O0 -D__EMUL__ -DAT_INPUT_HEIGHT=49 -DAT_INPUT_WIDTH=10 -DAT_INPUT_COLORS= -I. -I/home/marco-gwt/GWT/AutotilerV2/Emulation -I/home/marco-gwt/GWT/AutotilerV2/Autotiler -I/home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries -I/home/marco-gwt/GWT/AutotilerV2/DSP_Libraries -I/home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8 -IBUILD_MODEL_SQ8_EMUL -I/home/marco-gwt/GWT/gap_sdk/libs/gap_lib/include -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -I/home/marco-gwt/GWT/AutotilerV2/DSP_Generators -I/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL -o kws_ds_cnn_emul  BUILD_EMUL/main_emulation.o  BUILD_EMUL/BUILD_MODEL_SQ8_EMUL/KWS_ds_cnn_s_quantKernels.o  BUILD_EMUL//home/marco-gwt/GWT/gap_sdk/libs/gap_lib/img_io/ImgIO.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries/SSD_BasicKernels.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/Generators/BilinearResizes/ResizeBasicKernels.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries/CNN_CopyBasicKernels.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_AT_Misc.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/math_funcs.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Activation_SQ8.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Activation_HWC_SQ8.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Bias_Linear_SQ8.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Conv_SQ8.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Pooling_SQ8.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_Conv_DW_SQ8.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_MatAlgebra_SQ8.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/CNN_SoftMax_SQ8.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/CNN_Libraries_SQ8/RNN_SQ8.o  BUILD_EMUL//home/marco-gwt/GWT/gap_sdk/libs/gap_lib/wav_io/wavIO.o  BUILD_EMUL//home/marco-gwt/GWT/NN_menu/starters/keyword_spotting/BUILD_MFCC_MODEL/MFCCKernels.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/TwiddlesDef.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/RFFTTwiddlesDef.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/LUT_Tables/SwapTablesDef.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/MfccBasicKernels.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/FFT_Library.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/CmplxFunctions.o  BUILD_EMUL//home/marco-gwt/GWT/AutotilerV2/DSP_Libraries/PreProcessing.o  -lm
make[1]: Leaving directory '/home/marco-gwt/GWT/NN_menu/starters/keyword_spotting'
{'desired_samples': 16000, 'window_size_samples': 640, 'window_stride_samples': 320, 'spectrogram_length': 49, 'dct_coefficient_count': 10, 'fingerprint_width': 10, 'fingerprint_size': 490, 'label_count': 12, 'sample_rate': 16000, 'preprocess': 'mfcc', 'average_window_width': -1, 'use_power': False}
Pred/Tot:	  91/ 100	Accuracy:	91.00%
Pred/Tot:	 185/ 200	Accuracy:	92.50%
Pred/Tot:	 273/ 300	Accuracy:	91.00%
Pred/Tot:	 353/ 400	Accuracy:	88.25%
Pred/Tot:	 442/ 500	Accuracy:	88.40%
Pred/Tot:	 526/ 600	Accuracy:	87.67%
Pred/Tot:	 618/ 700	Accuracy:	88.29%
Pred/Tot:	 704/ 800	Accuracy:	88.00%
Pred/Tot:	 794/ 900	Accuracy:	88.22%
Pred/Tot:	 888/1000	Accuracy:	88.80%
Pred/Tot:	 979/1100	Accuracy:	89.00%
Pred/Tot:	1071/1200	Accuracy:	89.25%
Pred/Tot:	1158/1300	Accuracy:	89.08%
Pred/Tot:	1246/1400	Accuracy:	89.00%
Pred/Tot:	1329/1500	Accuracy:	88.60%
Pred/Tot:	1422/1600	Accuracy:	88.88%
Pred/Tot:	1510/1700	Accuracy:	88.82%
Pred/Tot:	1599/1800	Accuracy:	88.83%
Pred/Tot:	1689/1900	Accuracy:	88.89%
Pred/Tot:	1777/2000	Accuracy:	88.85%
Pred/Tot:	1866/2100	Accuracy:	88.86%
Pred/Tot:	1957/2200	Accuracy:	88.95%
Pred/Tot:	2046/2300	Accuracy:	88.96%
Pred/Tot:	2132/2400	Accuracy:	88.83%
Pred/Tot:	2217/2500	Accuracy:	88.68%
Pred/Tot:	2306/2600	Accuracy:	88.69%
Pred/Tot:	2397/2700	Accuracy:	88.78%
Pred/Tot:	2485/2800	Accuracy:	88.75%
Pred/Tot:	2573/2900	Accuracy:	88.72%
Pred/Tot:	2656/3000	Accuracy:	88.53%
Pred/Tot:	2748/3100	Accuracy:	88.65%
Pred/Tot:	2840/3200	Accuracy:	88.75%
Pred/Tot:	2933/3300	Accuracy:	88.88%
Pred/Tot:	3016/3400	Accuracy:	88.71%
Pred/Tot:	3107/3500	Accuracy:	88.77%
Pred/Tot:	3197/3600	Accuracy:	88.81%
Pred/Tot:	3282/3700	Accuracy:	88.70%
Pred/Tot:	3375/3800	Accuracy:	88.82%
Pred/Tot:	3465/3900	Accuracy:	88.85%
Pred/Tot:	3553/4000	Accuracy:	88.83%
Pred/Tot:	3642/4100	Accuracy:	88.83%
Pred/Tot:	3732/4200	Accuracy:	88.86%
Pred/Tot:	3818/4300	Accuracy:	88.79%
Pred/Tot:	3907/4400	Accuracy:	88.80%

FINAL VALIDATION ACCURACY:
Pred/Tot:	3947/4444	Accuracy:	88.82%

Confusion matrix:
[[371   0   0   0   0   0   0   0   0   0   0   0]
 [  2 277   7   4   3  12  10  24  11   2   5  14]
 [  0   6 371   7   0   3   7   1   0   0   1   1]
 [  1  15   4 325   2  17   4   1   0   0   1  36]
 [  0   7   1   0 317   3   3   1   2   5   7   4]
 [  1   8   0   5   0 347   2   0   1   0   4   9]
 [  0  10   8   1   2   2 317  11   0   0   0   1]
 [  0   7   0   0   0   0   5 347   0   0   0   4]
 [  2  16   0   0   5   0   0   0 326  11   1   2]
 [  0   3   0   1  35   1   0   1  16 306   4   6]
 [  2   6   0   0  14   5   4   0   0   2 315   2]
 [  2  18   1   6   2  10   0   0   2   0   3 328]]
Pred/Tot:	  88/ 100	Accuracy:	88.00%
Pred/Tot:	 180/ 200	Accuracy:	90.00%
Pred/Tot:	 271/ 300	Accuracy:	90.33%
Pred/Tot:	 359/ 400	Accuracy:	89.75%
Pred/Tot:	 446/ 500	Accuracy:	89.20%
Pred/Tot:	 538/ 600	Accuracy:	89.67%
Pred/Tot:	 629/ 700	Accuracy:	89.86%
Pred/Tot:	 722/ 800	Accuracy:	90.25%
Pred/Tot:	 813/ 900	Accuracy:	90.33%
Pred/Tot:	 904/1000	Accuracy:	90.40%
Pred/Tot:	 991/1100	Accuracy:	90.09%
Pred/Tot:	1083/1200	Accuracy:	90.25%
Pred/Tot:	1166/1300	Accuracy:	89.69%
Pred/Tot:	1254/1400	Accuracy:	89.57%
Pred/Tot:	1341/1500	Accuracy:	89.40%
Pred/Tot:	1433/1600	Accuracy:	89.56%
Pred/Tot:	1520/1700	Accuracy:	89.41%
Pred/Tot:	1613/1800	Accuracy:	89.61%
Pred/Tot:	1700/1900	Accuracy:	89.47%
Pred/Tot:	1789/2000	Accuracy:	89.45%
Pred/Tot:	1880/2100	Accuracy:	89.52%
Pred/Tot:	1974/2200	Accuracy:	89.73%
Pred/Tot:	2063/2300	Accuracy:	89.70%
Pred/Tot:	2153/2400	Accuracy:	89.71%
Pred/Tot:	2243/2500	Accuracy:	89.72%
Pred/Tot:	2330/2600	Accuracy:	89.62%
Pred/Tot:	2420/2700	Accuracy:	89.63%
Pred/Tot:	2509/2800	Accuracy:	89.61%
Pred/Tot:	2596/2900	Accuracy:	89.52%
Pred/Tot:	2682/3000	Accuracy:	89.40%
Pred/Tot:	2772/3100	Accuracy:	89.42%
Pred/Tot:	2863/3200	Accuracy:	89.47%
Pred/Tot:	2956/3300	Accuracy:	89.58%
Pred/Tot:	3045/3400	Accuracy:	89.56%
Pred/Tot:	3131/3500	Accuracy:	89.46%
Pred/Tot:	3224/3600	Accuracy:	89.56%
Pred/Tot:	3307/3700	Accuracy:	89.38%
Pred/Tot:	3398/3800	Accuracy:	89.42%
Pred/Tot:	3491/3900	Accuracy:	89.51%
Pred/Tot:	3575/4000	Accuracy:	89.38%
Pred/Tot:	3665/4100	Accuracy:	89.39%
Pred/Tot:	3757/4200	Accuracy:	89.45%
Pred/Tot:	3844/4300	Accuracy:	89.40%
Pred/Tot:	3934/4400	Accuracy:	89.41%
Pred/Tot:	4021/4500	Accuracy:	89.36%
Pred/Tot:	4105/4600	Accuracy:	89.24%
Pred/Tot:	4192/4700	Accuracy:	89.19%
Pred/Tot:	4285/4800	Accuracy:	89.27%

FINAL TESTING ACCURACY:
Pred/Tot:	4364/4889	Accuracy:	89.26%

Confusion matrix:
[[408   0   0   0   0   0   0   0   0   0   0   0]
 [  0 319   4   4   3   7  13  22   9   2   7  18]
 [  0   9 389   3   0   1  13   1   0   1   1   1]
 [  0  21   1 315   1  22   4   2   0   0   0  39]
 [  0   6   0   1 389   0   3   2   8   6   4   6]
 [  1  11   1   3   0 364   3   0   6   2   1  14]
 [  0   4  11   2   0   0 378  13   0   0   2   2]
 [  0   1   0   0   0   2   4 385   0   0   1   3]
 [  0  16   0   0   6   8   0   1 351   8   0   6]
 [  1   4   1   0  37   2   3   1   8 326   3  16]
 [  0   5   0   0  17  13   2   0   0   1 372   1]
 [  1  10   0  10   0   9   1   1   0   2   0 368]]
